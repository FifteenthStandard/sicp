<!DOCTYPE html>
<html lang="en-AU">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="color-scheme" content="dark light">
  <title>SICP | Structure and Interpretation of Computer Programs</title>
  <link rel="icon" href="favicon.png">
  <link rel="stylesheet" href="css/layout.css">
  <link rel="stylesheet" href="css/style.css">
  <script src="js/Editor.js" type="module"></script>
</head>

<body>
<header>
  <hgroup>
    <h1>Structure and Interpretation of Computer Programs</h1>
    <p>Second edition</p>
  </hgroup>
  <address>
    <p>
      Harold Abelson and Gerald Jay Sussman with Julie Sussman <br>
      foreword by Alan J. Perlis
    </p>
    <p>
      The MIT Press <br>
      Cambridge, Massachusetts&emsp;London, England
    </p>
    <p>
      McGraw-Hill Book Company <br>
      New York&emsp;St. Louis&emsp;San Francisco&emsp;Montreal&emsp;Toronto 
    </p>
  </address>
</header>

<nav>
  <h2>Contents</h2>
  <menu>
    <li><a href="#s0.1">Foreward</a></li>
    <li><a href="#s0.2">Preface to the Second Edition</a></li>
    <li><a href="#s0.3">Preface to the First Edition</a></li>
    <li><a href="#s0.4">Acknowledgements</a></li>
    <li><a href="#s1">1 Building Abstractions with Procedures</a>
      <menu>
        <li><a href="#s1.1">1.1 The Elements of Programming</a>
          <menu>
            <li><a href="#s1.1.1">1.1.1 Expressions</a></li>
            <li><a href="#s1.1.2">1.1.2 Naming and the Environment</a></li>
            <li><a href="#s1.1.3">1.1.3 Evaluating Combinations</a></li>
            <li><a href="#s1.1.4">1.1.4 Compound Procedures</a></li>
            <li><a href="#s1.1.5">1.1.5 The Substitution Model for Procedure Application</a></li>
            <li><a href="#s1.1.6">1.1.6 Conditional Expressions and Predicates</a></li>
            <li><a href="#s1.1.7">1.1.7 Example: Square Roots by Newton’s Method</a></li>
            <li><a href="#s1.1.8">1.1.8 Procedures as Black-Box Abstractions</a></li>
          </menu></li>
        <li><a href="#s1.2">1.2 Procedures and the Processes They Generate</a>
          <menu>
            <li><a href="#s1.2.1">1.2.1 Linear Recursion and Iteration</a></li>
            <li><a href="#s1.2.2">1.2.2 Tree Recursion</a></li>
            <li><a href="#s1.2.3">1.2.3 Orders of Growth</a></li>
            <li><a href="#s1.2.4">1.2.4 Exponentiation</a></li>
            <li><a href="#s1.2.5">1.2.5 Greatest Common Divisors</a></li>
            <li><a href="#s1.2.6">1.2.6 Example: Testing for Primality</a></li>
          </menu></li>
        <li><a href="#s1.3">1.3 Formulating Abstractions with Higher-Order Procedures</a>
          <menu>
            <li><a href="#s1.3.1">1.3.1 Procedures as Arguments</a></li>
            <li><a href="#s1.3.2">1.3.2 Constructing Procedures Using <code>Lambda</code></a></li>
            <!-- <li><a href="#s1.3.3">1.3.3 Procedures as General Methods</a></li> -->
            <!-- <li><a href="#s1.3.4">1.3.4 Procedures as Returned Values</a></li> -->
          </menu></li>
      </menu>
    </li>
    <!-- <li><a href="#s2">2 Building Abstractions with Data</a>
      <menu>
        <li><a href="#s2.1">2.1 Introduction to Data Abstraction</a>
          <menu>
            <li><a href="#s2.1.1">2.1.1 Example: Arithmetic Operations for Rational Numbers</a></li>
            <li><a href="#s2.1.2">2.1.2 Abstraction Barriers</a></li>
            <li><a href="#s2.1.3">2.1.3 What Is Meant by Data?</a></li>
            <li><a href="#s2.1.4">2.1.4 Extended Exercise: Interval Arithmetic</a></li>
          </menu></li>
        <li><a href="#s2.2">2.2 Hierarchical Data and the Closure Property</a>
          <menu>
            <li><a href="#s2.2.1">2.2.1 Representing Sequences</a></li>
            <li><a href="#s2.2.2">2.2.2 Hierarchical Structures</a></li>
            <li><a href="#s2.2.3">2.2.3 Sequences as Conventional Interfaces</a></li>
            <li><a href="#s2.2.4">2.2.4 Example: A Picture Language</a></li>
          </menu></li>
        <li><a href="#s2.3">2.3 Symbolic Data</a>
          <menu>
            <li><a href="#s2.3.1">2.3.1 Quotation</a></li>
            <li><a href="#s2.3.2">2.3.2 Example: Symbolic Differentiation</a></li>
            <li><a href="#s2.3.3">2.3.3 Example: Representing Sets</a></li>
            <li><a href="#s2.3.4">2.3.4 Example: Huffman Encoding Trees</a></li>
          </menu></li>
        <li><a href="#s2.4">2.4 Multiple Representations for Abstract Data</a>
          <menu>
            <li><a href="#s2.4.1">2.4.1 Representations for Complex Numbers</a></li>
            <li><a href="#s2.4.2">2.4.2 Tagged data</a></li>
            <li><a href="#s2.4.3">2.4.3 Data-Directed Programming and Additivity</a></li>
          </menu></li>
        <li><a href="#s2.5">2.5 Systems with Generic Operations</a>
          <menu>
            <li><a href="#s2.5.1">2.5.1 Generic Arithmetic Operations</a></li>
            <li><a href="#s2.5.2">2.5.2 Combining Data of Different Types</a></li>
            <li><a href="#s2.5.3">2.5.3 Example: Symbolic Algebra</a></li>
          </menu></li>
      </menu>
    </li> -->
    <!-- <li><a href="#s3">3 Modularity, Objects, and State</a>
      <menu>
        <li><a href="#s3.1">3.1 Assignment and Local State</a>
          <menu>
            <li><a href="#s3.1.1">3.1.1 Local State Variables</a></li>
            <li><a href="#s3.1.2">3.1.2 The Benefits of Introducing Assignment</a></li>
            <li><a href="#s3.1.3">3.1.3 The Costs of Introducing Assignment</a></li>
          </menu></li>
        <li><a href="#s3.2">3.2 The Environment Model of Evaluation</a>
          <menu>
            <li><a href="#s3.2.1">3.2.1 The Rules for Evaluation</a></li>
            <li><a href="#s3.2.2">3.2.2 Applying Simple Procedures</a></li>
            <li><a href="#s3.2.3">3.2.3 Frames as the Repository of Local State</a></li>
            <li><a href="#s3.2.4">3.2.4 Internal Definitions</a></li>
          </menu></li>
        <li><a href="#s3.3">3.3 Modeling with Mutable Data</a>
          <menu>
            <li><a href="#s3.3.1">3.3.1 Mutable List Structure</a></li>
            <li><a href="#s3.3.2">3.3.2 Representing Queues</a></li>
            <li><a href="#s3.3.3">3.3.3 Representing Tables</a></li>
            <li><a href="#s3.3.4">3.3.4 A Simulator for Digital Circuits</a></li>
            <li><a href="#s3.3.5">3.3.5 Propagation of Constraints</a></li>
          </menu></li>
        <li><a href="#s3.4">3.4 Concurrency: Time Is of the Essence</a>
          <menu>
            <li><a href="#s3.4.1">3.4.1 The Nature of Time in Concurrent Systems</a></li>
            <li><a href="#s3.4.2">3.4.2 Mechanisms for Controlling Concurrency</a></li>
          </menu></li>
        <li><a href="#s3.5">3.5 Streams</a>
          <menu>
            <li><a href="#s3.5.1">3.5.1 Streams Are Delayed Lists</a></li>
            <li><a href="#s3.5.2">3.5.2 Infinite Streams</a></li>
            <li><a href="#s3.5.3">3.5.3 Exploiting the Stream Paradigm</a></li>
            <li><a href="#s3.5.4">3.5.4 Streams and Delayed Evaluation</a></li>
            <li><a href="#s3.5.5">3.5.5 Modularity of Functional Programs and Modularity of Objects</a></li>
          </menu></li>
      </menu>
    </li> -->
    <!-- <li><a href="#s4">4 Metalinguistic Abstraction</a>
      <menu>
        <li><a href="#s4.1">4.1 The Metacircular Evaluator</a>
          <menu>
            <li><a href="#s4.1.1">4.1.1 The Core of the Evaluator</a></li>
            <li><a href="#s4.1.2">4.1.2 Representing Expressions</a></li>
            <li><a href="#s4.1.3">4.1.3 Evaluator Data Structures</a></li>
            <li><a href="#s4.1.4">4.1.4 Running the Evaluator as a Program</a></li>
            <li><a href="#s4.1.5">4.1.5 Data as Programs</a></li>
            <li><a href="#s4.1.6">4.1.6 Internal Definitions</a></li>
            <li><a href="#s4.1.7">4.1.7 Separating Syntactic Analysis from Execution</a></li>
          </menu></li>
        <li><a href="#s4.2">4.2 Variations on a Scheme—Lazy Evaluation</a>
          <menu>
            <li><a href="#s4.2.1">4.2.1 Normal Order and Applicative Order</a></li>
            <li><a href="#s4.2.2">4.2.2 An Interpreter with Lazy Evaluation</a></li>
            <li><a href="#s4.2.3">4.2.3 Streams as Lazy Lists</a></li>
          </menu></li>
        <li><a href="#s4.3">4.3 Variations on a Scheme—Nondeterministic Computing</a>
          <menu>
            <li><a href="#s4.3.1">4.3.1 Amb and Search</a></li>
            <li><a href="#s4.3.2">4.3.2 Examples of Nondeterministic Programs</a></li>
            <li><a href="#s4.3.3">4.3.3 Implementing the <code>Amb</code> Evaluator</a></li>
          </menu></li>
        <li><a href="#s4.4">4.4 Logic Programming</a>
          <menu>
            <li><a href="#s4.4.1">4.4.1 Deductive Information Retrieval</a></li>
            <li><a href="#s4.4.2">4.4.2 How the Query System Works</a></li>
            <li><a href="#s4.4.3">4.4.3 Is Logic Programming Mathematical Logic?</a></li>
            <li><a href="#s4.4.4">4.4.4 Implementing the Query System</a></li>
          </menu></li>
      </menu>
    </li> -->
    <!-- <li><a href="#s5">5 Computing with Register Machines</a>
      <menu>
        <li><a href="#s5.1">5.1 Designing Register Machines</a>
          <menu>
            <li><a href="#s5.1.1">5.1.1 A Language for Describing Register Machines</a></li>
            <li><a href="#s5.1.2">5.1.2 Abstraction in Machine Design</a></li>
            <li><a href="#s5.1.3">5.1.3 Subroutines</a></li>
            <li><a href="#s5.1.4">5.1.4 Using a Stack to Implement Recursion</a></li>
            <li><a href="#s5.1.5">5.1.5 Instruction Summary</a></li>
          </menu></li>
        <li><a href="#s5.2">5.2 A Register-Machine Simulator</a>
          <menu>
            <li><a href="#s5.2.1">5.2.1 The Machine Model</a></li>
            <li><a href="#s5.2.2">5.2.2 The Assembler</a></li>
            <li><a href="#s5.2.3">5.2.3 Generating Execution Procedures for Instructions</a></li>
            <li><a href="#s5.2.4">5.2.4 Monitoring Machine Performance</a></li>
          </menu></li>
        <li><a href="#s5.3">5.3 Storage Allocation and Garbage Collection</a>
          <menu>
            <li><a href="#s5.3.1">5.3.1 Memory as Vectors</a></li>
            <li><a href="#s5.3.2">5.3.2 Maintaining the Illusion of Infinite Memory</a></li>
          </menu></li>
        <li><a href="#s5.4">5.4 The Explicit-Control Evaluator</a>
          <menu>
            <li><a href="#s5.4.1">5.4.1 The Core of the Explicit-Control Evaluator</a></li>
            <li><a href="#s5.4.2">5.4.2 Sequence Evaluation and Tail Recursion</a></li>
            <li><a href="#s5.4.3">5.4.3 Conditionals, Assignments, and Definitions</a></li>
            <li><a href="#s5.4.4">5.4.4 Running the Evaluator</a></li>
          </menu></li>
        <li><a href="#s5.5">5.5 Compilation</a>
          <menu>
            <li><a href="#s5.5.1">5.5.1 Structure of the Compiler</a></li>
            <li><a href="#s5.5.2">5.5.2 Compiling Expressions</a></li>
            <li><a href="#s5.5.3">5.5.3 Compiling Combinations</a></li>
            <li><a href="#s5.5.4">5.5.4 Combining Instruction Sequences</a></li>
            <li><a href="#s5.5.5">5.5.5 An Example of Compiled Code</a></li>
            <li><a href="#s5.5.6">5.5.6 Lexical Addressing</a></li>
            <li><a href="#s5.5.7">5.5.7 Interfacing Compiled Code to the Evaluator</a></li>
          </menu></li>
      </menu> -->
    </li>
    <!-- <li><a href="#s6.1">References</a></li> -->
    <!-- <li><a href="#s6.2">List of Exercises</a></li> -->
    <!-- <li><a href="#s6.3">Index</a></li> -->
  </menu>
</nav>

<main>
<section id="s0">
<section id="s0.1">
  <h3><a href="#s0.1">Foreward</a></h3>
  <p>
    Educators, generals, dieticians, psychologists, and parents program.
    Armies, students, and some societies are programmed. An assault on large
    problems employs a succession of programs, most of which spring into
    existence en route. These programs are rife with issues that appear to be
    particular to the problem at hand. To appreciate programming as an
    intellectual activity in its own right you must turn to computer
    programming; you must read and write computer programs—many of them. It
    doesn’t matter much what the programs are about or what applications they
    serve. What does matter is how well they perform and how smoothly they fit
    with other programs in the creation of still greater programs. The
    programmer must seek both perfection of part and adequacy of collection. In
    this book the use of “program” is focused on the creation, execution, and
    study of programs written in a dialect of Lisp for execution on a digital
    computer. Using Lisp we restrict or limit not what we may program, but only
    the notation for our program descriptions.
  </p>
  <p>
    Our traffic with the subject matter of this book involves us with three
    foci of phenomena: the human mind, collections of computer programs, and
    the computer. Every computer program is a model, hatched in the mind, of a
    real or mental process. These processes, arising from human experience and
    thought, are huge in number, intricate in detail, and at any time only
    partially understood. They are modeled to our permanent satisfaction rarely
    by our computer programs. Thus even though our programs are carefully
    handcrafted discrete collections of symbols, mosaics of interlocking
    functions, they continually evolve: we change them as our perception of the
    model deepens, enlarges, generalizes until the model ultimately attains a
    metastable place within still another model with which we struggle. The
    source of the exhilaration associated with computer programming is the
    continual unfolding within the mind and on the computer of mechanisms
    expressed as programs and the explosion of perception they generate. If art
    interprets our dreams, the computer executes them in the guise of programs!
  </p>
  <p>
    For all its power, the computer is a harsh taskmaster. Its programs must be
    correct, and what we wish to say must be said accurately in every detail.
    As in every other symbolic activity, we become convinced of program truth
    through argument. Lisp itself can be assigned a semantics (another model,
    by the way), and if a program’s function can be specified, say, in the
    predicate calculus, the proof methods of logic can be used to make an
    acceptable correctness argument. Unfortunately, as programs get large and
    complicated, as they almost always do, the adequacy, consistency, and
    correctness of the specifications themselves become open to doubt, so that
    complete formal arguments of correctness seldom accompany large programs.
    Since large programs grow from small ones, it is crucial that we develop an
    arsenal of standard program structures of whose correctness we have become
    sure—we call them idioms—and learn to combine them into larger structures
    using organizational techniques of proven value. These techniques are
    treated at length in this book, and understanding them is essential to
    participation in the Promethean enterprise called programming. More than
    anything else, the uncovering and mastery of powerful organizational
    techniques accelerates our ability to create large, significant programs.
    Conversely, since writing large programs is very taxing, we are stimulated
    to invent new methods of reducing the mass of function and detail to be
    fitted into large programs.
  </p>
  <p>
    Unlike programs, computers must obey the laws of physics. If they wish to
    perform rapidly—a few nanoseconds per state change—they must transmit
    electrons only small distances (at most 1 1/2 feet). The heat generated by
    the huge number of devices so concentrated in space has to be removed. An
    exquisite engineering art has been developed balancing between multiplicity
    of function and density of devices. In any event, hardware always operates
    at a level more primitive than that at which we care to program. The
    processes that transform our Lisp programs to “machine” programs are
    themselves abstract models which we program. Their study and creation give
    a great deal of insight into the organizational programs associated with
    programming arbitrary models. Of course the computer itself can be so
    modeled. Think of it: the behavior of the smallest physical switching
    element is modeled by quantum mechanics described by differential equations
    whose detailed behavior is captured by numerical approximations represented
    in computer programs executing on computers composed of …!
  </p>
  <p>
    It is not merely a matter of tactical convenience to separately identify
    the three foci. Even though, as they say, it’s all in the head, this
    logical separation induces an acceleration of symbolic traffic between
    these foci whose richness, vitality, and potential is exceeded in human
    experience only by the evolution of life itself. At best, relationships
    between the foci are metastable. The computers are never large enough or
    fast enough. Each breakthrough in hardware technology leads to more massive
    programming enterprises, new organizational principles, and an enrichment
    of abstract models. Every reader should ask himself periodically “Toward
    what end, toward what end?”—but do not ask it too often lest you pass up
    the fun of programming for the constipation of bittersweet philosophy.
  </p>
  <p>
    Among the programs we write, some (but never enough) perform a precise
    mathematical function such as sorting or finding the maximum of a sequence
    of numbers, determining primality, or finding the square root. We call such
    programs algorithms, and a great deal is known of their optimal behavior,
    particularly with respect to the two important parameters of execution time
    and data storage requirements. A programmer should acquire good algorithms
    and idioms. Even though some programs resist precise specifications, it is
    the responsibility of the programmer to estimate, and always to attempt to
    improve, their performance.
  </p>
  <p>
    Lisp is a survivor, having been in use for about a quarter of a century.
    Among the active programming languages only Fortran has had a longer life.
    Both languages have supported the programming needs of important areas of
    application, Fortran for scientific and engineering computation and Lisp
    for artificial intelligence. These two areas continue to be important, and
    their programmers are so devoted to these two languages that Lisp and
    Fortran may well continue in active use for at least another
    quarter-century.
  </p>
  <p>
    Lisp changes. The Scheme dialect used in this text has evolved from the
    original Lisp and differs from the latter in several important ways,
    including static scoping for variable binding and permitting functions to
    yield functions as values. In its semantic structure Scheme is as closely
    akin to Algol 60 as to early Lisps. Algol 60, never to be an active
    language again, lives on in the genes of Scheme and Pascal. It would be
    difficult to find two languages that are the communicating coin of two more
    different cultures than those gathered around these two languages. Pascal
    is for building pyramids—imposing, breathtaking, static structures built by
    armies pushing heavy blocks into place. Lisp is for building
    organisms—imposing, breathtaking, dynamic structures built by squads
    fitting fluctuating myriads of simpler organisms into place. The organizing
    principles used are the same in both cases, except for one extraordinarily
    important difference: The discretionary exportable functionality entrusted
    to the individual Lisp programmer is more than an order of magnitude
    greater than that to be found within Pascal enterprises. Lisp programs
    inflate libraries with functions whose utility transcends the application
    that produced them. The list, Lisp’s native data structure, is largely
    responsible for such growth of utility. The simple structure and natural
    applicability of lists are reflected in functions that are amazingly
    nonidiosyncratic. In Pascal the plethora of declarable data structures
    induces a specialization within functions that inhibits and penalizes
    casual cooperation. It is better to have 100 functions operate on one data
    structure than to have 10 functions operate on 10 data structures. As a
    result the pyramid must stand unchanged for a millennium; the organism must
    evolve or perish.
  </p>
  <p>
    To illustrate this difference, compare the treatment of material and
    exercises within this book with that in any first-course text using Pascal.
    Do not labor under the illusion that this is a text digestible at MIT only,
    peculiar to the breed found there. It is precisely what a serious book on
    programming Lisp must be, no matter who the student is or where it is used.
  </p>
  <p>
    Note that this is a text about programming, unlike most Lisp books, which
    are used as a preparation for work in artificial intelligence. After all,
    the critical programming concerns of software engineering and artificial
    intelligence tend to coalesce as the systems under investigation become
    larger. This explains why there is such growing interest in Lisp outside of
    artificial intelligence.
  </p>
  <p>
    As one would expect from its goals, artificial intelligence research
    generates many significant programming problems. In other programming
    cultures this spate of problems spawns new languages. Indeed, in any very
    large programming task a useful organizing principle is to control and
    isolate traffic within the task modules via the invention of language.
    These languages tend to become less primitive as one approaches the
    boundaries of the system where we humans interact most often. As a result,
    such systems contain complex language-processing functions replicated many
    times. Lisp has such a simple syntax and semantics that parsing can be
    treated as an elementary task. Thus parsing technology plays almost no role
    in Lisp programs, and the construction of language processors is rarely an
    impediment to the rate of growth and change of large Lisp systems. Finally,
    it is this very simplicity of syntax and semantics that is responsible for
    the burden and freedom borne by all Lisp programmers. No Lisp program of
    any size beyond a few lines can be written without being saturated with
    discretionary functions. Invent and fit; have fits and reinvent! We toast
    the Lisp programmer who pens his thoughts within nests of parentheses.
  </p>
  <address>
    Alan J. Perlis <br>
    New Haven, Connecticut
  </address>
</section>

<section id="s0.2">
  <h3><a href="#s0.2">Preface to the Second Edition</a></h3>
  <blockquote>
    <p>
      Is it possible that software is not like anything else, that it is meant
      to be discarded: that the whole point is to always see it as a soap
      bubble?
    </p>
    <address>
      Alan J. Perlis
    </address>
  </blockquote>
  <p>
    The material in this book has been the basis of MIT’s entry-level computer
    science subject since 1980. We had been teaching this material for four
    years when the first edition was published, and twelve more years have
    elapsed until the appearance of this second edition. We are pleased that
    our work has been widely adopted and incorporated into other texts. We have
    seen our students take the ideas and programs in this book and build them
    in as the core of new computer systems and languages. In literal
    realization of an ancient Talmudic pun, our students have become our
    builders. We are lucky to have such capable students and such accomplished
    builders.
  </p>
  <p>
    In preparing this edition, we have incorporated hundreds of clarifications
    suggested by our own teaching experience and the comments of colleagues at
    MIT and elsewhere. We have redesigned most of the major programming systems
    in the book, including the generic-arithmetic system, the interpreters, the
    register-machine simulator, and the compiler; and we have rewritten all the
    program examples to ensure that any Scheme implementation conforming to the
    IEEE Scheme standard (IEEE 1990) will be able to run the code.
  </p>
  <p>
    This edition emphasizes several new themes. The most important of these is
    the central role played by different approaches to dealing with time in
    computational models: objects with state, concurrent programming,
    functional programming, lazy evaluation, and nondeterministic programming.
    We have included new sections on concurrency and nondeterminism, and we
    have tried to integrate this theme throughout the book.
  </p>
  <p>
    The first edition of the book closely followed the syllabus of our MIT
    one-semester subject. With all the new material in the second edition, it
    will not be possible to cover everything in a single semester, so the
    instructor will have to pick and choose. In our own teaching, we sometimes
    skip the section on logic programming (section <a href="#s4.4">4.4</a>), we
    have students use the register-machine simulator but we do not cover its
    implementation (section <a href="#s5.2">5.2</a>), and we give only a
    cursory overview of the compiler (section <a href="#s5.5">5.5</a>). Even
    so, this is still an intense course. Some instructors may wish to cover
    only the first three or four chapters, leaving the other material for
    subsequent courses.
  </p>
  <p>
    The World-Wide-Web site <a href="https://mitpress.mit.edu/sicp"
    >mitpress.mit.edu/sicp</a> provides support for users of this book. This
    includes programs from the book, sample programming assignments,
    supplementary materials, and downloadable implementations of the Scheme
    dialect of Lisp.
  </p>
</section>

<section id="s0.3">
  <h3><a href="#s0.3">Preface to the First Edition</a></h3>
  <blockquote>
    <p>
      A computer is like a violin. You can imagine a novice trying first a
      phonograph and then a violin. The latter, he says, sounds terrible. That
      is the argument we have heard from our humanists and most of our computer
      scientists. Computer programs are good, they say, for particular
      purposes, but they aren’t flexible. Neither is a violin, or a typewriter,
      until you learn how to use it.
    </p>
    <address>
      Marvin Minsky, “Why Programming Is a Good Medium for Expressing
      Poorly-Understood and Sloppily-Formulated Ideas”
    </address>
  </blockquote>
  <p>
    “The Structure and Interpretation of Computer Programs” is the entry-level
    subject in computer science at the Massachusetts Institute of Technology.
    It is required of all students at MIT who major in electrical engineering
    or in computer science, as one-fourth of the “common core curriculum,”
    which also includes two subjects on circuits and linear systems and a
    subject on the design of digital systems. We have been involved in the
    development of this subject since 1978, and we have taught this material in
    its present form since the fall of 1980 to between 600 and 700 students
    each year. Most of these students have had little or no prior formal
    training in computation, although many have played with computers a bit and
    a few have had extensive programming or hardware-design experience.
  </p>
  <p>
    Our design of this introductory computer-science subject reflects two major
    concerns. First, we want to establish the idea that a computer language is
    not just a way of getting a computer to perform operations but rather that
    it is a novel formal medium for expressing ideas about methodology. Thus,
    programs must be written for people to read, and only incidentally for
    machines to execute. Second, we believe that the essential material to be
    addressed by a subject at this level is not the syntax of particular
    programming-language constructs, nor clever algorithms for computing
    particular functions efficiently, nor even the mathematical analysis of
    algorithms and the foundations of computing, but rather the techniques used
    to control the intellectual complexity of large software systems.
  </p>
  <p>
    Our goal is that students who complete this subject should have a good feel
    for the elements of style and the aesthetics of programming. They should
    have command of the major techniques for controlling complexity in a large
    system. They should be capable of reading a 50-page-long program, if it is
    written in an exemplary style. They should know what not to read, and what
    they need not understand at any moment. They should feel secure about
    modifying a program, retaining the spirit and style of the original author.
  </p>
  <p>
    These skills are by no means unique to computer programming. The techniques
    we teach and draw upon are common to all of engineering design. We control
    complexity by building abstractions that hide details when appropriate. We
    control complexity by establishing conventional interfaces that enable us
    to construct systems by combining standard, well-understood pieces in a
    “mix and match” way. We control complexity by establishing new languages
    for describing a design, each of which emphasizes particular aspects of the
    design and deemphasizes others.
  </p>
  <p>
    Underlying our approach to this subject is our conviction that “computer
    science” is not a science and that its significance has little to do with
    computers. The computer revolution is a revolution in the way we think and
    in the way we express what we think. The essence of this change is the
    emergence of what might best be called procedural epistemology—the study of
    the structure of knowledge from an imperative point of view, as opposed to
    the more declarative point of view taken by classical mathematical
    subjects. Mathematics provides a framework for dealing precisely with
    notions of “what is.” Computation provides a framework for dealing
    precisely with notions of “how to.”
  </p>
  <p>
    In teaching our material we use a dialect of the programming language Lisp.
    We never formally teach the language, because we don’t have to. We just use
    it, and students pick it up in a few days. This is one great advantage of
    Lisp-like languages: They have very few ways of forming compound
    expressions, and almost no syntactic structure. All of the formal
    properties can be covered in an hour, like the rules of chess. After a
    short time we forget about syntactic details of the language (because there
    are none) and get on with the real issues—figuring out what we want to
    compute, how we will decompose problems into manageable parts, and how we
    will work on the parts. Another advantage of Lisp is that it supports (but
    does not enforce) more of the large-scale strategies for modular
    decomposition of programs than any other language we know. We can make
    procedural and data abstractions, we can use higher-order functions to
    capture common patterns of usage, we can model local state using assignment
    and data mutation, we can link parts of a program with streams and delayed
    evaluation, and we can easily implement embedded languages. All of this is
    embedded in an interactive environment with excellent support for
    incremental program design, construction, testing, and debugging. We thank
    all the generations of Lisp wizards, starting with John McCarthy, who have
    fashioned a fine tool of unprecedented power and elegance.
  </p>
  <p>
    Scheme, the dialect of Lisp that we use, is an attempt to bring together
    the power and elegance of Lisp and Algol. From Lisp we take the
    metalinguistic power that derives from the simple syntax, the uniform
    representation of programs as data objects, and the garbage-collected
    heap-allocated data. From Algol we take lexical scoping and block
    structure, which are gifts from the pioneers of programming-language design
    who were on the Algol committee. We wish to cite John Reynolds and Peter
    Landin for their insights into the relationship of Church’s lambda calculus
    to the structure of programming languages. We also recognize our debt to
     the mathematicians who scouted out this territory decades before computers
     appeared on the scene. These pioneers include Alonzo Church, Barkley
     Rosser, Stephen Kleene, and Haskell Curry.
  </p>
</section>

<section id="s0.4">
  <h3><a href="#s0.4">Acknowledgements</a></h3>
  <p>
    We would like to thank the many people who have helped us develop this book
    and this curriculum.
  </p>
  <p>
    Our subject is a clear intellectual descendant of “6.231,” a wonderful
    subject on programming linguistics and the lambda calculus taught at MIT in
    the late 1960s by Jack Wozencraft and Arthur Evans, Jr.
  </p>
  <p>
    We owe a great debt to Robert Fano, who reorganized MIT’s introductory
    curriculum in electrical engineering and computer science to emphasize the
    principles of engineering design. He led us in starting out on this
    enterprise and wrote the first set of subject notes from which this book
    evolved.
  </p>
  <p>
    Much of the style and aesthetics of programming that we try to teach were
    developed in conjunction with Guy Lewis Steele Jr., who collaborated with
    Gerald Jay Sussman in the initial development of the Scheme language. In
    addition, David Turner, Peter Henderson, Dan Friedman, David Wise, and Will
    Clinger have taught us many of the techniques of the functional programming
    community that appear in this book.
  </p>
  <p>
    Joel Moses taught us about structuring large systems. His experience with
    the Macsyma system for symbolic computation provided the insight that one
    should avoid complexities of control and concentrate on organizing the data
    to reflect the real structure of the world being modeled.
  </p>
  <p>
    Marvin Minsky and Seymour Papert formed many of our attitudes about
    programming and its place in our intellectual lives. To them we owe the
    understanding that computation provides a means of expression for exploring
    ideas that would otherwise be too complex to deal with precisely. They
    emphasize that a student’s ability to write and modify programs provides a
    powerful medium in which exploring becomes a natural activity.
  </p>
  <p>
    We also strongly agree with Alan Perlis that programming is lots of fun and
    we had better be careful to support the joy of programming. Part of this
    joy derives from observing great masters at work. We are fortunate to have
    been apprentice programmers at the feet of Bill Gosper and Richard
    Greenblatt.
  </p>
  <p>
    It is difficult to identify all the people who have contributed to the
    development of our curriculum. We thank all the lecturers, recitation
    instructors, and tutors who have worked with us over the past fifteen years
    and put in many extra hours on our subject, especially Bill Siebert, Albert
    Meyer, Joe Stoy, Randy Davis, Louis Braida, Eric Grimson, Rod Brooks, Lynn
    Stein, and Peter Szolovits. We would like to specially acknowledge the
    outstanding teaching contributions of Franklyn Turbak, now at Wellesley;
    his work in undergraduate instruction set a standard that we can all aspire
    to. We are grateful to Jerry Saltzer and Jim Miller for helping us grapple
    with the mysteries of concurrency, and to Peter Szolovits and David
    McAllester for their contributions to the exposition of nondeterministic
    evaluation in chapter 4.
  </p>
  <p>
    Many people have put in significant effort presenting this material at
    other universities. Some of the people we have worked closely with are
    Jacob Katzenelson at the Technion, Hardy Mayer at the University of
    California at Irvine, Joe Stoy at Oxford, Elisha Sacks at Purdue, and Jan
    Komorowski at the Norwegian University of Science and Technology. We are
    exceptionally proud of our colleagues who have received major teaching
    awards for their adaptations of this subject at other universities,
    including Kenneth Yip at Yale, Brian Harvey at the University of California
    at Berkeley, and Dan Huttenlocher at Cornell.
  </p>
  <p>
    Al Moyé arranged for us to teach this material to engineers at
    Hewlett-Packard, and for the production of videotapes of these lectures. We
    would like to thank the talented instructors—in particular Jim Miller, Bill
    Siebert, and Mike Eisenberg—who have designed continuing education courses
    incorporating these tapes and taught them at universities and industry all
    over the world.
  </p>
  <p>
    Many educators in other countries have put in significant work translating
    the first edition. Michel Briand, Pierre Chamard, and André Pic produced a
    French edition; Susanne Daniels-Herold produced a German edition; and Fumio
    Motoyoshi produced a Japanese edition. We do not know who produced the
    Chinese edition, but we consider it an honor to have been selected as the
    subject of an “unauthorized” translation.
  </p>
  <p>
    It is hard to enumerate all the people who have made technical
    contributions to the development of the Scheme systems we use for
    instructional purposes. In addition to Guy Steele, principal wizards have
    included Chris Hanson, Joe Bowbeer, Jim Miller, Guillermo Rozas, and
    Stephen Adams. Others who have put in significant time are Richard
    Stallman, Alan Bawden, Kent Pitman, Jon Taft, Neil Mayle, John Lamping,
    Gwyn Osnos, Tracy Larrabee, George Carrette, Soma Chaudhuri, Bill
    Chiarchiaro, Steven Kirsch, Leigh Klotz, Wayne Noss, Todd Cass, Patrick
    O’Donnell, Kevin Theobald, Daniel Weise, Kenneth Sinclair, Anthony
    Courtemanche, Henry M. Wu, Andrew Berlin, and Ruth Shyu.
  </p>
  <p>
    Beyond the MIT implementation, we would like to thank the many people who
    worked on the IEEE Scheme standard, including William Clinger and Jonathan
    Rees, who edited the R4RS, and Chris Haynes, David Bartley, Chris Hanson,
    and Jim Miller, who prepared the IEEE standard.
  </p>
  <p>
    Dan Friedman has been a long-time leader of the Scheme community. The
    community’s broader work goes beyond issues of language design to encompas
    significant educational innovations, such as the high-school curriculum
    based on EdScheme by Schemer’s Inc., and the wonderful books by Mike
    Eisenberg and by Brian Harvey and Matthew Wright.
  </p>
  <p>
    We appreciate the work of those who contributed to making this a real book,
    especially Terry Ehling, Larry Cohen, and Paul Bethge at the MIT Press.
    Ella Mazel found the wonderful cover image. For the second edition we are
    particularly grateful to Bernard and Ella Mazel for help with the book
    design, and to David Jones, TEX wizard extraordinaire. We also are indebted
    to those readers who made penetrating comments on the new draft: Jacob
    Katzenelson, Hardy Mayer, Jim Miller, and especially Brian Harvey, who did
    unto this book as Julie did unto his book Simply Scheme.
  </p>
  <p>
    Finally, we would like to acknowledge the support of the organizations that
    have encouraged this work over the years, including support from
    Hewlett-Packard, made possible by Ira Goldstein and Joel Birnbaum, and
    support from DARPA, made possible by Bob Kahn.
  </p>
</section>
</section>

<section id="s1">
<section id="s1.0">
  <h2><a href="#s1">Chapter 1: Building Abstractions with Procedures</a></h2>
  <blockquote>
    <p>
      The acts of the mind, wherein it exerts its power over simple ideas, are
      chiefly these three: 1. Combining several simple ideas into one compound
      one, and thus all complex ideas are made. 2. The second is bringing two
      ideas, whether simple or complex, together, and setting them by one
      another so as to take a view of them at once, without uniting them into
      one, by which it gets all its ideas of relations. 3. The third is
      separating them from all other ideas that accompany them in their real
      existence: this is called abstraction, and thus all its general ideas are
      made.
    </p>
    <address>
      John Locke, <em>An Essay Concerning Human Understanding</em> (1690)
    </address>
  </blockquote>
  <p>
    We are about to study the idea of a <em>computational process</em>.
    Computational processes are abstract beings that inhabit computers. As they
    evolve, processes manipulate other abstract things called <em>data</em>.
    The evolution of a process is directed by a pattern of rules called a
    <em>program</em>. People create programs to direct processes. In effect, we
    conjure the spirits of the computer with our spells.
  </p>
  <p>
    A computational process is indeed much like a sorcerer’s idea of a spirit.
    It cannot be seen or touched. It is not composed of matter at all. However,
    it is very real. It can perform intellectual work. It can answer questions.
    It can affect the world by disbursing money at a bank or by controlling a
    robot arm in a factory. The programs we use to conjure processes are like a
    sorcerer’s spells. They are carefully composed from symbolic expressions in
    arcane and esoteric <em>programming languages</em> that prescribe the tasks
    we want our processes to perform.
  </p>
  <p>
    A computational process, in a correctly working computer, executes programs
    precisely and accurately. Thus, like the sorcerer’s apprentice, novice
    programmers must learn to understand and to anticipate the consequences of
    their conjuring. Even small errors (usually called <em>bugs</em> or
    <em>glitches</em>) in programs can have complex and unanticipated
    consequences.
  </p>
  <p>
    Fortunately, learning to program is considerably less dangerous than
    learning sorcery, because the spirits we deal with are conveniently
    contained in a secure way. Real-world programming, however, requires care,
    expertise, and wisdom. A small bug in a computer-aided design program, for
    example, can lead to the catastrophic collapse of an airplane or a dam or
    the self-destruction of an industrial robot.
  </p>
  <p>
    Master software engineers have the ability to organize programs so that
    they can be reasonably sure that the resulting processes will perform the
    tasks intended. They can visualize the behavior of their systems in
    advance. They know how to structure programs so that unanticipated problems
    do not lead to catastrophic consequences, and when problems do arise, they
    can <em>debug</em> their programs. Well-designed computational systems,
    like well-designed automobiles or nuclear reactors, are designed in a
    modular manner, so that the parts can be constructed, replaced, and
    debugged separately.
  </p>
  <h5>Programming in Lisp</h5>
  <p>
    We need an appropriate language for describing processes, and we will use
    for this purpose the programming language Lisp. Just as our everyday
    thoughts are usually expressed in our natural language (such as English,
    French, or Japanese), and descriptions of quantitative phenomena are
    expressed with mathematical notations, our procedural thoughts will be
    expressed in Lisp. Lisp was invented in the late 1950s as a formalism for
    reasoning about the use of certain kinds of logical expressions, called
    recursion equations, as a model for computation. The language was conceived
    by John McCarthy and is based on his paper “Recursive Functions of Symbolic
    Expressions and Their Computation by Machine” (McCarthy 1960).
  </p>
  <p>
    Despite its inception as a mathematical formalism, Lisp is a practical
    programming language. A Lisp interpreter is a machine that carries out
    processes described in the Lisp language. The first Lisp interpreter was
    implemented by McCarthy with the help of colleagues and students in the
    Artificial Intelligence Group of the MIT Research Laboratory of Electronics
    and in the MIT Computation Center.<sup id="r1.1s"><a href="#r1.1d"
    >1</a></sup> Lisp, whose name is an acronym for LISt Processing, was
    designed to provide symbol-manipulating capabilities for attacking
    programming problems such as the symbolic differentiation and integration
    of algebraic expressions. It included for this purpose new data objects
    known as atoms and lists, which most strikingly set it apart from all other
    languages of the period.
  </p>
  <p>
    Lisp was not the product of a concerted design effort. Instead, it evolved
    informally in an experimental manner in response to users’ needs and to
    pragmatic implementation considerations. Lisp’s informal evolution has
    continued through the years, and the community of Lisp users has
    traditionally resisted attempts to promulgate any “official” definition of
    the language. This evolution, together with the flexibility and elegance of
    the initial conception, has enabled Lisp, which is the second oldest
    language in widespread use today (only Fortran is older), to continually
    adapt to encompass the most modern ideas about program design. Thus, Lisp
    is by now a family of dialects, which, while sharing most of the original
    features, may differ from one another in significant ways. The dialect of
    Lisp used in this book is called Scheme.<sup id="r1.2s"><a href="#r1.2d"
    >2</a></sup>
  </p>
  <p>
    Because of its experimental character and its emphasis on symbol
    manipulation, Lisp was at first very inefficient for numerical
    computations, at least in comparison with Fortran. Over the years, however,
    Lisp compilers have been developed that translate programs into machine
    code that can perform numerical computations reasonably efficiently. And
    for special applications, Lisp has been used with great effectiveness.<sup
    id="r1.3s"><a href="#r1.3d">3</a></sup> Although Lisp has not yet overcome
    its old reputation as hopelessly inefficient, Lisp is now used in many
    applications where efficiency is not the central concern. For example, Lisp
    has become a language of choice for operating-system shell languages and
    for extension languages for editors and computer-aided design systems.
  </p>
  <p>
    If Lisp is not a mainstream language, why are we using it as the framework
    for our discussion of programming? Because the language possesses unique
    features that make it an excellent medium for studying important
    programming constructs and data structures and for relating them to the
    linguistic features that support them. The most significant of these
    features is the fact that Lisp descriptions of processes, called
    procedures, can themselves be represented and manipulated as Lisp data. The
    importance of this is that there are powerful program-design techniques
    that rely on the ability to blur the traditional distinction between
    “passive” data and “active” processes. As we shall discover, Lisp’s
    flexibility in handling procedures as data makes it one of the most
    convenient languages in existence for exploring these techniques. The
    ability to represent procedures as data also makes Lisp an excellent
    language for writing programs that must manipulate other programs as data,
    such as the interpreters and compilers that support computer languages.
    Above and beyond these considerations, programming in Lisp is great fun.
  </p>
</section>
<hr>
<footer>
  <p>
    <sup id="r1.1d"><a href="#r1.1s">1</a></sup> The <em>Lisp 1 Programmer’s
    Manual</em> appeared in 1960, and the <em>Lisp 1.5 Programmer’s
    Manual</em> (McCarthy 1965) was published in 1962. The early history of
    Lisp is described in McCarthy 1978.
  </p>
  <p>
    <sup id="r1.2d"><a href="#r1.2s">2</a></sup> The two dialects in which
    most major Lisp programs of the 1970s were written are MacLisp (Moon
    1978; Pitman 1983), developed at the MIT Project MAC, and Interlisp
    (Teitelman 1974), developed at Bolt Beranek and Newman Inc. and the Xerox
    Palo Alto Research Center. Portable Standard Lisp (Hearn 1969; Griss
    1981) was a Lisp dialect designed to be easily portable between different
    machines. MacLisp spawned a number of subdialects, such as Franz Lisp,
    which was developed at the University of California at Berkeley, and
    Zetalisp (Moon 1981), which was based on a special-purpose processor
    designed at the MIT Artificial Intelligence Laboratory to run Lisp very
    efficiently. The Lisp dialect used in this book, called Scheme (Steele
    1975), was invented in 1975 by Guy Lewis Steele Jr. and Gerald Jay
    Sussman of the MIT Artificial Intelligence Laboratory and later
    reimplemented for instructional use at MIT. Scheme became an IEEE
    standard in 1990 (IEEE 1990). The Common Lisp dialect (Steele 1982,
    Steele 1990) was developed by the Lisp community to combine features from
    the earlier Lisp dialects to make an industrial standard for Lisp. Common
    Lisp became an ANSI standard in 1994 (ANSI 1994).
  </p>
  <p>
    <sup id="r1.3d"><a href="#r1.3s">3</a></sup> One such special application
    was a breakthrough computation of scientific importance—an integration of
    the motion of the Solar System that extended previous results by nearly
    two orders of magnitude, and demonstrated that the dynamics of the Solar
    System is chaotic. This computation was made possible by new integration
    algorithms, a special-purpose compiler, and a special-purpose computer
    all implemented with the aid of software tools written in Lisp (Abelson
    et al. 1992; Sussman and Wisdom 1992).
  </p>
</footer>

<section id="s1.1">
<section id="s1.1.0">
  <h3><a href="#s1.1">1.1 The Elements of Programming</a></h3>
  <p>
    A powerful programming language is more than just a means for instructing a
    computer to perform tasks. The language also serves as a framework within
    which we organize our ideas about processes. Thus, when we describe a
    language, we should pay particular attention to the means that the language
    provides for combining simple ideas to form more complex ideas. Every
    powerful language has three mechanisms for accomplishing this:
  </p>
  <ul>
    <li>
      <em>primitive expressions</em>, which represent the simplest entities the
      language is concerned with,
    </li>
    <li>
      <em>means of combination</em>, by which compound elements are built from
      simpler ones, and
    </li>
    <li>
      <em>means of abstraction</em>, by which compound elements can be named
      and manipulated as units.
    </li>
  </ul>
    In programming, we deal with two kinds of elements: procedures and data.
    (Later we will discover that they are really not so distinct.) Informally,
    data is “stuff” that we want to manipulate, and procedures are descriptions
    of the rules for manipulating the data. Thus, any powerful programming
    language should be able to describe primitive data and primitive procedures
    and should have methods for combining and abstracting procedures and data.
  </p>
  <p>
    In this chapter we will deal only with simple numerical data so that we can
    focus on the rules for building procedures.<sup id="r1.4s"><a href="#r1.4d"
    >4</a></sup> In later chapters we will see that these same rules allow us
    to build procedures to manipulate compound data as well.
  </p>
</section>

<section id="s1.1.1">
  <h4><a href="#s1.1.1">1.1.1 Expressions</a></h4>
  <p>
    One easy way to get started at programming is to examine some typical
    interactions with an interpreter for the Scheme dialect of Lisp. Imagine
    that you are sitting at a computer terminal. You type an
    <em>expression</em>, and the interpreter responds by displaying the result
    of its evaluating that expression.
  </p>
  <p>
    One kind of primitive expression you might type is a number. (More
    precisely, the expression that you type consists of the numerals that
    represent the number in base 10.) If you present Lisp with a number
  </p>
  <textarea name="1.1.1-01">486</textarea>
  <p>
    the interpreter will respond by printing<sup id="r1.5s"><a href="#r1.5d"
    >5</a></sup>
  </p>
  <output for="1.1.1-01" class="block">486</output>
  <p>
    Expressions representing numbers may be combined with an expression
    representing a primitive procedure (such as <code>+</code> or
    <code>*</code>) to form a compound expression that represents the
    application of the procedure to those numbers. For example:
  </p>
  <textarea name="1.1.1-02">
(+ 137 349)
(- 1000 334)
(* 5 99)
(/ 10 5)
(+ 2.7 10)</textarea>
  <output for="1.1.1-02" class="block">486
666
495
2
12.7</output>
  <p>
    Expressions such as these, formed by delimiting a list of expressions
    within parentheses in order to denote procedure application, are called
    <em>combinations</em>. The leftmost element in the list is called the
    <em>operator</em>, and the other elements are called <em>operands</em>. The
    value of a combination is obtained by applying the procedure specified by
    the operator to the <em>arguments</em> that are the values of the operands.
  </p>
  <p>
    The convention of placing the operator to the left of the operands is known
    as <em>prefix notation</em>, and it may be somewhat confusing at first
    because it departs significantly from the customary mathematical
    convention. Prefix notation has several advantages, however. One of them is
    that it can accommodate procedures that may take an arbitrary number of
    arguments, as in the following examples:
</p>
  <textarea name="1.1.1-03">
(+ 21 35 12 7)
(* 25 4 12)</textarea>
  <output for="1.1.1-03" class="block">75
1200</output>
  <p>
    No ambiguity can arise, because the operator is always the leftmost element
    and the entire combination is delimited by the parentheses.
  </p>
  <p>
    A second advantage of prefix notation is that it extends in a
    straightforward way to allow combinations to be <em>nested</em>, that is,
    to have combinations whose elements are themselves combinations:
  </p>
  <textarea name="1.1.1-04">
(+ (* 3 5) (- 10 6))</textarea>
  <output for="1.1.1-04" class="block">19</output>
  <p>
    There is no limit (in principle) to the depth of such nesting and to the
    overall complexity of the expressions that the Lisp interpreter can
    evaluate. It is we humans who get confused by still relatively simple
    expressions such as
  </p>
  <textarea name="1.1.1-05">
(+ (* 3 (+ (* 2 4) (+ 3 5))) (+ (- 10 7) 6))</textarea>
  <p>
    which the interpreter would readily evaluate to be <output for="1.1.1-05"
    >57</output>. We can help ourselves by writing such an expression in the
    form
  </p>
  <textarea name="1.1.1-06">
(+ (* 3
      (+ (* 2 4)
          (+ 3 5)))
   (+ (- 10 7)
      6))</textarea>
  <output for="1.1.1-06" class="block">57</output>
  <p>
    following a formatting convention known as <em>pretty-printing</em>, in
    which each long combination is written so that the operands are aligned
    vertically. The resulting indentations display clearly the structure of the
    expression.<sup id="r1.6s"><a href="#r1.6d">6</a></sup>
  </p>
  <p>
    Even with complex expressions, the interpreter always operates in the same
    basic cycle: It reads an expression from the terminal, evaluates the
    expression, and prints the result. This mode of operation is often
    expressed by saying that the interpreter runs in a <em>read-eval-print</em>
    loop. Observe in particular that it is not necessary to explicitly instruct
    the interpreter to print the value of the expression.<sup id="r1.7s"><a
    href="#r1.7d">7</a></sup>
  </p>
</section>

<section id="s1.1.2">
  <h4><a href="#s1.1.2">1.1.2 Naming and the Environment</a></h4>
  <p>
    A critical aspect of a programming language is the means it provides for
    using names to refer to computational objects. We say that the name
    identifies a <em>variable</em> whose <em>value</em> is the object.
  </p>
  <p>
    In the Scheme dialect of Lisp, we name things with <code>define</code>.
    Typing
  </p>
  <textarea name="1.1.2-01">(define size 2)</textarea>
  <output for="1.1.2-01" class="block">size</output>
  <p>
    causes the interpreter to associate the value <code>2</code> with the name
    <code>size</code>.<sup id="r1.8s"><a href="#r1.8d">8</a></sup> Once the
    name <code>size</code> has been associated with the number <code>2</code>,
    we can refer to the value <code>2</code> by name:
  </p>
  <textarea name="1.1.2-02" data-extends="1.1.2-01">
size
(* 5 size)</textarea>
  <output for="1.1.2-02" class="block">2
10</output>
  <p>
    Here are further examples of the use of <code>define</code>:
  </p>
  <textarea name="1.1.2-03">
(define pi 3.14159)
(define radius 10)
(* pi (* radius radius))
(define circumference (* 2 pi radius))
circumference</textarea>
  <output for="1.1.2-03" class="block">pi
radius
314.159
circumference
62.8318</output>
  <p>
    <code>Define</code> is our language’s simplest means of abstraction, for it
    allows us to use simple names to refer to the results of compound
    operations, such as the <code>circumference</code> computed above. In
    general, computational objects may have very complex structures, and it
    would be extremely inconvenient to have to remember and repeat their
    details each time we want to use them. Indeed, complex programs are
    constructed by building, step by step, computational objects of increasing
    complexity. The interpreter makes this step-by-step program construction
    particularly convenient because name-object associations can be created
    incrementally in successive interactions. This feature encourages the
    incremental development and testing of programs and is largely responsible
    for the fact that a Lisp program usually consists of a large number of
    relatively simple procedures.
  </p>
  <p>
    It should be clear that the possibility of associating values with symbols
    and later retrieving them means that the interpreter must maintain some
    sort of memory that keeps track of the name-object pairs. This memory is
    called the <code>environment</code> (more precisely the <code>global
    environment</code>, since we will see later that a computation may involve
    a number of different environments).<sup id="r1.9s"><a href="#r1.9d"
    >9</a></sup>
  </p>
</section>

<section id="s1.1.3">
  <h4><a href="#s1.1.3">1.1.3 Evaluating Combinations</a></h4>
  <p>
    One of our goals in this chapter is to isolate issues about thinking
    procedurally. As a case in point, let us consider that, in evaluating
    combinations, the interpreter is itself following a procedure.
  </p>
  <ul>
    <li>
      To evaluate a combination, do the following:
      <ol>
        <li>
          Evaluate the subexpressions of the combination.
        </li>
        <li>
          Apply the procedure that is the value of the leftmost subexpression
          (the operator) to the arguments that are the values of the other
          subexpressions (the operands).
        </li>
      </ol>
    </li>
  </ul>
  <p>
    Even this simple rule illustrates some important points about processes in
    general. First, observe that the first step dictates that in order to
    accomplish the evaluation process for a combination we must first perform
    the evaluation process on each element of the combination. Thus, the
    evaluation rule is <em>recursive</em> in nature; that is, it includes, as
    one of its steps, the need to invoke the rule itself.<sup id="r1.10s"><a
    href="#r1.10d">10</a></sup>
  </p>
  <p>
    Notice how succinctly the idea of recursion can be used to express what, in
    the case of a deeply nested combination, would otherwise be viewed as a
    rather complicated process. For example, evaluating
  </p>
  <textarea name="1.1.3-01">
(* (+ 2 (* 4 6))
   (+ 3 5 7))</textarea>
  <output for="1.1.3-01" class="block">390</output>
  <p>
    requires that the evaluation rule be applied to four different
    combinations. We can obtain a picture of this process by representing the
    combination in the form of a tree, as shown in figure <a href="#f1.1"
    >1.1</a>. Each combination is represented by a node with branches
    corresponding to the operator and the operands of the combination stemming
    from it. The terminal nodes (that is, nodes with no branches stemming from
    them) represent either operators or numbers. Viewing evaluation in terms of
    the tree, we can imagine that the values of the operands percolate upward,
    starting from the terminal nodes and then combining at higher and higher
    levels. In general, we shall see that recursion is a very powerful
    technique for dealing with hierarchical, treelike objects. In fact, the
    “percolate values upward” form of the evaluation rule is an example of a
    general kind of process known as <em>tree accumulation</em>.
  </p>
  <figure id="f1.1">
    <img alt="Figure 1.1: Tree representation" src="./img/f1.1.gif"
      loading="lazy">
    <figcaption>
      Figure 1.1: Tree representation, showing the value of each
      subcombination.
    </figcaption>
  </figure>
  <p>
    Next, observe that the repeated application of the first step brings us to
    the point where we need to evaluate, not combinations, but primitive
    expressions such as numerals, built-in operators, or other names. We take
    care of the primitive cases by stipulating that
  </p>
  <ul>
    <li>
      the values of numerals are the numbers that they name,
    </li>
    <li>
      the values of built-in operators are the machine instruction sequences
      that carry out the corresponding operations, and
    </li>
    <li>
      the values of other names are the objects associated with those names in
      the environment.
    </li>
  </ul>
  <p>
    We may regard the second rule as a special case of the third one by
    stipulating that symbols such as <code>+</code> and <code>*</code> are also
    included in the global environment, and are associated with the sequences
    of machine instructions that are their “values.” The key point to notice is
    the role of the environment in determining the meaning of the symbols in
    expressions. In an interactive language such as Lisp, it is meaningless to
    speak of the value of an expression such as <code>(+&nbsp;x&nbsp;1)</code>
    without specifying any information about the environment that would provide
    a meaning for the symbol <code>x</code> (or even for the symbol
    <code>+</code>). As we shall see in chapter 3, the general notion of the
    environment as providing a context in which evaluation takes place will
    play an important role in our understanding of program execution.
  </p>
  <p>
    Notice that the evaluation rule given above does not handle definitions.
    For instance, evaluating <code>(define&nbsp;x&nbsp;3)</code> does not apply
    <code>define</code> to two arguments, one of which is the value of the
    symbol <code>x</code> and the other of which is <code>3</code>, since the
    purpose of the <code>define</code> is precisely to associate <code>x</code>
    with a value. (That is, <code>(define&nbsp;x&nbsp;3)</code> is not a
    combination.)
  </p>
  <p>
    Such exceptions to the general evaluation rule are called <em>special
    forms</em>. <code>Define</code> is the only example of a special form that
    we have seen so far, but we will meet others shortly. Each special form has
    its own evaluation rule. The various kinds of expressions (each with its
    associated evaluation rule) constitute the syntax of the programming
    language. In comparison with most other programming languages, Lisp has a
    very simple syntax; that is, the evaluation rule for expressions can be
    described by a simple general rule together with specialized rules for a
    small number of special forms.<sup id="r1.11s"><a href="#r1.11d"
    >11</a></sup>
  </p>
</section>

<section id="s1.1.4">
  <h4><a href="#s1.1.4">1.1.4 Compound Procedures</a></h4>
  <p>
    We have identified in Lisp some of the elements that must appear in any
    powerful programming language:
  </p>
  <ul>
    <li>
      Numbers and arithmetic operations are primitive data and procedures.
    </li>
    <li>
      Nesting of combinations provides a means of combining operations.
    </li>
    <li>
      Definitions that associate names with values provide a limited means of
      abstraction.
    </li>
  </ul>
  <p>
    Now we will learn about <em>procedure definitions</em>, a much more
    powerful abstraction technique by which a compound operation can be given a
    name and then referred to as a unit.
  </p>
  <p>
    We begin by examining how to express the idea of “squaring.” We might say,
    “To square something, multiply it by itself.” This is expressed in our
    language as
  </p>
  <textarea name="1.1.4-01">(define (square x) (* x x))</textarea>
  <output for="1.1.4-01" class="block">square</output>
  <p>
    We can understand this in the following way:
  </p>
  <pre><code>(define (square  x)        (*         x     x))
 ↑         ↑     ↑          ↑         ↑     ↑
To      square something, multiply   it by itself.</code></pre>
  <p>
    We have here a <em>compound procedure</em>, which has been given the name
    <code>square</code>. The procedure represents the operation of multiplying
    something by itself. The thing to be multiplied is given a local name,
    <code>x</code>, which plays the same role that a pronoun plays in natural
    language. Evaluating the definition creates this compound procedure and
    associates it with the name square.<sup id="r1.12s"><a href="#r1.12d"
    >12</a></sup>
  </p>
  <p>
    The general form of a procedure definition is
  </p>
  <pre><code>(define (<em>&lt;name&gt;</em> <em>&lt;formal parameters&gt;</em
    >) <em>&lt;body&gt;</em>)</code></pre>
  <p>
    The <code><em>&lt;name&gt;</em></code> is a symbol to be associated with
    the procedure definition in the environment.<sup id="r1.13s"><a
    href="#r1.13d">13</a></sup> The <code><em>&lt;formal parameters&gt;</em
    ></code> are the names used within the body of the procedure to refer to
    the corresponding arguments of the procedure. The <code><em
    >&lt;body&gt;</em></code> is an expression that will yield the value of the
    procedure application when the formal parameters are replaced by the actual
    arguments to which the procedure is applied.<sup id="r1.14s"><a
    href="#r1.14d">14</a></sup> The <code><em>&lt;name&gt;</em></code> and the
    <code><em>&lt;formal parameters&gt;</em></code> are grouped within
    parentheses, just as they would be in an actual call to the procedure being
    defined.
  </p>
  <p>
    Having defined square, we can now use it:
  </p>
  <textarea name="1.1.4-02" data-extends="1.1.4-01">
(square 21)
(square (+ 2 5))
(square (square 3))</textarea>
  <output for="1.1.4-02" class="block">441
49
81</output>
  <p>
    We can also use <code>square</code> as a building block in defining other
    procedures. For example, <math><msup><mi>x</mi><mn>2</mn></msup><mo
    >+</mo><msup><mi>y</mi><mn>2</mn></msup></math> can be expressed as
  </p>
  <pre><code>(+ (square x) (square y))</code></pre>
  <p>
    We can easily define a procedure <code>sum-of-squares</code> that, given
    any two numbers as arguments, produces the sum of their squares:
  </p>
  <textarea name="1.1.4-03" data-extends="1.1.4-01">
(define (sum-of-squares x y)
  (+ (square x) (square y)))

(sum-of-squares 3 4)</textarea>
  <output for="1.1.4-03" class="block">sum-of-squares
25</output>
  <p>
    Now we can use <code>sum-of-squares</code> as a building block in
    constructing further procedures:
  </p>
  <textarea name="1.1.4-04" data-extends="1.1.4-03">
(define (f a)
  (sum-of-squares (+ a 1) (* a 2)))

(f 5)</textarea>
  <output for="1.1.4-04" class="block">f
136</output>
  <p>
    Compound procedures are used in exactly the same way as primitive
    procedures. Indeed, one could not tell by looking at the definition of
    <code>sum-of-squares</code> given above whether square was built into the
    interpreter, like <code>+</code> and <code>*</code>, or defined as a
    compound procedure.
  </p>
</section>

<section id="s1.1.5">
  <h4><a href="#s1.1.5">1.1.5 The Substitution Model for Procedure
    Application</a></h4>
  <p>
    To evaluate a combination whose operator names a compound procedure, the
    interpreter follows much the same process as for combinations whose
    operators name primitive procedures, which we described in section
    <a href="#s1.1.3">1.1.3</a>. That is, the interpreter evaluates the
    elements of the combination and applies the procedure (which is the value
    of the operator of the combination) to the arguments (which are the values
    of the operands of the combination).
  </p>
  <p>
    We can assume that the mechanism for applying primitive procedures to
    arguments is built into the interpreter. For compound procedures, the
    application process is as follows:
  </p>
  <ul>
    <li>
      To apply a compound procedure to arguments, evaluate the body of the
      procedure with each formal parameter replaced by the corresponding
      argument.
    </li>
  </ul>
  <p>
    To illustrate this process, let’s evaluate the combination
  </p>
  <pre><code>(f 5)</code></pre>
  <p>
    where <code>f</code> is the procedure defined in section <a href="#s1.1.4"
    >1.1.4</a>. We begin by retrieving the body of <code>f</code>:
  </p>
  <pre><code>(sum-of-squares (+ a 1) (* a 2))</code></pre>
  <p>
    Then we replace the formal parameter <code>a</code> by the argument
    <code>5</code>:
  </p>
  <pre><code>(sum-of-squares (+ 5 1) (* 5 2))</code></pre>
  <p>
    Thus the problem reduces to the evaluation of a combination with two
    operands and an operator <code>sum-of-squares</code>. Evaluating this
    combination involves three subproblems. We must evaluate the operator to
    get the procedure to be applied, and we must evaluate the operands to get
    the arguments. Now <code>(+&nbsp;5&nbsp;1)</code> produces <code>6</code>
    and <code>(*&nbsp;5&nbsp;2)</code> produces <code>10</code>, so we must
    apply the <code>sum-of-squares</code> procedure to <code>6</code> and
    <code>10</code>. These values are substituted for the formal parameters
    <code>x</code> and <code>y</code> in the body of
    <code>sum-of-squares</code>, reducing the expression to
  </p>
  <pre><code>(+ (square 6) (square 10))</code></pre>
  <p>
    If we use the definition of <code>square</code>, this reduces to
  </p>
  <pre><code>(+ (* 6 6) (* 10 10))</code></pre>
  <p>
    which reduces by multiplication to
  </p>
  <pre><code>(+ 36 100)</code></pre>
  <p>
    and finally to
  </p>
  <pre><code>136</code></pre>
  <p>
    The process we have just described is called the <em>substitution
    model</em> for procedure application. It can be taken as a model that
    determines the “meaning” of procedure application, insofar as the
    procedures in this chapter are concerned. However, there are two points
    that should be stressed:
  </p>
  <ul>
    <li>
      The purpose of the substitution is to help us think about procedure
      application, not to provide a description of how the interpreter really
      works. Typical interpreters do not evaluate procedure applications by
      manipulating the text of a procedure to substitute values for the formal
      parameters. In practice, the “substitution” is accomplished by using a
      local environment for the formal parameters. We will discuss this more
      fully in chapters 3 and 4 when we examine the implementation of an
      interpreter in detail.
    </li>
    <li>
      Over the course of this book, we will present a sequence of increasingly
      elaborate models of how interpreters work, culminating with a complete
      implementation of an interpreter and compiler in chapter 5. The
      substitution model is only the first of these models—a way to get started
      thinking formally about the evaluation process. In general, when modeling
      phenomena in science and engineering, we begin with simplified,
      incomplete models. As we examine things in greater detail, these simple
      models become inadequate and must be replaced by more refined models. The
      substitution model is no exception. In particular, when we address in
      chapter 3 the use of procedures with “mutable data,” we will see that the
      substitution model breaks down and must be replaced by a more complicated
      model of procedure application.<sup id="r1.15s"><a href="#r1.15d"
      >15</a></sup>
    </li>
  </ul>
  <h5>Applicative order versus normal order</h5>
  <p>
    According to the description of evaluation given in section <a
    href="#s1.1.3">1.1.3</a>, the interpreter first evaluates the operator and
    operands and then applies the resulting procedure to the resulting
    arguments. This is not the only way to perform evaluation. An alternative
    evaluation model would not evaluate the operands until their values were
    needed. Instead it would first substitute operand expressions for
    parameters until it obtained an expression involving only primitive
    operators, and would then perform the evaluation. If we used this method,
    the evaluation of
  </p>
  <pre><code>(f 5)</code></pre>
  <p>
    would proceed according to the sequence of expansions
  </p>
  <pre><code>(sum-of-squares (+ 5 1) (* 5 2))

(+    (square (+ 5 1))      (square (* 5 2))  )

(+    (* (+ 5 1) (+ 5 1))   (* (* 5 2) (* 5 2)))</code></pre>
  <p>
    followed by the reductions
  </p>
  <pre><code>(+         (* 6 6)             (* 10 10))

(+           36                   100)
                    136</code></pre>
  <p>
    This gives the same answer as our previous evaluation model, but the
    process is different. In particular, the evaluations of <code
    >(+&nbsp;5&nbsp;1)</code> and <code>(*&nbsp;5&nbsp;2)</code> are each
    performed twice here, corresponding to the reduction of the expression
  </p>
  <pre><code>(* x x)</code></pre>
  <p>
    with <code>x</code> replaced respectively by <code>(+&nbsp;5&nbsp;1)</code>
    and <code>(*&nbsp;5&nbsp;2)</code>.
  </p>
  <p>
    This alternative “fully expand and then reduce” evaluation method is known
    as <em>normal-order evaluation</em>, in contrast to the “evaluate the
    arguments and then apply” method that the interpreter actually uses, which
    is called applicative-order evaluation. It can be shown that, for procedure
    applications that can be modeled using substitution (including all the
    procedures in the first two chapters of this book) and that yield
    legitimate values, normal-order and applicative-order evaluation produce
    the same value. (See exercise <a href="#e1.5">1.5</a> for an instance of an
    “illegitimate” value where normal-order and applicative-order evaluation do
    not give the same result.)
  </p>
  <p>
    Lisp uses applicative-order evaluation, partly because of the additional
    efficiency obtained from avoiding multiple evaluations of expressions such
    as those illustrated with <code>(+&nbsp;5&nbsp;1)</code> and <code
    >(*&nbsp;5&nbsp;2)</code> above and, more significantly, because
    normal-order evaluation becomes much more complicated to deal with when we
    leave the realm of procedures that can be modeled by substitution. On the
    other hand, normal-order evaluation can be an extremely valuable tool, and
    we will investigate some of its implications in chapters 3 and 4.<sup
    id="r1.16s"><a href="#r1.16d">16</a></sup>
  </p>
</section>

<section id="s1.1.6">
  <h4><a href="#s1.1.6">1.1.6 Conditional Expressions and Predicates</a></h4>
  <p>
    The expressive power of the class of procedures that we can define at this
    point is very limited, because we have no way to make tests and to perform
    different operations depending on the result of a test. For instance, we
    cannot define a procedure that computes the absolute value of a number by
    testing whether the number is positive, negative, or zero and taking
    different actions in the different cases according to the rule
  </p>
  <figure>
    <math display="block">
      <mrow>
        <mrow><mo>|</mo><mi>x</mi><mo>|</mo></mrow><mo>=</mo><mo>&lbrace;</mo>
        <mtable>
          <mtr>
            <mtd><mi>x</mi></mtd>
            <mtd><mtext>if&nbsp;</mtext><mi>x</mi><mo>&gt;</mo><mn>0</mn></mtd>
          </mtr>
          <mtr>
            <mtd><mn>0</mn></mtd>
            <mtd><mtext>if&nbsp;</mtext><mi>x</mi><mo>=</mo><mn>0</mn></mtd>
          </mtr>
          <mtr>
            <mtd><mo>-</mo><mi>x</mi></mtd>
            <mtd><mtext>if&nbsp;</mtext><mi>x</mi><mo>&lt;</mo><mn>0</mn></mtd>
          </mtr>
        </mtable>
      </mrow>
    </math>
  </figure>
  <p>
    This construct is called a <em>case analysis</em>, and there is a special
    form in Lisp for notating such a case analysis. It is called
    <code>cond</code> (which stands for “conditional”), and it is used as
    follows:
  </p>
  <textarea name="1.1.6-01">
(define (abs x)
  (cond ((&gt; x 0) x)
        ((= x 0) 0)
        ((&lt; x 0) (- x))))</textarea>
  <output for="1.1.6-01" class="block">abs</output>
  <p>
    The general form of a conditional expression is
  </p>
  <pre><code>(cond (<em>&lt;p1&gt;</em> <em>&lt;e1&gt;</em>)
      (<em>&lt;p2&gt;</em> <em>&lt;e2&gt;</em>)
      ⋮
      (<em>&lt;pn&gt;</em> <em>&lt;en&gt;</em>))</code></pre>
  <p>
    consisting of the symbol <code>cond</code> followed by parenthesized pairs
    of expressions (<code><em>&lt;p&gt;</em> <em>&lt;e&gt;</em></code>) called
    clauses. The first expression in each pair is a <em>predicate</em>—that is,
    an expression whose value is interpreted as either true or false.<sup
    id="r1.17s"><a href="#r1.17d">17</a></sup>
  </p>
  <p>
    Conditional expressions are evaluated as follows. The predicate
    <code><em>&lt;p1&gt;</em></code> is evaluated first. If its value is false,
    then <code><em>&lt;p2&gt;</em></code> is evaluated. If <code><em
    >&lt;p2&gt;</em></code>’s value is also false, then <code><em
    >&lt;p3&gt;</em></code> is evaluated. This process continues until a
    predicate is found whose value is true, in which case the interpreter
    returns the value of the corresponding <em>consequent expression</em>
    <code><em>&lt;e&gt;</em></code> of the clause as the value of the
    conditional expression. If none of the <code><em>&lt;p&gt;</em></code>’s is
    found to be true, the value of the cond is undefined.
  </p>
  <p>
    The word <em>predicate</em> is used for procedures that return true or
    false, as well as for expressions that evaluate to true or false. The
    absolute-value procedure <code>abs</code> makes use of the primitive
    predicates <code>&gt;</code>, <code>&lt;</code>, and <code>=</code>.<sup
    id="r1.18s"><a href="#r1.18d">18</a></sup> These take two numbers as
    arguments and test whether the first number is, respectively, greater than,
    less than, or equal to the second number, returning true or false
    accordingly.
  </p>
  <p>
    Another way to write the absolute-value procedure is
  </p>
  <textarea name="1.1.6-02">
(define (abs x)
  (cond ((&lt; x 0) (- x))
        (else x)))</textarea>
  <output for="1.1.6-02" class="block">abs</output>
  <p>
    which could be expressed in English as “If <code>x</code> is less than zero
    return <code>-&nbsp;x</code>; otherwise return <code>x</code>.”
    <code>Else</code> is a special symbol that can be used in place of the
    <code><em>&lt;p&gt;</em></code> in the final clause of a <code>cond</code>.
    This causes the <code>cond</code> to return as its value the value of the
    corresponding <code><em>&lt;e&gt;</em></code> whenever all previous clauses
    have been bypassed. In fact, any expression that always evaluates to a true
    value could be used as the <code><em>&lt;p&gt;</em></code> here.
  </p>
  <p>
    Here is yet another way to write the absolute-value procedure:
  </p>
  <textarea name="1.1.6-03">
(define (abs x)
  (if (&lt; x 0)
      (- x)
      x))</textarea>
  <output for="1.1.6-03" class="block">abs</output>
  <p>
    This uses the special form <code>if</code>, a restricted type of
    conditional that can be used when there are precisely two cases in the
    case analysis. The general form of an <code>if</code> expression is
  </p>
  <pre><code>(if <em>&lt;predicate&gt;</em> <em>&lt;consequent&gt;</em> <em
    >&lt;alternative&gt;</em>)</code></pre>
  <p>
    To evaluate an <code>if</code> expression, the interpreter starts by
    evaluating the <code><em>&lt;predicate&gt;</em></code> part of the
    expression. If the <code><em>&lt;predicate&gt;</em></code> evaluates to a
    true value, the interpreter then evaluates the <code><em
    >&lt;consequent&gt;</em></code> and returns its value. Otherwise it
    evaluates the <code><em>&lt;alternative&gt;</em></code> and returns its
    value.<sup id="r1.19s"><a href="#r1.19d">19</a></sup>
  </p>
  <p>
    In addition to primitive predicates such as <code>&lt;</code>,
    <code>=</code>, and <code>&gt;</code>, there are logical composition
    operations, which enable us to construct compound predicates.
    The three most frequently used are these:
  </p>
  <ul>
    <li>
      <code>(and <em>&lt;e1&gt;</em> … <em>&lt;en&gt;</em>)</code> <br>
      The interpreter evaluates the expressions <code><em>&lt;e&gt;</em></code>
      one at a time, in left-to-right order. If any <code><em>&lt;e&gt;</em
      ></code> evaluates to false, the value of the <code>and</code> expression
      is false, and the rest of the <code><em>&lt;e&gt;</em></code>’s are not
      evaluated. If all <code><em>&lt;e&gt;</em></code>’s evaluate to true
      values, the value of the <code>and</code> expression is the value of the
      last one.
    </li>
    <li>
      <code>(or <em>&lt;e1&gt;</em> … <em>&lt;en&gt;</em>)</code> <br>
      The interpreter evaluates the expressions <code><em>&lt;e&gt;</em></code>
      one at a time, in left-to-right order. If any <code><em>&lt;e&gt;</em
      ></code> evaluates to a true value, that value is returned as the value
      of the <code>or</code> expression, and the rest of the <code><em
      >&lt;e&gt;</em></code>’s are not evaluated. If all <code><em
      >&lt;e&gt;</em></code>’s evaluate to false, the value of the
      <code>or</code> expression is false.
    </li>
    <li>
      <code>(not <em>&lt;e&gt;</em>)</code> <br>
      The value of a <code>not</code> expression is true when the expression
      <code><em>&lt;e&gt;</em></code> evaluates to false, and false otherwise.
    </li>
  </ul>
  <p>
    Notice that <code>and</code> and <code>or</code> are special forms, not
    procedures, because the subexpressions are not necessarily all evaluated.
    <code>Not</code> is an ordinary procedure.
  </p>
  <p>
    As an example of how these are used, the condition that a number
    <code>x</code> be in the range <code>5 &lt; x &lt; 10</code> may be
    expressed as
  </p>
  <pre><code>(and (&gt; x 5) (&lt; x 10))</code></pre>
  <p>
    As another example, we can define a predicate to test whether one number is
    greater than or equal to another as
  </p>
  <textarea name="1.1.6-04">
(define (&gt;= x y)
  (or (&gt; x y) (= x y)))</textarea>
  <output for="1.1.6-04" class="block">&gt;=</output>
  <p>
    or alternatively as
  </p>
  <textarea name="1.1.6-05">
(define (&gt;= x y)
  (not (&lt; x y)))</textarea>
  <output for="1.1.6-05" class="block">&gt;=</output>
</section>

<section id="e1.1">
  <h5><a href="#e1.1">Exercise 1.1</a></h5>
  <p>
    Below is a sequence of expressions. What is the result printed by the
    interpreter in response to each expression? Assume that the sequence is to
    be evaluated in the order in which it is presented.
  </p>
  <textarea name="e1.1-01">
10
(+ 5 3 4)
(- 9 1)
(/ 6 2)
(+ (* 2 4) (- 4 6))
(define a 3)
(define b (+ a 1))
(+ a b (* a b))
(= a b)
(if (and (&gt; b a) (&lt; b (* a b)))
    b
    a)
(cond ((= a 4) 6)
      ((= b 4) (+ 6 7 a))
      (else 25))
(+ 2 (if (&gt; b a) b a))
(* (cond ((&gt; a b) a)
         ((&lt; a b) b)
         (else -1))
   (+ a 1))</textarea>
  <output for="e1.1-01" class="block">&ZeroWidthSpace;</output>
</section>

<section id="e1.2">
  <h5><a href="#e1.2">Exercise 1.2</a></h5>
  <p>
    Translate the following expression into prefix form
  </p>
  <figure>
    <math display="block">
      <mfrac>
        <mrow>
          <mn>5</mn><mo>+</mo><mn>4</mn><mo>+</mo>
          <mo>&lpar;</mo><mn>2</mn><mo>-</mo>
            <mo>&lpar;</mo><mn>3</mn><mo>-</mo>
              <mo>&lpar;</mo><mn>6</mn><mo>+</mo>
                <mfrac><mn>4</mn><mn>5</mn></mfrac>
              <mo>&rpar;</mo>
            <mo>&rpar;</mo>
          <mo>&rpar;</mo>
        </mrow>
        <mrow>
          <mn>3</mn>
          <mo>&lpar;</mo><mn>6</mn><mo>-</mo><mn>2</mn><mo>&rpar;</mo>
          <mo>&lpar;</mo><mn>2</mn><mo>-</mo><mn>7</mn><mo>&rpar;</mo>
        </mrow>
      </mfrac>
    </math>
  </figure>
  <textarea name="e1.2-01"></textarea>
  <output for="e1.2-01" class="block">&ZeroWidthSpace;</output>
</section>

<section id="e1.3">
  <h5><a href="#e1.3">Exercise 1.3</a></h5>
  <p>
    Define a procedure that takes three numbers as arguments and returns the
    sum of the squares of the two larger numbers.
  </p>
  <textarea name="e1.3-01"></textarea>
  <output for="e1.3-01" class="block">&ZeroWidthSpace;</output>
</section>

<section id="e1.4">
  <h5><a href="#e1.4">Exercise 1.4</a></h5>
  <p>
    Observe that our model of evaluation allows for combinations whose
    operators are compound expressions. Use this observation to describe the
    behavior of the following procedure:
  </p>
  <textarea name="e1.4-01">
(define (a-plus-abs-b a b)
  ((if (&gt; b 0) + -) a b))</textarea>
  <output for="e1.4-01" class="block">a-plus-abs-b</output>
</section>

<section id="e1.5">
  <h5><a href="#e1.5">Exercise 1.5</a></h5>
  <p>
    Ben Bitdiddle has invented a test to determine whether the interpreter he
    is faced with is using applicative-order evaluation or normal-order
    evaluation. He defines the following two procedures:
  </p>
  <textarea name="e1.5-01">
(define (p) (p))

(define (test x y)
  (if (= x 0)
      0
      y))</textarea>
  <output for="e1.5-01" class="block">p
test</output>
  <p>
    Then he evaluates the expression
  </p>
  <textarea name="e1.5-02" data-extends="e1.5-01">(test 0 (p))</textarea>
  <output for="e1.5-02" class="block">&ZeroWidthSpace;</output>
  <p>
    What behavior will Ben observe with an interpreter that uses
    applicative-order evaluation? What behavior will he observe with an
    interpreter that uses normal-order evaluation? Explain your answer. (Assume
    that the evaluation rule for the special form <code>if</code> is the same
    whether the interpreter is using normal or applicative order: The predicate
    expression is evaluated first, and the result determines whether to
    evaluate the consequent or the alternative expression.)
  </p>
</section>

<section id="s1.1.7">
  <h4><a href="#s1.1.7">1.1.7 Example: Square Roots by Newton’s Method</a></h4>
  <p>
    Procedures, as introduced above, are much like ordinary mathematical
    functions. They specify a value that is determined by one or more
    parameters. But there is an important difference between mathematical
    functions and computer procedures. Procedures must be effective.
  </p>
  <p>
    As a case in point, consider the problem of computing square roots. We can
    define the <code>square-root</code> function as
  </p>
  <figure>
    <math display="block">
      <mrow>
        <msqrt><mi>x</mi></msqrt><mo>=</mo>
        <mtext>the </mtext><mi>y</mi><mtext> such that </mtext>
        <mi>y</mi><mo>&GreaterEqual;</mo><mn>0</mn>
        <mtext> and </mtext>
        <msup><mi>y</mi><mn>2</mn></msup><mo>=</mo><mi>x</mi>
      </mrow>
    </math>
  </figure>
  <p>
    This describes a perfectly legitimate mathematical function. We could use
    it to recognize whether one number is the square root of another, or to
    derive facts about square roots in general. On the other hand, the
    definition does not describe a procedure. Indeed, it tells us almost
    nothing about how to actually find the square root of a given number. It
    will not help matters to rephrase this definition in pseudo-Lisp:
  </p>
  <pre><code>(define (sqrt x)
  (the y (and (&gt;= y 0)
              (= (square y) x))))</code></pre>
  <p>
    This only begs the question.
  </p>
  <p>
    The contrast between function and procedure is a reflection of the general
    distinction between describing properties of things and describing how to
    do things, or, as it is sometimes referred to, the distinction between
    declarative knowledge and imperative knowledge. In mathematics we are
    usually concerned with declarative (what is) descriptions, whereas in
    computer science we are usually concerned with imperative (how to)
    descriptions.<sup id="r1.20s"><a href="#r1.20d">20</a></sup>
  </p>
  <p>
    How does one compute square roots? The most common way is to use Newton’s
    method of successive approximations, which says that whenever we have a
    guess <code>y</code> for the value of the square root of a number
    <code>x</code>, we can perform a simple manipulation to get a better guess
    (one closer to the actual square root) by averaging <code>y</code> with
    <code>x/y</code>.<sup id="r1.21s"><a href="#r1.21d">21</a></sup> For
    example, we can compute the square root of 2 as follows. Suppose our
    initial guess is 1:
  </p>
  <table>
    <thead>
      <tr>
        <th>Guess</th>
        <th>Quotient</th>
        <th>Average</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>1</td>
        <td>(2/1) = 2</td>
        <td>((2 + 1)/2) = 1.5</td>
      </tr>
      <tr>
        <td>1.5</td>
        <td>(2/1.5) = 1.3333</td>
        <td>((1.3333 + 1.5)/2) = 1.4167</td>
      </tr>
      <tr>
        <td>1.4167</td>
        <td>(2/1.4167) = 1.4118</td>
        <td>((1.4167 + 1.4118)/2) = 1.4142</td>
      </tr>
      <tr>
        <td>1.4142</td>
        <td>…</td>
        <td>…</td>
      </tr>
    </tbody>
  </table>
  <p>
    Continuing this process, we obtain better and better approximations to the
    square root.
  </p>
  <p>
    Now let’s formalize the process in terms of procedures. We start with a
    value for the radicand (the number whose square root we are trying to
    compute) and a value for the guess. If the guess is good enough for our
    purposes, we are done; if not, we must repeat the process with an improved
    guess. We write this basic strategy as a procedure:
  </p>
  <textarea name="1.1.7-01" data-extends="1.1.7-02 1.1.7-04">
(define (sqrt-iter guess x)
  (if (good-enough? guess x)
      guess
      (sqrt-iter (improve guess x)
                 x)))</textarea>
  <output for="1.1.7-01" class="block">sqrt-iter</output>
  <p>
    A guess is improved by averaging it with the quotient of the radicand and
    the old guess:
  </p>
  <textarea name="1.1.7-02" data-extends="1.1.7-03">
(define (improve guess x)
  (average guess (/ x guess)))</textarea>
  <output for="1.1.7-02" class="block">improve</output>
  <p>
    where
  </p>
  <textarea name="1.1.7-03">
(define (average x y)
  (/ (+ x y) 2))</textarea>
  <output for="1.1.7-03" class="block">average</output>
  <p>
    We also have to say what we mean by “good enough.” The following will do
    for illustration, but it is not really a very good test. (See exercise <a
    href="#e1.7">1.7</a>.) The idea is to improve the answer until it is close
    enough so that its square differs from the radicand by less than a
    predetermined tolerance (here 0.001):<sup id="r1.22s"><a href="#r1.22d"
    >22</a></sup>
  </p>
  <textarea name="1.1.7-04" data-extends="1.1.4-01 1.1.6-03">
(define (good-enough? guess x)
  (&lt; (abs (- (square guess) x)) 0.001))</textarea>
  <output for="1.1.7-04" class="block">good-enough?</output>
  <p>
    Finally, we need a way to get started. For instance, we can always guess
    that the square root of any number is 1:<sup id="r1.23s"><a href="#r1.23d"
    >23</a></sup>
  </p>
  <textarea name="1.1.7-05" data-extends="1.1.7-01 1.1.7-04">
(define (sqrt x)
  (sqrt-iter 1.0 x))</textarea>
  <output for="1.1.7-05" class="block">sqrt</output>
  <p>
    If we type these definitions to the interpreter, we can use sqrt just as we
    can use any procedure:
  </p>
  <textarea name="1.1.7-06" data-extends="1.1.7-05">
(sqrt 9)
(sqrt (+ 100 37))
(sqrt (+ (sqrt 2) (sqrt 3)))
(square (sqrt 1000))</textarea>
  <output for="1.1.7-06" class="block">3.00009155413138
11.704699917758145
1.7739279023207892
1000.000369924366</output>
  <p>
    The <code>sqrt</code> program also illustrates that the simple procedural
    language we have introduced so far is sufficient for writing any purely
    numerical program that one could write in, say, C or Pascal. This might
    seem surprising, since we have not included in our language any iterative
    (looping) constructs that direct the computer to do something over and over
    again. <code>Sqrt-iter</code>, on the other hand, demonstrates how
    iteration can be accomplished using no special construct other than the
    ordinary ability to call a procedure.<sup id="r1.24s"><a href="#r1.24d"
    >24</a></sup>
  </p>
</section>

<section id="e1.6">
  <h5><a href="#e1.6">Exercise 1.6</a></h5>
  <p>
    Alyssa P. Hacker doesn’t see why <code>if</code> needs to be provided as a
    special form. “Why can’t I just define it as an ordinary procedure in terms
    of <code>cond</code>?” she asks. Alyssa’s friend Eva Lu Ator claims this
    can indeed be done, and she defines a new version of <code>if</code>:
  </p>
  <textarea name="e1.6-01">
(define (new-if predicate then-clause else-clause)
  (cond (predicate then-clause)
        (else else-clause)))</textarea>
  <output for="e1.6-01" class="block">new-if</output>
  <p>
    Eva demonstrates the program for Alyssa:
  </p>
  <textarea name="e1.6-02" data-extends="e1.6-01">
(new-if (= 2 3) 0 5)

(new-if (= 1 1) 0 5)</textarea>
<output for="e1.6-02" class="block">5
0</output>
  <p>
    Delighted, Alyssa uses <code>new-if</code> to rewrite the
    <code>square-root</code> program:
  </p>
  <textarea name="e1.6-03" data-extends="1.1.7-02 1.1.7-04 e1.6-01">
(define (sqrt-iter guess x)
  (new-if (good-enough? guess x)
          guess
          (sqrt-iter (improve guess x)
                     x)))</textarea>
<output for="e1.6-03" class="block">sqrt-iter</output>
  <p>
    What happens when Alyssa attempts to use this to compute square roots?
    Explain.
  </p>
</section>

<section id="e1.7">
  <h5><a href="#e1.7">Exercise 1.7</a></h5>
  <p>
    The <code>good-enough?</code> test used in computing square roots will not
    be very effective for finding the square roots of very small numbers. Also,
    in real computers, arithmetic operations are almost always performed with
    limited precision. This makes our test inadequate for very large numbers.
    Explain these statements, with examples showing how the test fails for
    small and large numbers. An alternative strategy for implementing
    <code>good-enough?</code> is to watch how guess changes from one iteration
    to the next and to stop when the change is a very small fraction of the
    guess. Design a <code>square-root</code> procedure that uses this kind of
    end test. Does this work better for small and large numbers?
  </p>
  <textarea name="e1.7-01" data-extends="1.1.4-01 1.1.6-03 1.1.7-02">
(define (sqrt-iter guess x)
  ???)
(define (good-enough? ???)
  ???)
(define (sqrt x)
  (sqrt-iter 1.0 x))
(sqrt 9)</textarea>
  <output for="e1.7-01" class="block">&ZeroWidthSpace;</output>
</section>

<section id="e1.8">
  <h5><a href="#e1.8">Exercise 1.8</a></h5>
  <p>
    Newton’s method for cube roots is based on the fact that if <code>y</code>
    is an approximation to the cube root of <code>x</code>, then a better
    approximation is given by the value
  </p>
  <figure>
    <math display="block">
      <mfrac>
        <mrow><mi>x</mi><mo>/</mo><msup><mi>y</mi><mn>2</mn></msup><mo>+</mo>
          <mn>2</mn><mi>y</mi></mrow>
        <mrow><mn>3</mn></mrow>
      </mfrac>
    </math>
  </figure>
  <p>
    Use this formula to implement a <code>cube-root</code> procedure analogous
    to the <code>square-root</code> procedure. (In section <a href="#s1.3.4"
    >1.3.4</a> we will see how to implement Newton’s method in general as an
    abstraction of these <code>square-root</code> and <code>cube-root</code>
    procedures.)
  </p>
  <textarea name="e1.8-01" data-extends="1.1.6-03 1.1.7-03"></textarea>
  <output for="e1.8-01" class="block">&ZeroWidthSpace;</output>
</section>

<section id="s1.1.8">
  <h4><a href="#s1.1.8">1.1.8 Procedures as Black-Box Abstractions</a></h4>
  <p>
    <code>Sqrt</code> is our first example of a process defined by a set of
    mutually defined procedures. Notice that the definition of
    <code>sqrt-iter</code> is recursive; that is, the procedure is defined in
    terms of itself. The idea of being able to define a procedure in terms of
    itself may be disturbing; it may seem unclear how such a “circular”
    definition could make sense at all, much less specify a well-defined
    process to be carried out by a computer. This will be addressed more
    carefully in section <a href="#s1.2">1.2</a>. But first let’s consider some
    other important points illustrated by the <code>sqrt</code> example.
  </p>
  <p>
    Observe that the problem of computing square roots breaks up naturally into
    a number of subproblems: how to tell whether a guess is good enough, how to
    improve a guess, and so on. Each of these tasks is accomplished by a
    separate procedure. The entire <code>sqrt</code> program can be viewed as a
    cluster of procedures (shown in figure <a href="#f1.2">1.2</a>) that
    mirrors the decomposition of the problem into subproblems.
  </p>
  <figure>
    <img alt="Figure 1.2: Procedural decomposition of the sqrt program"
      src="./img/f1.2.gif" loading="lazy">
    <figcaption>
      Figure 1.2: Procedural decomposition of the <code>sqrt</code> program.
    </figcaption>
  </figure>
  <p>
    The importance of this decomposition strategy is not simply that one is
    dividing the program into parts. After all, we could take any large program
    and divide it into parts—the first ten lines, the next ten lines, the next
    ten lines, and so on. Rather, it is crucial that each procedure
    accomplishes an identifiable task that can be used as a module in defining
    other procedures. For example, when we define the <code>good-enough?</code>
    procedure in terms of <code>square</code>, we are able to regard the
    <code>square</code> procedure as a “black box.” We are not at that moment
    concerned with how the procedure computes its result, only with the fact
    that it computes the square. The details of how the square is computed can
    be suppressed, to be considered at a later time. Indeed, as far as
    the <code>good-enough?</code> procedure is concerned, <code>square</code>
    is not quite a procedure but rather an abstraction of a procedure, a
    so-called procedural abstraction. At this level of abstraction, any
    procedure that computes the square is equally good.
  </p>
  <p>
    Thus, considering only the values they return, the following two procedures
    for squaring a number should be indistinguishable. Each takes a numerical
    argument and produces the square of that number as the value.<sup
    id="r1.25s"><a href="#r1.25d">25</a></sup>
  </p>
  <pre><code>(define (square x) (* x x))

(define (square x) 
  (exp (double (log x))))

(define (double x) (+ x x))</code></pre>
  <p>
    So a procedure definition should be able to suppress detail. The users of
    the procedure may not have written the procedure themselves, but may have
    obtained it from another programmer as a black box. A user should not need
    to know how the procedure is implemented in order to use it.
  </p>
  <h5>Local names</h5>
  <p>
    One detail of a procedure’s implementation that should not matter to the
    user of the procedure is the implementer’s choice of names for the
    procedure’s formal parameters. Thus, the following procedures should not be
    distinguishable:
  </p>
  <pre><code>(define (square x) (* x x))

(define (square y) (* y y))</code></pre>
  <p>
    This principle—that the meaning of a procedure should be independent of the
    parameter names used by its author—seems on the surface to be self-evident,
    but its consequences are profound. The simplest consequence is that the
    parameter names of a procedure must be local to the body of the procedure.
    For example, we used <code>square</code> in the definition of
    <code>good-enough?</code> in our <code>square-root</code> procedure:
  </p>
  <pre><code>(define (good-enough? guess x)
  (&lt; (abs (- (square guess) x)) 0.001))</code></pre>
  <p>
    The intention of the author of <code>good-enough?</code> is to determine if
    the square of the first argument is within a given tolerance of the second
    argument. We see that the author of <code>good-enough?</code> used the name
    <code>guess</code> to refer to the first argument and <code>x</code> to
    refer to the second argument. The argument of <code>square</code> is
    <code>guess</code>. If the author of <code>square</code> used
    <code>x</code> (as above) to refer to that argument, we see that the
    <code>x</code> in <code>good-enough?</code> must be a different
    <code>x</code> than the one in <code>square</code>. Running the procedure
    <code>square</code> must not affect the value of <code>x</code> that is
    used by <code>good-enough?</code>, because that value of <code>x</code> may
    be needed by <code>good-enough?</code> after <code>square</code> is done
    computing.
  </p>
  <p>
    If the parameters were not local to the bodies of their respective
    procedures, then the parameter <code>x</code> in <code>square</code> could
    be confused with the parameter <code>x</code> in <code>good-enough?</code>,
    and the behavior of <code>good-enough?</code> would depend upon which
    version of <code>square</code> we used. Thus, <code>square</code> would not
    be the black box we desired.
  </p>
  <p>
    A formal parameter of a procedure has a very special role in the procedure
    definition, in that it doesn’t matter what name the formal parameter has.
    Such a name is called a <em>bound variable</em>, and we say that the
    procedure definition <em>binds</em> its formal parameters. The meaning of a
    procedure definition is unchanged if a bound variable is consistently
    renamed throughout the definition.<sup id="r1.26s"><a href="#r1.26d"
    >26</a></sup> If a variable is not bound, we say that it is <em>free</em>.
    The set of expressions for which a binding defines a name is called the
    <em>scope</em> of that name. In a procedure definition, the bound variables
    declared as the formal parameters of the procedure have the body of the
    procedure as their scope.
  </p>
  <p>
    In the definition of <code>good-enough?</code> above, <code>guess</code>
    and <code>x</code> are bound variables but <code>&lt;</code>,
    <code>-</code>, <code>abs</code>, and <code>square</code> are free. The
    meaning of <code>good-enough?</code> should be independent of the names we
    choose for <code>guess</code> and <code>x</code> so long as they are
    distinct and different from <code>&lt;</code>, <code>-</code>,
    <code>abs</code>, and <code>square</code>. (If we renamed
    <code>guess</code> to <code>abs</code> we would have introduced a bug by
    <em>capturing</em> the variable <code>abs</code>. It would have changed
    from free to bound.) The meaning of <code>good-enough?</code> is not
    independent of the names of its free variables, however. It surely depends
    upon the fact (external to this definition) that the symbol
    <code>abs</code> names a procedure for computing the absolute value of a
    number. <code>Good-enough?</code> will compute a different function if we
    substitute <code>cos</code> for <code>abs</code> in its definition.
  </p>
  <h5>Internal definitions and block structure</h5>
  <p>
    We have one kind of name isolation available to us so far: The formal
    parameters of a procedure are local to the body of the procedure. The
    <code>square-root</code> program illustrates another way in which we would
    like to control the use of names. The existing program consists of separate
    procedures:
  </p>
  <textarea name="1.1.8-01" data-extends="1.1.4-01 1.1.6-03 1.1.7-03">
(define (sqrt x)
  (sqrt-iter 1.0 x))
(define (sqrt-iter guess x)
  (if (good-enough? guess x)
      guess
      (sqrt-iter (improve guess x) x)))
(define (good-enough? guess x)
  (&lt; (abs (- (square guess) x)) 0.001))
(define (improve guess x)
  (average guess (/ x guess)))</textarea>
  <output for="1.1.8-01" class="block">sqrt
sqrt-iter
good-enough?
improve</output>
  <p>
    The problem with this program is that the only procedure that is important
    to users of <code>sqrt</code> is <code>sqrt</code>. The other procedures
    (<code>sqrt-iter</code>, <code>good-enough?</code>, and
    <code>improve</code>) only clutter up their minds. They may not define any
    other procedure called <code>good-enough?</code> as part of another program
    to work together with the <code>square-root</code> program, because
    <code>sqrt</code> needs it. The problem is especially severe in the
    construction of large systems by many separate programmers. For example, in
    the construction of a large library of numerical procedures, many numerical
    functions are computed as successive approximations and thus might have
    procedures named <code>good-enough?</code> and <code>improve</code> as
    auxiliary procedures. We would like to localize the subprocedures, hiding
    them inside <code>sqrt</code> so that <code>sqrt</code> could coexist with
    other successive approximations, each having its own private
    <code>good-enough?</code> procedure. To make this possible, we allow a
    procedure to have internal definitions that are local to that procedure.
    For example, in the <code>square-root</code> problem we can write
  </p>
  <textarea name="1.1.8-02" data-extends="1.1.4-01 1.1.6-03 1.1.7-03">
(define (sqrt x)
  (define (good-enough? guess x)
    (&lt; (abs (- (square guess) x)) 0.001))
  (define (improve guess x)
    (average guess (/ x guess)))
  (define (sqrt-iter guess x)
    (if (good-enough? guess x)
        guess
        (sqrt-iter (improve guess x) x)))
  (sqrt-iter 1.0 x))</textarea>
  <output for="1.1.8-02" class="block">sqrt</output>
  <p>
    Such nesting of definitions, called <em>block structure</em>, is basically
    the right solution to the simplest name-packaging problem. But there is a
    better idea lurking here. In addition to internalizing the definitions of
    the auxiliary procedures, we can simplify them. Since <code>x</code> is
    bound in the definition of <code>sqrt</code>, the procedures
    <code>good-enough?</code>, <code>improve</code>, and
    <code>sqrt-iter</code>, which are defined internally to <code>sqrt</code>,
    are in the scope of <code>x</code>. Thus, it is not necessary to pass
    <code>x</code> explicitly to each of these procedures. Instead, we allow
    <code>x</code> to be a free variable in the internal definitions, as shown
    below. Then <code>x</code> gets its value from the argument with which the
    enclosing procedure <code>sqrt</code> is called. This discipline is called
    <em>lexical scoping</em>.<sup id="r1.27s"><a href="#r1.27d">27</a></sup>
  </p>
  <textarea name="1.1.8-03" data-extends="1.1.4-01 1.1.6-03 1.1.7-03">
(define (sqrt x)
  (define (good-enough? guess)
    (&lt; (abs (- (square guess) x)) 0.001))
  (define (improve guess)
    (average guess (/ x guess)))
  (define (sqrt-iter guess)
    (if (good-enough? guess)
        guess
        (sqrt-iter (improve guess))))
  (sqrt-iter 1.0))</textarea>
  <output for="1.1.8-03" class="block">sqrt</output>
  <p>
    We will use block structure extensively to help us break up large programs
    into tractable pieces.<sup id="r1.28s"><a href="#r1.28d">28</a></sup> The
    idea of block structure originated with the programming language Algol 60.
    It appears in most advanced programming languages and is an important tool
    for helping to organize the construction of large programs.
  </p>
</section>
<hr>
<footer>
  <p>
    <sup id="r1.4d"><a href="#r1.4s">4</a></sup> The characterization of
    numbers as “simple data” is a barefaced bluff. In fact, the treatment of
    numbers is one of the trickiest and most confusing aspects of any
    programming language. Some typical issues involved are these: Some computer
    systems distinguish <em>integers</em>, such as 2, from <em>real
    numbers</em>, such as 2.71. Is the real number 2.00 different from the
    integer 2? Are the arithmetic operations used for integers the same as the
    operations used for real numbers? Does 6 divided by 2 produce 3, or 3.0?
    How large a number can we represent? How many decimal places of accuracy
    can we represent? Is the range of integers the same as the range of real
    numbers? Above and beyond these questions, of course, lies a collection of
    issues concerning roundoff and truncation errors—the entire science of
    numerical analysis. Since our focus in this book is on large-scale program
    design rather than on numerical techniques, we are going to ignore these
    problems. The numerical examples in this chapter will exhibit the usual
    roundoff behavior that one observes when using arithmetic operations that
    preserve a limited number of decimal places of accuracy in noninteger
    operations.
  </p>
  <p>
    <sup id="r1.5d"><a href="#r1.5s">5</a></sup> Throughout this book, when we
    wish to emphasize the distinction between the input typed by the user and
    the response printed by the interpreter, we will show the latter in slanted
    characters.
  </p>
  <p>
    <sup id="r1.6d"><a href="#r1.6s">6</a></sup> Lisp systems typically provide
    features to aid the user in formatting expressions. Two especially useful
    features are one that automatically indents to the proper pretty-print
    position whenever a new line is started and one that highlights the
    matching left parenthesis whenever a right parenthesis is typed.
  </p>
  <p>
    <sup id="r1.7d"><a href="#r1.7s">7</a></sup> Lisp obeys the convention that
    every expression has a value. This convention, together with the old
    reputation of Lisp as an inefficient language, is the source of the quip by
    Alan Perlis (paraphrasing Oscar Wilde) that “Lisp programmers know the
    value of everything but the cost of nothing.”
  </p>
  <p>
    <sup id="r1.8d"><a href="#r1.8s">8</a></sup> In this book, we do not show
    the interpreter’s response to evaluating definitions, since this is highly
    implementation-dependent.
  </p>
  <p>
    <sup id="r1.9d"><a href="#r1.9s">9</a></sup> Chapter 3 will show that this
    notion of environment is crucial, both for understanding how the
    interpreter works and for implementing interpreters.
  </p>
  <p>
    <sup id="r1.10d"><a href="#r1.10s">10</a></sup> It may seem strange that
    the evaluation rule says, as part of the first step, that we should
    evaluate the leftmost element of a combination, since at this point that
    can only be an operator such as <code>+</code> or <code>*</code>
    representing a built-in primitive procedure such as addition or
    multiplication. We will see later that it is useful to be able to work with
    combinations whose operators are themselves compound expressions.
  </p>
  <p>
    <sup id="r1.11d"><a href="#r1.11s">11</a></sup> Special syntactic forms
    that are simply convenient alternative surface structures for things that
    can be written in more uniform ways are sometimes called <em>syntactic
    sugar</em>, to use a phrase coined by Peter Landin. In comparison with
    users of other languages, Lisp programmers, as a rule, are less concerned
    with matters of syntax. (By contrast, examine any Pascal manual and notice
    how much of it is devoted to descriptions of syntax.) This disdain for
    syntax is due partly to the flexibility of Lisp, which makes it easy to
    change surface syntax, and partly to the observation that many “convenient”
    syntactic constructs, which make the language less uniform, end up causing
    more trouble than they are worth when programs become large and complex. In
    the words of Alan Perlis, “Syntactic sugar causes cancer of the semicolon.”
  </p>
  <p>
    <sup id="r1.12d"><a href="#r1.12s">12</a></sup> Observe that there are two
    different operations being combined here: we are creating the procedure,
    and we are giving it the name <code>square</code>. It is possible, indeed
    important, to be able to separate these two notions—to create procedures
    without naming them, and to give names to procedures that have already been
    created. We will see how to do this in section <a href="#s1.3.2">1.3.2</a>.
  </p>
  <p>
    <sup id="r1.13d"><a href="#r1.13s">13</a></sup> Throughout this book, we
    will describe the general syntax of expressions by using italic symbols
    delimited by angle brackets—e.g., <code><em>&lt;name&gt;</em></code>—to
    denote the “slots” in the expression to be filled in when such an
    expression is actually used.
  </p>
  <p>
    <sup id="r1.14d"><a href="#r1.14s">14</a></sup> More generally, the body of
    the procedure can be a sequence of expressions. In this case, the
    interpreter evaluates each expression in the sequence in turn and returns
    the value of the final expression as the value of the procedure
    application.
  </p>
  <p>
    <sup id="r1.15d"><a href="#r1.15s">15</a></sup> Despite the simplicity of
    the substitution idea, it turns out to be surprisingly complicated to give
    a rigorous mathematical definition of the substitution process. The problem
    arises from the possibility of confusion between the names used for the
    formal parameters of a procedure and the (possibly identical) names used in
    the expressions to which the procedure may be applied. Indeed, there is a
    long history of erroneous definitions of substitution in the literature of
    logic and programming semantics. See Stoy 1977 for a careful discussion of
    substitution.
  </p>
  <p>
    <sup id="r1.16d"><a href="#r1.16s">16</a></sup> In chapter 3 we will
    introduce <em>stream processing</em>, which is a way of handling apparently
    “infinite” data structures by incorporating a limited form of normal-order
    evaluation. In section <a href="#s4.2">4.2</a> we will modify the Scheme
    interpreter to produce a normal-order variant of Scheme.
  </p>
  <p>
    <sup id="r1.17d"><a href="#r1.17s">17</a></sup> “Interpreted as either true
    or false” means this: In Scheme, there are two distinguished values that
    are denoted by the constants <code>#t</code> and <code>#f</code>. When the
    interpreter checks a predicate’s value, it interprets <code>#f</code> as
    false. Any other value is treated as true. (Thus, providing <code>#t</code>
    is logically unnecessary, but it is convenient.) In this book we will use
    names true and false, which are associated with the values <code>#t</code>
    and <code>#f</code> respectively.
  </p>
  <p>
    <sup id="r1.18d"><a href="#r1.18s">18</a></sup> Abs also uses the “minus”
    operator <code>-</code>, which, when used with a single operand, as in
    <code>(-&nbsp;x)</code>, indicates negation.
  </p>
  <p>
    <sup id="r1.19d"><a href="#r1.19s">19</a></sup> A minor difference between
    <code>if</code> and <code>cond</code> is that the <code><em>&lt;e&gt;</em
    ></code> part of each cond clause may be a sequence of expressions. If the
    corresponding <code><em>&lt;p&gt;</em></code> is found to be true, the
    expressions <code><em>&lt;e&gt;</em></code> are evaluated in sequence and
    the value of the final expression in the sequence is returned as the value
    of the <code>cond</code>. In an <code>if</code> expression, however, the
    <code><em>&lt;consequent&gt;</em></code> and <code><em
    >&lt;alternative&gt;</em></code> must be single expressions.
  </p>
  <p>
    <sup id="r1.20d"><a href="#r1.20s">20</a></sup> Declarative and imperative
    descriptions are intimately related, as indeed are mathematics and computer
    science. For instance, to say that the answer produced by a program is
    “correct” is to make a declarative statement about the program. There is a
    large amount of research aimed at establishing techniques for proving that
    programs are correct, and much of the technical difficulty of this subject
    has to do with negotiating the transition between imperative statements
    (from which programs are constructed) and declarative statements (which can
    be used to deduce things). In a related vein, an important current area in
    programming-language design is the exploration of so-called very high-level
    languages, in which one actually programs in terms of declarative
    statements. The idea is to make interpreters sophisticated enough so that,
    given “what is” knowledge specified by the programmer, they can generate
    “how to” knowledge automatically. This cannot be done in general, but there
    are important areas where progress has been made. We shall revisit this
    idea in chapter 4.
  </p>
  <p>
    <sup id="r1.21d"><a href="#r1.21s">21</a></sup> This square-root algorithm
    is actually a special case of Newton’s method, which is a general technique
    for finding roots of equations. The square-root algorithm itself was
    developed by Heron of Alexandria in the first century A.D. We will see how
    to express the general Newton’s method as a Lisp procedure in section <a
    href="#s1.3.4">1.3.4</a>.
  </p>
  <p>
    <sup id="r1.22d"><a href="#r1.22s">22</a></sup> We will usually give
    predicates names ending with question marks, to help us remember that they
    are predicates. This is just a stylistic convention. As far as the
    interpreter is concerned, the question mark is just an ordinary character.
  </p>
  <p>
    <sup id="r1.23d"><a href="#r1.23s">23</a></sup> Observe that we express our
    initial guess as 1.0 rather than 1. This would not make any difference in
    many Lisp implementations. MIT Scheme, however, distinguishes between exact
    integers and decimal values, and dividing two integers produces a rational
    number rather than a decimal. For example, dividing 10 by 6 yields 5/3,
    while dividing 10.0 by 6.0 yields 1.6666666666666667. (We will learn how to
    implement arithmetic on rational numbers in section <a href="#s2.1.1"
    >2.1.1</a>.) If we start with an initial guess of 1 in our square-root
    program, and x is an exact integer, all subsequent values produced in the
    square-root computation will be rational numbers rather than decimals.
    Mixed operations on rational numbers and decimals always yield decimals, so
    starting with an initial guess of 1.0 forces all subsequent values to be
    decimals.
  </p>
  <p>
    <sup id="r1.24d"><a href="#r1.24s">24</a></sup> Readers who are worried
    about the efficiency issues involved in using procedure calls to implement
    iteration should note the remarks on “tail recursion” in section <a
    href="#s1.2.1">1.2.1</a>.
  </p>
  <p>
    <sup id="r1.25d"><a href="#r1.25s">25</a></sup> It is not even clear which
    of these procedures is a more efficient implementation. This depends upon
    the hardware available. There are machines for which the “obvious”
    implementation is the less efficient one. Consider a machine that has
    extensive tables of logarithms and antilogarithms stored in a very
    efficient manner.
  </p>
  <p>
    <sup id="r1.26d"><a href="#r1.26s">26</a></sup> The concept of consistent
    renaming is actually subtle and difficult to define formally. Famous
    logicians have made embarrassing errors here.
  </p>
  <p>
    <sup id="r1.27d"><a href="#r1.27s">27</a></sup> Lexical scoping dictates
    that free variables in a procedure are taken to refer to bindings made by
    enclosing procedure definitions; that is, they are looked up in the
    environment in which the procedure was defined. We will see how this works
    in detail in chapter 3 when we study environments and the detailed behavior
    of the interpreter.
  </p>
  <p>
    <sup id="r1.28d"><a href="#r1.28s">28</a></sup> Embedded definitions must
    come first in a procedure body. The management is not responsible for the
    consequences of running programs that intertwine definition and use.
  </p>
</footer>
</section>
</section>

<section id="s1.2">
<section id="s1.2.0">
  <h3><a href="#s1.2">1.2 Procedures and the Processes They Generate</a></h3>
  <p>
    We have now considered the elements of programming: We have used primitive
    arithmetic operations, we have combined these operations, and we have
    abstracted these composite operations by defining them as compound
    procedures. But that is not enough to enable us to say that we know how to
    program. Our situation is analogous to that of someone who has learned the
    rules for how the pieces move in chess but knows nothing of typical
    openings, tactics, or strategy. Like the novice chess player, we don’t yet
    know the common patterns of usage in the domain. We lack the knowledge of
    which moves are worth making (which procedures are worth defining). We lack
    the experience to predict the consequences of making a move (executing a
    procedure).
  </p>
  <p>
    The ability to visualize the consequences of the actions under
    consideration is crucial to becoming an expert programmer, just as it is in
    any synthetic, creative activity. In becoming an expert photographer, for
    example, one must learn how to look at a scene and know how dark each
    region will appear on a print for each possible choice of exposure and
    development conditions. Only then can one reason backward, planning
    framing, lighting, exposure, and development to obtain the desired effects.
    So it is with programming, where we are planning the course of action to be
    taken by a process and where we control the process by means of a program.
    To become experts, we must learn to visualize the processes generated by
    various types of procedures. Only after we have developed such a skill can
    we learn to reliably construct programs that exhibit the desired behavior.
  </p>
  <p>
    A procedure is a pattern for the <em>local evolution</em> of a
    computational process. It specifies how each stage of the process is built
    upon the previous stage. We would like to be able to make statements about
    the overall, or <em>global</em>, behavior of a process whose local
    evolution has been specified by a procedure. This is very difficult to do
    in general, but we can at least try to describe some typical patterns of
    process evolution.
  </p>
  <p>
    In this section we will examine some common “shapes” for processes
    generated by simple procedures. We will also investigate the rates at which
    these processes consume the important computational resources of time and
    space. The procedures we will consider are very simple. Their role is like
    that played by test patterns in photography: as oversimplified prototypical
    patterns, rather than practical examples in their own right.
  </p>
</section>

<section id="s1.2.1">
  <h4><a href="#s1.2.1">1.2.1 Linear Recursion and Iteration</a></h4>
  <figure id="f1.3">
    <img alt="Figure 1.3: A linear recursive process" src="./img/f1.3.gif"
      loading="lazy">
    <figcaption>
      Figure 1.3: A linear recursive process for computing 6!
    </figcaption>
  </figure>
  <p>
    We begin by considering the factorial function, defined by
  </p>
  <figure>
    <math display="block">
      <mrow>
        <mi>n</mi><mo>!</mo><mo>=</mo>
        <mi>n</mi><mo>⋅</mo>
        <mo>&lpar;</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo>&rpar;</mo><mo>⋅</mo>
        <mo>&lpar;</mo><mi>n</mi><mo>-</mo><mn>2</mn><mo>&rpar;</mo>
        <mo>⋅⋅⋅</mo><mn>3</mn><mo>⋅</mo><mn>2</mn><mo>⋅</mo><mn>1</mn>
      </mrow>
    </math>
  </figure>
  <p>
    There are many ways to compute factorials. One way is to make use of the
    observation that <math><mi>n</mi><mo>!</mo></math> is equal to
    <math>n</math> times <math><mo>&lpar;</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo
    >&rpar;</mo><mo>!</mo></math> for any positive integer
    <math><mi>n</mi></math>:
  </p>
  <figure>
    <math display="block">
      <mrow>
        <mi>n</mi><mo>!</mo><mo>=</mo>
        <mi>n</mi><mo>⋅</mo><mo>&lbrack;</mo>
        <mo>&lpar;</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo>&rpar;</mo><mo>⋅</mo>
        <mo>&lpar;</mo><mi>n</mi><mo>-</mo><mn>2</mn><mo>&rpar;</mo>
        <mo>⋅⋅⋅</mo><mn>3</mn><mo>⋅</mo><mn>2</mn><mo>⋅</mo><mn>1</mn>
        <mo>&rbrack;</mo><mo>=</mo>
        <mi>n</mi><mo>⋅</mo>
        <mo>&lpar;</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo>&rpar;</mo><mo>!</mo>
      </mrow>
    </math>
  </figure>
  <p>
    Thus, we can compute <math><mi>n</mi><mo>!</mo></math> by computing
    <math><mo>&lpar;</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo>&rpar;</mo><mo
    >!</mo></math> and multiplying the result by <math><mi>n</mi></math>. If we
    add the stipulation that <math><mn>1</mn><mo>!</mo></math> is equal to
    <math><mn>1</mn></math>, this observation translates directly into a
    procedure:
  </p>
  <textarea name="1.2.1-01">
(define (factorial n)
  (if (= n 1)
      1
      (* n (factorial (- n 1)))))</textarea>
  <output for="1.2.1-01" class="block">factorial</output>
  </p>
  <p>
    We can use the substitution model of section <a href="#s1.1.5">1.1.5</a> to
    watch this procedure in action computing <math><mn>6</mn><mo>!</mo></math>,
    as shown in figure <a href="#f1.3">1.3</a>.
  </p>
  <p>
    Now let’s take a different perspective on computing factorials. We could
    describe a rule for computing <math><mi>n</mi><mo>!</mo></math> by
    specifying that we first multiply <math><mn>1</mn></math> by
    <math><mn>2</mn></math>, then multiply the result by
    <math><mn>3</mn></math>, then by <math><mn>4</mn></math>, and so on until
    we reach <math><mi>n</mi></math>. More formally, we maintain a running
    product, together with a counter that counts from <math><mn>1</mn></math>
    up to <math><mi>n</mi></math>. We can describe the computation by saying
    that the counter and the product simultaneously change from one step to the
    next according to the rule
  </p>
  <figure>
    <math display="block">
      <mtable>
        <mtr>
          <mtd><mtext>product</mtext><mo>&larr;</mo><mtext>counter</mtext><mo
            >⋅</mo><mtext>product</mtext></mtd>
        </mtr>
        <mtr>
          <mtd><mtext>counter</mtext><mo>&larr;</mo><mtext>counter</mtext><mo
            >+</mo><mn>1</mn></mtd>
        </mtr>
      </mtable>
    </math>
  </figure>
  <p>
    and stipulating that <math><mi>n</mi><mo>!</mo></math> is the value of the
    product when the counter exceeds <math><mi>n</mi></math>.
  </p>
  <figure id="f1.4">
    <img alt="Figure 1.4: A linear iterative process" src="./img/f1.4.gif"
      loading="lazy">
    <figcaption>
      Figure 1.4: A linear iterative process for computing <math><mn>6</mn><mo
      >!</mo></math>.
    </figcaption>
  </figure>
  <p>
    Once again, we can recast our description as a procedure for computing
    factorials:<sup id="r1.29s"><a href="#r1.29d">29</a></sup>
  </p>
  <textarea name="1.2.1-02">
(define (factorial n)
  (fact-iter 1 1 n))

(define (fact-iter product counter max-count)
  (if (> counter max-count)
      product
      (fact-iter (* counter product)
                 (+ counter 1)
                 max-count)))</textarea>
  <output for="1.2.1-02" class="block">factorial
fact-iter</output>
  <p>
    As before, we can use the substitution model to visualize the process of
    computing <math><mn>6</mn><mo>!</mo></math>, as shown in figure
    <a href="#f1.4">1.4</a>.
  </p>
  <p>
    Compare the two processes. From one point of view, they seem hardly
    different at all. Both compute the same mathematical function on the same
    domain, and each requires a number of steps proportional to
    <math><mi>n</mi></math> to compute <math><mi>n</mi><mo>!</mo></math>.
    Indeed, both processes even carry out the same sequence of multiplications,
    obtaining the same sequence of partial products. On the other hand, when we
    consider the “shapes” of the two processes, we find that they evolve quite
    differently.
  </p>
  <p>
    Consider the first process. The substitution model reveals a shape of
    expansion followed by contraction, indicated by the arrow in figure <a
    href="#f1.3">1.3</a>. The expansion occurs as the process builds up a chain
    of <em>deferred operations</em> (in this case, a chain of multiplications).
    The contraction occurs as the operations are actually performed. This type
    of process, characterized by a chain of deferred operations, is called a
    <em>recursive process</em>. Carrying out this process requires that the
    interpreter keep track of the operations to be performed later on. In the
    computation of <math><mi>n</mi><mo>!</mo></math>, the length of the chain
    of deferred multiplications, and hence the amount of information needed to
    keep track of it, grows linearly with <math><mi>n</mi></math> (is
    proportional to <math><mi>n</mi></math>), just like the number of steps.
    Such a process is called a <em>linear recursive process</em>.
  </p>
  <p>
    By contrast, the second process does not grow and shrink. At each step, all
    we need to keep track of, for any <math><mi>n</mi></math>, are the current
    values of the variables <code>product</code>, <code>counter</code>, and
    <code>max-count</code>. We call this an <em>iterative process</em>. In
    general, an iterative process is one whose state can be summarized by a
    fixed number of <em>state variables</em>, together with a fixed rule that
    describes how the state variables should be updated as the process moves
    from state to state and an (optional) end test that specifies conditions
    under which the process should terminate. In computing
    <math><mi>n</mi><mo>!</mo></math>, the number of steps required grows
    linearly with <math><mi>n</mi></math>. Such a process is called a
    <em>linear iterative process</em>.
  </p>
  <p>
    The contrast between the two processes can be seen in another way. In the
    iterative case, the program variables provide a complete description of the
    state of the process at any point. If we stopped the computation between
    steps, all we would need to do to resume the computation is to supply the
    interpreter with the values of the three program variables. Not so with the
    recursive process. In this case there is some additional “hidden”
    information, maintained by the interpreter and not contained in the program
    variables, which indicates “where the process is” in negotiating the chain
    of deferred operations. The longer the chain, the more information must be
    maintained.<sup id="r1.30s"><a href="#r1.30d">30</a></sup>
  </p>
  <p>
    In contrasting iteration and recursion, we must be careful not to confuse
    the notion of a recursive <em>process</em> with the notion of a recursive
    <em>procedure</em>. When we describe a procedure as recursive, we are
    referring to the syntactic fact that the procedure definition refers
    (either directly or indirectly) to the procedure itself. But when we
    describe a process as following a pattern that is, say, linearly recursive,
    we are speaking about how the process evolves, not about the syntax of how
    a procedure is written. It may seem disturbing that we refer to a recursive
    procedure such as <code>fact-iter</code> as generating an iterative
    process. However, the process really is iterative: Its state is captured
    completely by its three state variables, and an interpreter need keep track
    of only three variables in order to execute the process.
  </p>
  <p>
    One reason that the distinction between process and procedure may be
    confusing is that most implementations of common languages (including Ada,
    Pascal, and C) are designed in such a way that the interpretation of any
    recursive procedure consumes an amount of memory that grows with the number
    of procedure calls, even when the process described is, in principle,
    iterative. As a consequence, these languages can describe iterative
    processes only by resorting to special-purpose “looping constructs” such as
    do, repeat, until, for, and while. The implementation of Scheme we shall
    consider in chapter 5 does not share this defect. It will execute an
    iterative process in constant space, even if the iterative process is
    described by a recursive procedure. An implementation with this property is
    called tail-recursive. With a tail-recursive implementation, iteration can
    be expressed using the ordinary procedure call mechanism, so that special
    iteration constructs are useful only as syntactic sugar.<sup id="r1.31s"><a
    href="#r1.31d">31</a></sup>
  </p>
</section>

<section id="e1.9">
  <h5><a href="#e1.9">Exercise 1.9</a></h5>
  <p>
    Each of the following two procedures defines a method for adding two
    positive integers in terms of the procedures <code>inc</code>, which
    increments its argument by <code>1</code>, and <code>dec</code>, which
    decrements its argument by <code>1</code>.
  </p>
  <textarea name="e1.9-01">
(define (+ a b)
  (if (= a 0)
      b
      (inc (+ (dec a) b))))</textarea>
  <output for="e1.9-01" class="block">+</output>
  <textarea name="e1.9-02">
(define (+ a b)
  (if (= a 0)
      b
      (+ (dec a) (inc b))))</textarea>
  <output for="e1.9-02" class="block">+</output>
  <p>
    Using the substitution model, illustrate the process generated by each
    procedure in evaluating <code>(+ 4 5)</code>. Are these processes iterative
    or recursive?
  </p>
</section>

<section id="e1.10">
  <h5><a href="#e1.10">Exercise 1.10</a></h5>
  <p>
    The following procedure computes a mathematical function called Ackermann’s
    function.
  </p>
  <textarea name="e1.10-01">
(define (A x y)
  (cond ((= y 0) 0)
        ((= x 0) (* 2 y))
        ((= y 1) 2)
        (else (A (- x 1)
                 (A x (- y 1))))))</textarea>
  <output for="e1.10-01" class="block">A</output>
  <p>
    What are the values of the following expressions?
  </p>
  <textarea name="e1.10-02" data-extends="e1.10-01">
(A 1 10)
(A 2 4)
(A 3 3)</textarea>
  <output for="e1.10-02" class="block">&ZeroWidthSpace;</output>
  <p>
    Consider the following procedures, where <code>A</code> is the procedure
    defined above:
  </p>
  <textarea name="e1.10-03" data-extends="e1.10-01">
(define (f n) (A 0 n))
(define (g n) (A 1 n))
(define (h n) (A 2 n))
(define (k n) (* 5 n n))</textarea>
  <output for="e1.10-03" class="block">f
g
h
k</output>
  <p>
    Give concise mathematical definitions for the functions computed by the
    procedures <code>f</code>, <code>g</code>, and <code>h</code> for positive
    integer values of <math><mi>n</mi></math>. For example, <code>(k n)</code>
    computes <math><mn>5</mn><msup><mi>n</mi><mn>2</mn></msup></math>.
  </p>
</section>

<section id="s1.2.2">
  <h4><a href="#s1.2.2">1.2.2 Tree Recursion</a></h4>
  <p>
    Another common pattern of computation is called <em>tree recursion</em>. As
    an example, consider computing the sequence of Fibonacci numbers, in which
    each number is the sum of the preceding two:
  </p>
  <figure>
    <math display="block">
      <mrow>
        <mn>0</mn><mtext>, </mtext><mn>1</mn><mtext>, </mtext><mn>1</mn>
        <mtext>, </mtext><mn>2</mn><mtext>, </mtext><mn>3</mn>
        <mtext>, </mtext><mn>5</mn><mtext>, </mtext><mn>8</mn>
        <mtext>, </mtext><mn>13</mn><mtext>, </mtext><mn>21</mn>
        <mtext>, </mtext><mn>…</mn>
      </mrow>
    </math>
  </figure>
  <p>
    In general, the Fibonacci numbers can be defined by the rule
  </p>
  <figure>
    <math display="block">
      <mrow>
        <mrow>
          <mtext>Fib</mtext>
          <mo>&lpar;</mo>
          <mi>n</mi>
          <mo>&rpar;</mo>
        </mrow>
        <mo>=</mo>
        <mo>&lbrace;</mo>
        <mtable>
          <mtr>
            <mtd><mn>0</mn></mtd>
            <mtd><mtext>if&nbsp;</mtext><mi>n</mi><mo>=</mo><mn>0</mn></mtd>
          </mtr>
          <mtr>
            <mtd><mn>1</mn></mtd>
            <mtd><mtext>if&nbsp;</mtext><mi>n</mi><mo>=</mo><mn>1</mn></mtd>
          </mtr>
          <mtr>
            <mtd>
              <mtext>Fib</mtext><mo>&lpar;</mo><mi>n</mi><mo>-</mo><mn>1</mn>
              <mo>&rpar;</mo><mo>+</mo><mtext>Fib</mtext><mo>&lpar;</mo>
              <mi>n</mi><mo>-</mo><mn>2</mn><mo>&rpar;</mo>
            </mtd>
            <mtd><mtext>otherwise</mtext></mtd>
          </mtr>
        </mtable>
      </mrow>
    </math>
  </figure>
  <p>
    We can immediately translate this definition into a recursive procedure for
    computing Fibonacci numbers:
  </p>
  <textarea name="1.2.2-01">
(define (fib n)
  (cond ((= n 0) 0)
        ((= n 1) 1)
        (else (+ (fib (- n 1))
                 (fib (- n 2))))))</textarea>
  <output for="1.2.2-01" class="block">fib</output>
  <figure id="f1.5">
    <img alt="Figure 1.5: A tree-recursive process" src="./img/f1.5.gif"
      loading="lazy">
    <figcaption>
      Figure 1.5: The tree-recursive process generated in computing
      <code>(fib&nbsp;5)</code>.
    </figcaption>
  </figure>
  <p>
    Consider the pattern of this computation. To compute <code>(fib 5)</code>,
    we compute <code>(fib 4)</code> and <code>(fib 3)</code>. To compute
    <code>(fib 4)</code>, we compute <code>(fib 3)</code> and <code>(fib
    2)</code>. In general, the evolved process looks like a tree, as shown in
    figure <a href="#f1.5">1.5</a>. Notice that the branches split into two at
    each level (except at the bottom); this reflects the fact that the
    <code>fib</code> procedure calls itself twice each time it is invoked.
  </p>
  <p>
    This procedure is instructive as a prototypical tree recursion, but it is a
    terrible way to compute Fibonacci numbers because it does so much redundant
    computation. Notice in figure <a href="#f1.5">1.5</a> that the entire
    computation of <code>(fib 3)</code>—almost half the work—is duplicated. In
    fact, it is not hard to show that the number of times the procedure will
    compute <code>(fib 1)</code> or <code>(fib 0)</code> (the number of leaves
    in the above tree, in general) is precisely <math><mtext>Fib</mtext><mo
    >&lpar;</mo><mi>n</mi><mo>+</mo><mn>1</mn><mo>&rpar;</mo></math>. To get an
    idea of how bad this is, one can show that the value of <math><mtext
    >Fib</mtext><mo>&lpar;</mo><mi>n</mi><mo>&rpar;</mo></math> grows
    exponentially with <math><mi>n</mi></math>. More precisely (see exercise
    <a href="#e1.13">1.13</a>), <math><mtext>Fib</mtext><mo>&lpar;</mo><mi
    >n</mi><mo>&rpar;</mo></math> is the closest integer to <math><msup><mi
    >ϕ</mi><mn>5</mn></msup><mo>/</mo><msqrt><mn>5</mn></msqrt></math>, where
  </p>
  <figure>
    <math display="block">
      <mrow>
        <mi>ϕ</mi><mo>=</mo>
        <mfrac>
          <mrow><mn>1</mn><mo>+</mo><msqrt><mn>5</mn></msqrt></mrow>
          <mn>2</mn>
        </mfrac>
        <mo>≈</mo><mn>1.6180</mn>
      </mrow>
    </math>
  </figure>
  <p>
    is the <em>golden ratio</em>, which satisfies the equation
  </p>
  <figure>
    <math display="block">
      <mrow>
        <msup><mi>ϕ</mi><mn>2</mn></msup><mo>=</mo>
        <mi>ϕ</mi><mo>+</mo><mn>1</mn>
      </mrow>
    </math>
  </figure>
  <p>
    Thus, the process uses a number of steps that grows exponentially with the
    input. On the other hand, the space required grows only linearly with the
    input, because we need keep track only of which nodes are above us in the
    tree at any point in the computation. In general, the number of steps
    required by a tree-recursive process will be proportional to the number of
    nodes in the tree, while the space required will be proportional to the
    maximum depth of the tree.
  </p>
  <p>
    We can also formulate an iterative process for computing the Fibonacci
    numbers. The idea is to use a pair of integers <math><mi>a</mi></math> and
    <math><mi>b</mi></math>, initialized to <math><mtext>Fib</mtext><mo
    >&lpar;</mo><mn>1</mn><mo>&rpar;</mo><mo>=</mo><mn>1</mn></math> and
    <math><mtext>Fib</mtext><mo>&lpar;</mo><mn>0</mn><mo>&rpar;</mo><mo
    >=</mo><mn>0</mn></math>, and to repeatedly apply the simultaneous
    transformations
  </p>
  <figure>
    <math display="block">
      <mtable>
        <mtr>
          <mtd><mi>a</mi><mo>&larr;</mo><mi>a</mi><mo>+</mo><mi>b</mi></mtd>
        </mtr>
        <mtr>
          <mtd><mi>b</mi><mo>&larr;</mo><mi>a</mi></mtd>
        </mtr>
      </mtable>
    </math>
  </figure>
  <p>
    It is not hard to show that, after applying this transformation <math><mi
    >n</mi></math> times, <math><mi>a</mi></math> and <math><mi>b</mi></math>
    will be equal, respectively, to <math><mtext>Fib</mtext><mo>&lpar;</mo><mi
    >n</mi><mo>+</mo><mn>1</mn><mo>&rpar;</mo></math> and <math><mtext
    >Fib</mtext><mo>&lpar;</mo><mi>n</mi><mo>&rpar;</mo></math>. Thus, we can
    compute Fibonacci numbers iteratively using the procedure
  </p>
  <textarea name="1.2.2-02">
(define (fib n)
  (fib-iter 1 0 n))

(define (fib-iter a b count)
  (if (= count 0)
      b
      (fib-iter (+ a b) a (- count 1))))</textarea>
  <output for="1.2.2-02" class="block">fib
fib-iter</output>
  <p>
    This second method for computing <math><mtext>Fib</mtext><mo>&lpar;</mo><mi
    >n</mi><mo>&rpar;</mo></math> is a linear iteration. The difference in
    number of steps required by the two methods—one linear in
    <math><mi>n</mi></math>, one growing as fast as <math><mtext
    >Fib</mtext><mo>&lpar;</mo><mi>n</mi><mo>&rpar;</mo></math> itself—is
    enormous, even for small inputs.
  </p>
  <p>
    One should not conclude from this that tree-recursive processes are
    useless. When we consider processes that operate on hierarchically
    structured data rather than numbers, we will find that tree recursion is a
    natural and powerful tool.<sup id="r1.32s"><a href="#r1.32d">32</a></sup>
    But even in numerical operations, tree-recursive processes can be useful in
    helping us to understand and design programs. For instance, although the
    first <code>fib</code> procedure is much less efficient than the second
    one, it is more straightforward, being little more than a translation into
    Lisp of the definition of the Fibonacci sequence. To formulate the
    iterative algorithm required noticing that the computation could be recast
    as an iteration with three state variables.
  </p>
</section>

<section id="ex1.1">
  <h5><a href="#ex1.1">Example: Counting change</a></h5>
  <p>
    It takes only a bit of cleverness to come up with the iterative Fibonacci
    algorithm. In contrast, consider the following problem: How many different
    ways can we make change of $1.00, given half-dollars, quarters, dimes,
    nickels, and pennies? More generally, can we write a procedure to compute
    the number of ways to change any given amount of money?
  </p>
  <p>
    This problem has a simple solution as a recursive procedure. Suppose we
    think of the types of coins available as arranged in some order. Then the
    following relation holds:
  </p>
  <p>
    The number of ways to change amount <math><mi>a</mi></math> using
    <math><mi>n</mi></math> kinds of coins equals
  </p>
  <ul>
    <li>
      the number of ways to change amount a using all but the first kind of
      coin, plus
    </li>
    <li>
      the number of ways to change amount <math><mi>a</mi><mo>-</mo><mi
      >d</mi></math> using all <math><mi>n</mi></math> kinds of coins, where
      <math><mi>d</mi></math> is the denomination of the first kind of coin.
    </li>
  </ul>
  <p>
    To see why this is true, observe that the ways to make change can be
    divided into two groups: those that do not use any of the first kind of
    coin, and those that do. Therefore, the total number of ways to make change
    for some amount is equal to the number of ways to make change for the
    amount without using any of the first kind of coin, plus the number of ways
    to make change assuming that we do use the first kind of coin. But the
    latter number is equal to the number of ways to make change for the amount
    that remains after using a coin of the first kind.
  </p>
  <p>
    Thus, we can recursively reduce the problem of changing a given amount to
    the problem of changing smaller amounts using fewer kinds of coins.
    Consider this reduction rule carefully, and convince yourself that we can
    use it to describe an algorithm if we specify the following degenerate
    cases:<sup id="r1.33s"><a href="#r1.33d">33</a></sup>
  </p>
  <ul>
    <li>
      If <math><mi>a</mi></math> is exactly <math><mn>0</mn></math>, we should
      count that as <math><mn>1</mn></math> way to make change.
    </li>
    <li>
      If <math><mi>a</mi></math> is less than <math><mn>0</mn></math>, we
      should count that as <math><mn>0</mn></math> ways to make change.
    </li>
    <li>
      If <math><mi>n</mi></math> is <math><mn>0</mn></math>, we should count
      that as <math><mn>0</mn></math> ways to make change.
    </li>
  </ul>
  <p>
    We can easily translate this description into a recursive procedure:
  </p>
  <textarea name="ex1.1-01">
(define (count-change amount)
  (cc amount 5))
(define (cc amount kinds-of-coins)
  (cond ((= amount 0) 1)
        ((or (&lt; amount 0) (= kinds-of-coins 0)) 0)
        (else (+ (cc amount
                     (- kinds-of-coins 1))
                 (cc (- amount
                        (first-denomination kinds-of-coins))
                     kinds-of-coins)))))
(define (first-denomination kinds-of-coins)
  (cond ((= kinds-of-coins 1) 1)
        ((= kinds-of-coins 2) 5)
        ((= kinds-of-coins 3) 10)
        ((= kinds-of-coins 4) 25)
        ((= kinds-of-coins 5) 50)))</textarea>
  <output for="ex1.1-01" class="block">count-change
cc
first-denomination</output>
  <p>
    (The <code>first-denomination</code> procedure takes as input the number of
    kinds of coins available and returns the denomination of the first kind.
    Here we are thinking of the coins as arranged in order from largest to
    smallest, but any order would do as well.) We can now answer our original
    question about changing a dollar:
  </p>
  <textarea name="ex1.1-02" data-extends="ex1.1-01">
(count-change 100)</textarea>
  <output for="ex1.1-02" class="block">292</output>
  <p>
    <code>Count-change</code> generates a tree-recursive process with
    redundancies similar to those in our first implementation of
    <code>fib</code>. (It will take quite a while for that 292 to be computed.)
    On the other hand, it is not obvious how to design a better algorithm for
    computing the result, and we leave this problem as a challenge. The
    observation that a tree-recursive process may be highly inefficient but
    often easy to specify and understand has led people to propose that one
    could get the best of both worlds by designing a “smart compiler” that
    could transform tree-recursive procedures into more efficient procedures
    that compute the same result.<sup id="r1.34s"><a href="#r1.34d"
    >34</a></sup>
  </p>
</section>

<section id="e1.11">
  <h5><a href="#e1.11">Exercise 1.11</a></h5>
  <p>
    A function <math><mi>f</mi></math> is defined by the rule that <math><mi
    >f</mi><mo>&lpar;</mo><mi>n</mi><mo>&rpar;</mo><mo>=</mo><mi>n</mi></math>
    if <math><mi>n</mi><mo>&lt;</mo><mn>3</mn></math> and <math><mi>f</mi><mo
    >&lpar;</mo><mi>n</mi><mo>&rpar;</mo><mo>=</mo><mi>f</mi><mo>&lpar;</mo><mi
    >n</mi><mo>-</mo><mn>1</mn><mo>&rpar;</mo><mo>+</mo><mn>2</mn><mi>f</mi><mo
    >&lpar;</mo><mi>n</mi><mo>-</mo><mn>2</mn><mo>&rpar;</mo><mo>+</mo><mn
    >3</mn><mi>f</mi><mo>&lpar;</mo><mi>n</mi><mo>-</mo><mn>3</mn><mo
    >&rpar;</mo></math> if <math><mi>n</mi><mo>&gt;</mo><mn>3</mn></math>.
    Write a procedure that computes <math><mi>f</mi></math> by means of a
    recursive process. Write a procedure that computes <math><mi>f</mi></math>
    by means of an iterative process.
  </p>
  <textarea name="e1.11-01"></textarea>
  <output for="e1.11-01" class="block">&nbsp;</output>
</section>

<section id="e1.12">
  <h5><a href="#e1.2">Exercise 1.12</a></h5>
  <p>
    The following pattern of numbers is called <em>Pascal’s triangle</em>.
  </p>
  <figure>
    <math display="block">
      <mtable>
        <mtr>
          <mtd><mn>1</mn></mtd>
        </mtr>
        <mtr>
          <mtd><mn>1</mn>&emsp;<mn>1</mn></mtd>
        </mtr>
        <mtr>
          <mtd><mn>1</mn>&emsp;<mn>2</mn>&emsp;<mn>1</mn></mtd>
        </mtr>
        <mtr>
          <mtd><mn>1</mn>&emsp;<mn>3</mn>&emsp;<mn>3</mn>&emsp;<mn>1</mn></mtd>
        </mtr>
        <mtr>
          <mtd><mn>1</mn>&emsp;<mn>4</mn>&emsp;<mn>6</mn>&emsp;<mn
          >4</mn>&emsp;<mn>1</mn></mtd>
        </mtr>
        <mtr>
          <mtd>…</mtd>
        </mtr>
      </mtable>
    </math>
  </figure>
  <p>
    The numbers at the edge of the triangle are all 1, and each number inside
    the triangle is the sum of the two numbers above it.<sup id="r1.35s"><a
    href="#r1.35d">35</a></sup> Write a procedure that computes elements of
    Pascal’s triangle by means of a recursive process.
  </p>
  <textarea name="e1.12-01"></textarea>
  <output for="e1.12-01" class="block">&nbsp;</output>
</section>

<section id="e1.13">
  <h5><a href="#e1.13">Exercise 1.13</a></h5>
  <p>
    Prove that <math><mtext>Fib</mtext><mo>&lpar;</mo><mi>n</mi><mo
    >&rpar;</mo></math> is the closest integer to <math><msup><mi>ϕ</mi><mi
    >n</mi></msup><mo>/</mo><msqrt><mn>5</mn></msqrt></math>, where <math><mi
    >ϕ</mi><mo>=</mo><mfrac><mrow><mn>1</mn><mo>+</mo><msqrt><mn
    >5</mn></msqrt></mrow><mn>2</mn></mfrac></math>. Hint: Let <math><mi
    >ψ</mi><mo>=</mo><mfrac><mrow><mn>1</mn><mo>-</mo><msqrt><mn
    >5</mn></msqrt></mrow><mn>2</mn></mfrac></math>. Use induction and the
    definition of the Fibonnaci numbers (see section <a href="#s1.2.2"
    >1.2.2</a>) to prove that <math><mtext>Fib</mtext><mo>&lpar;</mo><mi
    >n</mi><mo>&rpar;</mo><mo>=</mo><mfrac><mrow><msup><mi>ϕ</mi><mi
    >n</mi></msup><mo>-</mo><msup><mi>ψ</mi><mi>n</mi></msup></mrow><msqrt><mn
    >5</mn></msqrt></mfrac></math>.
  </p>
</section>

<section id="s1.2.3">
  <h4><a href="#s1.2.3">1.2.3 Orders of Growth</a></h4>
  <p>
    The previous examples illustrate that processes can differ considerably in
    the rates at which they consume computational resources. One convenient way
    to describe this difference is to use the notion of <em>order of
    growth</em> to obtain a gross measure of the resources required by a
    process as the inputs become larger.
  </p>
  <p>
    Let <math><mi>n</mi></math> be a parameter that measures the size of the
    problem, and let <math><mi>R</mi><mo>&lpar;</mo><mi>n</mi><mo
    >&rpar;</mo></math> be the amount of resources the process requires for a
    problem of size <math><mi>n</mi></math>. In our previous examples we took
    <math><mi>n</mi></math> to be the number for which a given function is to
    be computed, but there are other possibilities. For instance, if our goal
    is to compute an approximation to the square root of a number, we might
    take <math><mi>n</mi></math> to be the number of digits accuracy required.
    For matrix multiplication we might take <math><mi>n</mi></math> to be the
    number of rows in the matrices. In general there are a number of properties
    of the problem with respect to which it will be desirable to analyze a
    given process. Similarly, <math><mi>R</mi><mo>&lpar;</mo><mi>n</mi><mo
    >&rpar;</mo></math> might measure the number of internal storage registers
    used, the number of elementary machine operations performed, and so on. In
    computers that do only a fixed number of operations at a time, the time
    required will be proportional to the number of elementary machine
    operations performed.
  </p>
  <p>
    We say that <math><mi>R</mi><mo>&lpar;</mo><mi>n</mi><mo
    >&rpar;</mo></math> has order of growth <math><mi>θ</mi><mo>&lpar;</mo><mi
    >f</mi><mo>&lpar;</mo><mi>n</mi><mo >&rpar;</mo><mo>&rpar;</mo></math>,
    written <math><mi>R</mi><mo>&lpar;</mo><mi>n</mi><mo>&rpar;</mo><mo
    >=</mo><mi>θ</mi><mo>&lpar;</mo><mi>f</mi><mo>&lpar;</mo><mi>n</mi><mo
    >&rpar;</mo><mo>&rpar;</mo></math>(pronounced “theta of <math><mi>f</mi><mo
    >&lpar;</mo><mi>n</mi><mo>&rpar;</mo></math>”), if there are positive
    constants <math><msub><mi>k</mi><mn>1</mn></msub></math> and <math><msub
    ><mi>k</mi><mn>2</mn></msub></math> independent of <math><mi>n</mi></math>
    such that
  </p>
  <figure>
    <math display="block">
      <mrow>
        <msub><mi>k</mi><mn>1</mn></msub><mi>f</mi><mo>&lpar;</mo><mi>n</mi><mo
        >&rpar;</mo><mo>&LessSlantEqual;</mo><mi>R</mi><mo>&lpar;</mo><mi
        >n</mi><mo>&rpar;</mo><mo>&LessSlantEqual;</mo><msub><mi>k</mi><mn
        >2</mn></msub><mi>f</mi><mo>&lpar;</mo><mi>n</mi><mo>&rpar;</mo>
      </mrow>
    </math>
  </figure>
  <p>
    for any sufficiently large value of <math><mi>n</mi></math>. (In other
    words, for large <math><mi>n</mi></math>, the value <math><mi>R</mi><mo
    >&lpar;</mo><mi>n</mi><mo>&rpar;</mo></math> is sandwiched between <math
    ><msub><mi>k</mi><mn>1</mn></msub><mi>f</mi><mo>&lpar;</mo><mi>n</mi><mo
    >&rpar;</mo></math> and <math><msub><mi>k</mi><mn>2</mn></msub><mi
    >f</mi><mo>&lpar;</mo><mi>n</mi><mo>&rpar;</mo></math>.)
  </p>
  <p>
    For instance, with the linear recursive process for computing factorial
    described in section <a href="#s1.2.1">1.2.1</a> the number of steps grows
    proportionally to the input <math><mi>n</mi></math>. Thus, the steps
    required for this process grows as <math><mi>θ</mi><mo>&lpar;</mo><mi
    >n</mi><mo>&rpar;</mo></math>. We also saw that the space required grows as
    <math><mi>θ</mi><mo>&lpar;</mo><mi>n</mi><mo>&rpar;</mo></math>. For the
    iterative factorial, the number of steps is still <math><mi>θ</mi><mo
    >&lpar;</mo><mi>n</mi><mo>&rpar;</mo></math> but the space is <math><mi
    >θ</mi><mo>&lpar;</mo><mn>1</mn><mo>&rpar;</mo></math>—that is,
    constant.<sup id="r1.36s"><a href="#r1.36d">36</a></sup> The tree-recursive
    Fibonacci computation requires <math><mi>θ</mi><mo>&lpar;</mo><msup><mi
    >ϕ</mi><mi>n</mi></msup><mo>&rpar;</mo></math> steps and space <math><mi
    >θ</mi><mo>&lpar;</mo><mi>n</mi><mo>&rpar;</mo></math>, where <math><mi
    >ϕ</mi></math> is the golden ratio described in section <a href="#s1.2.2"
    >1.2.2</a>.
  </p>
  <p>
    Orders of growth provide only a crude description of the behavior of a
    process. For example, a process requiring <math><msup><mi>n</mi><mn
    >2</mn></msup></math> steps and a process requiring <math><mn
    >1000</mn><msup><mi>n</mi><mn>2</mn></msup></math> steps and a process
    requiring <math><mn>3</mn><msup><mi>n</mi><mn>2</mn></msup><mo>+</mo><mn
    >10</mn><mi>n</mi><mo>+</mo><mn>17</mn></math> steps all have <math><mi
    >θ</mi><mo>&lpar;</mo><msup><mi>n</mi><mn>2</mn></msup><mo
    >&rpar;</mo></math> order of growth. On the other hand, order of growth
    provides a useful indication of how we may expect the behavior of the
    process to change as we change the size of the problem. For a <math><mi
    >θ</mi><mo>&lpar;</mo><mi>n</mi><mo>&rpar;</mo></math> (linear) process,
    doubling the size will roughly double the amount of resources used. For an
    exponential process, each increment in problem size will multiply the
    resource utilization by a constant factor. In the remainder of section
    <a href="#s1.2">1.2</a> we will examine two algorithms whose order of
    growth is logarithmic, so that doubling the problem size increases the
    resource requirement by a constant amount.
  </p>
</section>

<section id="e1.14">
  <h5><a href="#e1.14">Exercise 1.14</a></h5>
  <p>
    Draw the tree illustrating the process generated by the
    <code>count-change</code> procedure of section <a href="#s1.2.2">1.2.2</a>
    in making change for 11 cents. What are the orders of growth of the space
    and number of steps used by this process as the amount to be changed
    increases?
  </p>
</section>

<section id="e1.15">
  <h5><a href="#e1.15">Exercise 1.15</a></h5>
  <p>
    The sine of an angle (specified in radians) can be computed by making use
    of the approximation <math><mtext>sin </mtext><mi>x</mi><mo>≈</mo><mi
    >x</mi></math>if <math><mi>x</mi></math> is sufficiently small, and the
    trigonometric identity
  </p>
  <figure>
    <math display="block">
      <mrow>
        <mtext>sin </mtext><mi>x</mi><mo>=</mo><mn>3</mn><mtext>sin </mtext>
        <mfrac><mi>x</mi><mn>3</mn></mfrac><mo>-</mo><mn>4</mn><msup><mtext
        >sin</mtext><mn>3</mn></msup><mfrac><mi>x</mi><mn>3</mn></mfrac>
      </mrow>
    </math>
  </figure>
  <p>
    to reduce the size of the argument of <math><mtext>sin</mtext></math>. (For
    purposes of this exercise an angle is considered “sufficiently small” if
    its magnitude is not greater than 0.1 radians.) These ideas are
    incorporated in the following procedures:
  </p>
  <textarea name="e1.15-01">
(define (cube x) (* x x x))
(define (p x) (- (* 3 x) (* 4 (cube x))))
(define (sine angle)
   (if (not (> (abs angle) 0.1))
       angle
       (p (sine (/ angle 3.0)))))
  </textarea>
  <output for="e1.15-01" class="block">cube
p
sine</output>
  <ol type="a">
    <li>How many times is the procedure <code>p</code> applied when
      <code>(sine&nbsp;12.15)</code> is evaluated?</li>
    <li>
      What is the order of growth in space and number of steps (as a function
      of <math><mi>a</mi></math>) used by the process generated by the
      <code>sine</code> procedure when <code>(sine&nbsp;a)</code> is evaluated?
    </li>
  </ol>
</section>

<section id="s1.2.4">
  <h4><a href="#s1.2.4">1.2.4 Exponentiation</a></h4>
  <p>
    Consider the problem of computing the exponential of a given number. We
    would like a procedure that takes as arguments a base <math><mi
    >b</mi></math> and a positive integer exponent <math><mi>n</mi></math> and
    computes <math><msup><mi>b</mi><mi>n</mi></msup></math>. One way to do this
    is via the recursive definition
  </p>
  <figure>
    <math display="block">
      <mtable>
        <mtr>
          <mtd><msup><mi>b</mi><mi>n</mi></msup><mo>=</mo><mi>b</mi><mo
            >⋅</mo><msup><mi>b</mi><mrow><mi>n</mi><mo>-</mo><mn
            >1</mn></mrow></msup></mtd>
        </mtr>
        <mtr>
          <mtd><msup><mi>b</mi><mn>0</mn></msup><mo>=</mo><mn>1</mn></mtd>
        </mtr>
      </mtable>
    </math>
  </figure>
  <p>
    which translates readily into the procedure
  </p>
  <textarea name="1.2.4-01">
(define (expt b n)
  (if (= n 0)
      1
      (* b (expt b (- n 1)))))</textarea>
  <output for="1.2.4-01" class="block">expt</output>
  <p>
    This is a linear recursive process, which requires <math><mi>θ</mi><mo
    >&lpar;</mo><mi>n</mi><mo>&rpar;</mo></math> steps and <math><mi>θ</mi><mo
    >&lpar;</mo><mi>n</mi><mo>&rpar;</mo></math> space. Just as with factorial,
    we can readily formulate an equivalent linear iteration:
  </p>
  <textarea name="1.2.4-02">
(define (expt b n)
  (expt-iter b n 1))

(define (expt-iter b counter product)
  (if (= counter 0)
      product
      (expt-iter b
                 (- counter 1)
                 (* b product))))
</textarea>
  <output for="1.2.4-02" class="block">expt
expt-iter</output>
  <p>
    This version requires <math><mi>θ</mi><mo>&lpar;</mo><mi>n</mi><mo
    >&rpar;</mo></math> steps and <math><mi>θ</mi><mo>&lpar;</mo><mn>1</mn><mo
    >&rpar;</mo></math> space.
  </p>
  <p>
    We can compute exponentials in fewer steps by using successive squaring.
    For instance, rather than computing <math><msup><mi>b</mi><mn
    >8</mn></msup></math> as
  </p>
  <figure>
    <math display="block">
      <mrow>
        <mi>b</mi><mo>⋅</mo><mo>&lpar;</mo><mi>b</mi><mo>⋅</mo><mo
        >&lpar;</mo><mi>b</mi><mo>⋅</mo><mo>&lpar;</mo><mi>b</mi><mo>⋅</mo><mo
        >&lpar;</mo><mi>b</mi><mo>⋅</mo><mo>&lpar;</mo><mi>b</mi><mo>⋅</mo><mo
        >&lpar;</mo><mi>b</mi><mo>⋅</mo><mi>b</mi><mo>&rpar;</mo><mo
        >&rpar;</mo><mo>&rpar;</mo><mo>&rpar;</mo><mo>&rpar;</mo><mo
        >&rpar;</mo>
      </mrow>
    </math>
  </figure>
  <p>
    we can compute it using three multiplications:
  </p>
  <figure>
    <math display="block">
      <mtable>
        <mtr>
          <mtd><msup><mi>b</mi><mn>2</mn></msup><mo>=</mo><mi>b</mi><mo
            >⋅</mo><mi>b</mi></mtd>
        </mtr>
        <mtr>
          <mtd><msup><mi>b</mi><mn>4</mn></msup><mo>=</mo><msup><mi>b</mi><mn
            >2</mn></msup><mo>⋅</mo><msup><mi>b</mi><mn>2</mn></msup></mtd>
        </mtr>
        <mtr>
          <mtd><msup><mi>b</mi><mn>8</mn></msup><mo>=</mo><msup><mi>b</mi><mn
            >4</mn></msup><mo>⋅</mo><msup><mi>b</mi><mn>4</mn></msup></mtd>
        </mtr>
      </mtable>
    </math>
  </figure>
  <p>
    This method works fine for exponents that are powers of 2. We can also take
    advantage of successive squaring in computing exponentials in general if we
    use the rule
  </p>
  <figure>
    <math display="block">
      <mrow>
        <msup><mi>b</mi><mi>n</mi></msup><mo>=</mo><mo>&lbrace;</mo>
        <mtable>
          <mtr>
            <mtd><msup><mrow><mo>&lpar;</mo><msup><mi>b</mi><mrow><mo
              >&lpar;</mo><mi>n</mi><mo>/</mo><mn>2</mn><mo
              >&rpar;</mo></mrow></msup><mo>&rpar;</mo></mrow><mn
              >2</mn></msup></mtd>
            <mtd><mtext>if </mtext><mi>n</mi><mtext> is even</mtext></mtd>
          </mtr>
          <mtr>
            <mtd><mi>b</mi><mo>⋅</mo><msup><mi>b</mi><mrow><mi>n</mi><mo
              >-</mo><mn>1</mn></mrow></msup></mtd>
            <mtd><mtext>if </mtext><mi>n</mi><mtext> is odd</mtext></mtd>
          </mtr>
        </mtable>
      </mrow>
    </math>
  </figure>
  <p>
    We can express this method as a procedure:
  </p>
  <textarea name="1.2.4-03" data-extends="1.1.4-01 1.2.4-04">
(define (fast-expt b n)
  (cond ((= n 0) 1)
        ((even? n) (square (fast-expt b (/ n 2))))
        (else (* b (fast-expt b (- n 1))))))</textarea>
  <output for="1.2.4-03" class="block">fast-expt</output>
  <p>
    where the predicate to test whether an integer is even is defined in terms
    of the primitive procedure <code>remainder</code> by
  </p>
  <textarea name="1.2.4-04">
(define (even? n)
  (= (remainder n 2) 0))</textarea>
  <output for="1.2.4-04" class="block">even?</output>
  <p>
    The process evolved by <code>fast-expt</code> grows logarithmically with
    <math><mi>n</mi></math> in both space and number of steps. To see this,
    observe that computing <math><msup><mi>b</mi><mrow><mn>2</mn><mi
    >n</mi></mrow></msup></math> using <code>fast-expt</code> requires only one
    more multiplication than computing <math><msup><mi>b</mi><mi
    >n</mi></msup></math>. The size of the exponent we can compute therefore
    doubles (approximately) with every new multiplication we are allowed. Thus,
    the number of multiplications required for an exponent of <math><mi
    >n</mi></math> grows about as fast as the logarithm of <math><mi
    >n</mi></math> to the base 2. The process has <math><mi>θ</mi><mo
    >&lpar;</mo><mtext>log </mtext><mi>n</mi><mo>&rpar;</mo></math> growth.<sup
    id="r1.37s"><a href="#r1.37d">37</a></sup>
  </p>
  <p>
    The difference between <math><mi>θ</mi><mo>&lpar;</mo><mtext
    >log </mtext><mi>n</mi><mo>&rpar;</mo></math> growth and <math><mi
    >θ</mi><mo>&lpar;</mo><mi>n</mi><mo>&rpar;</mo></math> growth becomes
    striking as <math><mi>n</mi></math> becomes large. For example, <code
    >fast-expt</code> for <math><mi>n</mi><mo>=</mo><mn>1000</mn></math>
    requires only 14 multiplications.<sup id="r1.38s"><a href="#r1.38d"
    >38</a></sup> It is also possible to use the idea of successive squaring to
    devise an iterative algorithm that computes exponentials with a logarithmic
    number of steps (see exercise <a href="e1.16">1.16</a>), although, as is
    often the case with iterative algorithms, this is not written down so
    straightforwardly as the recursive algorithm.<sup id="r1.39s"><a
    href="#r1.39d">39</a></sup>
  </p>
</section>

<section id="e1.16">
  <h5><a href="#e1.16">Exercise 1.16</a></h5>
  <p>
    Design a procedure that evolves an iterative exponentiation process that
    uses successive squaring and uses a logarithmic number of steps, as does
    <code>fast-expt</code>. (Hint: Using the observation that <math><msup><mrow
    ><mo>&lpar;</mo><msup><mi>b</mi><mrow><mi>n</mi><mo>/</mo><mn
    >2</mn></mrow></msup><mo>&rpar;</mo></mrow><mn>2</mn></msup><mo>=</mo><msup
    ><mrow><mo>&lpar;</mo><msup><mi>b</mi><mn>2</mn></msup><mo
    >&rpar;</mo></mrow><mrow><mi>n</mi><mo>/</mo><mn
    >2</mn></mrow></msup></math>, keep, along with the exponent <math><mi
    >n</mi></math> and the base <math><mi>b</mi></math>, an additional state
    variable <math><mi>a</mi></math>, and define the state transformation in
    such a way that the product <math><mi>a</mi><mo>⋅</mo><msup><mi>b</mi><mi
    >n</mi></msup></math> is unchanged from state to state. At the beginning of
    the process <math><mi>a</mi></math> is taken to be 1, and the answer is
    given by the value of <math><mi>a</mi></math> at the end of the process. In
    general, the technique of defining an invariant quantity that remains
    unchanged from state to state is a powerful way to think about the design
    of iterative algorithms.)
  </p>
  <textarea name="e1.16-01"></textarea>
  <output for="e1.16-01" class="block">&nbsp;</output>
</section>

<section id="e1.17">
  <h5><a href="#e1.17">Exercise 1.17</a></h5>
  <p>
    The exponentiation algorithms in this section are based on performing
    exponentiation by means of repeated multiplication. In a similar way, one
    can perform integer multiplication by means of repeated addition. The
    following multiplication procedure (in which it is assumed that our
    language can only add, not multiply) is analogous to the <code>expt</code>
    procedure:
  </p>
  <textarea name="e1.17-01">
(define (* a b)
  (if (= b 0)
      0
      (+ a (* a (- b 1)))))</textarea>
  <output for="e1.17-01" class="block">*</output>
  <p>
    This algorithm takes a number of steps that is linear in <code>b</code>.
    Now suppose we include, together with addition, operations
    <code>double</code>, which doubles an integer, and <code>halve</code>,
    which divides an (even) integer by 2. Using these, design a multiplication
    procedure analogous to <code>fast-expt</code> that uses a logarithmic
    number of steps.
  </p>
  <textarea name="e1.17-02">
(define (double x) (+ x x))
(define (halve x) (/ x 2))</textarea>
  <output for="1.17-02" class="block">double
halve</output>
</section>

<section id="e1.18">
  <h5><a href="#e.18">Exercise 1.18</a></h5>
  <p>
    Using the results of exercises <a href="#e1.16">1.16</a> and <a
    href="#e1.17">1.17</a>, devise a procedure that generates an iterative
    process for multiplying two integers in terms of adding, doubling, and
    halving and uses a logarithmic number of steps.<sup id="r1.40s"><a
    href="#r1.40d">40</a></sup>
  </p>
  <textarea name="e1.18-01" data-extends="e1.17-02"></textarea>
  <output for="e1.18-01" class="block">&nbsp;</output>
</section>

<section id="e1.19">
  <h5><a href="#e1.19">Exercise 1.19</a></h5>
  <p>
    There is a clever algorithm for computing the Fibonacci numbers in a
    logarithmic number of steps. Recall the transformation of the state
    variables <math><mi>a</mi></math> and <math><mi>b</mi></math> in the <code
    >fib-iter</code> process of section <a href="#s1.2.2">1.2.2</a>: <math><mi
    >a</mi><mo>&larr;</mo><mi>a</mi><mo>+</mo><mi>b</mi></math> and <math><mi
    >b</mi><mo>&larr;</mo><mi>a</mi></math>. Call this transformation <math><mi
    >T</mi></math>, and observe that applying <math><mi>T</mi></math> over and
    over again <math><mi>n</mi></math> times, starting with 1 and 0, produces
    the pair <math><mtext>Fib</mtext><mo>&lpar;</mo><mi>n</mi><mo>+</mo><mn
    >1</mn><mo>&rpar;</mo></math> and <math><mtext>Fib</mtext><mo
    >&lpar;</mo><mi>n</mi><mo>&rpar;</mo></math>. In other words, the Fibonacci
    numbers are produced by applying <math><msup><mi>T</mi><mi
    >n</mi></msup></math>, the <math><mi>n</mi></math>th power of the
    transformation <math><mi>T</mi></math>, starting with the pair (1,0). Now
    consider <math><mi>T</mi></math> to be the special case of <math><mi
    >p</mi><mo>=</mo><mn>0</mn></math> and <math><mi>q</mi><mo>=</mo><mn
    >1</mn></math> in a family of transformations <math><msub><mi>T</mi><mrow
    ><mi>p</mi><mi>q</mi></mrow></msub></math>, where <math><msub><mi
    >T</mi><mrow><mi>p</mi><mi>q</mi></mrow></msub></math> transforms the pair
    <math><mo>&lpar;</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo>&rpar;</mo></math>
    according to <math><mi>a</mi><mo>&larr;</mo><mi>b</mi><mi>q</mi><mo
    >+</mo><mi>a</mi><mi>q</mi><mo>+</mo><mi>a</mi><mi>p</mi></math> and <math
    ><mi>b</mi><mo>&larr;</mo><mi>b</mi><mi>p</mi><mo>+</mo><mi>a</mi><mi
    >q</mi></math>. Show that if we apply such a transformation <math><msub><mi
    >T</mi><mrow><mi>p</mi><mi>q</mi></mrow></msub></math> twice, the effect is
    the same as using a single transformation <math><msub><mi>T</mi><mrow><mi
    >p</mi><mo>′</mo><mi>q</mi><mo>′</mo></mrow></msub></math> of the same
    form, and compute <math><mi>p</mi><mo>′</mo></math> and <math><mi>q</mi><mo
    >′</mo></math> in terms of <math><mi>p</mi></math> and <math><mi
    >q</mi></math>. This gives us an explicit way to square these
    transformations, and thus we can compute <math><msup><mi>T</mi><mi
    >n</mi></msup></math> using successive squaring, as in the <code
    >fast-expt</code> procedure. Put this all together to complete the
    following procedure, which runs in a logarithmic number of steps:<sup
    id="r1.41s"><a href="#r1.41d">41</a></sup>
  </p>
  <textarea name="e1.19-01" data-extends="1.2.4-04">
(define (fib n)
  (fib-iter 1 0 0 1 n))
(define (fib-iter a b p q count)
  (cond ((= count 0) b)
        ((even? count)
         (fib-iter a
                   b
                   &lt;??&gt;      ; compute p'
                   &lt;??&gt;      ; compute q'
                   (/ count 2)))
        (else (fib-iter (+ (* b q) (* a q) (* a p))
                        (+ (* b p) (* a q))
                        p
                        q
                        (- count 1)))))</textarea>
  <output for="e1.19-01" class="block">fib
fib-iter</output>
</section>

<section id="s1.2.5">
  <h4><a href="#s1.2.5">1.2.5 Greatest Common Divisors</a></h4>
  <p>
    The greatest common divisor (GCD) of two integers <math><mi>a</mi></math>
    and <math><mi>b</mi></math> is defined to be the largest integer that
    divides both <math><mi>a</mi></math> and <math><mi>b</mi></math> with no
    remainder. For example, the GCD of 16 and 28 is 4. In chapter 2, when we
    investigate how to implement rational-number arithmetic, we will need to be
    able to compute GCDs in order to reduce rational numbers to lowest terms.
    (To reduce a rational number to lowest terms, we must divide both the
    numerator and the denominator by their GCD. For example, 16/28 reduces to
    4/7.) One way to find the GCD of two integers is to factor them and search
    for common factors, but there is a famous algorithm that is much more
    efficient.
  </p>
  <p>
    The idea of the algorithm is based on the observation that, if <math><mi
    >r</mi></math> is the remainder when <math><mi>a</mi></math> is divided by
    <math><mi>b</mi></math>, then the common divisors of <math><mi
    >a</mi></math> and <math><mi>b</mi></math> are precisely the same as the
    common divisors of <math><mi>b</mi></math> and <math><mi>r</mi></math>.
    Thus, we can use the equation
  </p>
  <figure>
    <math display="block">
      <mrow><mtext>GCD</mtext><mo>&lpar;</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo
      >&rpar;</mo><mo>=</mo><mtext>GCD</mtext><mo>&lpar;</mo><mi>b</mi><mo
      >,</mo><mi>r</mi><mo>&rpar;</mo></mrow>
    </math>
  </figure>
  <p>
    to successively reduce the problem of computing a GCD to the problem of
    computing the GCD of smaller and smaller pairs of integers. For example,
  </p>
  <figure>
    <math display="block">
      <mtable>
        <mtr>
          <mtd><mtext>GCD</mtext><mo>&lpar;</mo><mn>206</mn><mo>,</mo><mn
            >40</mn><mo>&rpar;</mo></mtd>
          <mtd><mo>=</mo></mtd>
          <mtd><mtext>GCD</mtext><mo>&lpar;</mo><mn>40</mn><mo>,</mo><mn
            >6</mn><mo>&rpar;</mo></mtd>
        </mtr>
        <mtr>
          <mtd></mtd>
          <mtd><mo>=</mo></mtd>
          <mtd><mtext>GCD</mtext><mo>&lpar;</mo><mn>6</mn><mo>,</mo><mn
            >4</mn><mo>&rpar;</mo></mtd>
        </mtr>
        <mtr>
          <mtd></mtd>
          <mtd><mo>=</mo></mtd>
          <mtd><mtext>GCD</mtext><mo>&lpar;</mo><mn>4</mn><mo>,</mo><mn
            >2</mn><mo>&rpar;</mo></mtd>
        </mtr>
        <mtr>
          <mtd></mtd>
          <mtd><mo>=</mo></mtd>
          <mtd><mtext>GCD</mtext><mo>&lpar;</mo><mn>2</mn><mo>,</mo><mn
            >0</mn><mo>&rpar;</mo></mtd>
        </mtr>
        <mtr>
          <mtd></mtd>
          <mtd><mo>=</mo></mtd>
          <mtd><mn>2</mn></mtd>
        </mtr>
      </mtable>
    </math>
  </figure>
  <p>
    reduces GCD(206,40) to GCD(2,0), which is 2. It is possible to show that
    starting with any two positive integers and performing repeated reductions
    will always eventually produce a pair where the second number is 0. Then
    the GCD is the other number in the pair. This method for computing the GCD
    is known as <em>Euclid’s Algorithm</em>.<sup id="r1.42s"><a href="#r1.42d"
    >42</a></sup>
  </p>
  <p>
    It is easy to express Euclid’s Algorithm as a procedure:
  </p>
  <textarea name="1.2.5-01">
(define (gcd a b)
  (if (= b 0)
      a
      (gcd b (remainder a b))))</textarea>
  <output for="1.2.5-01" class="block">gcd</output>
  <p>
    This generates an iterative process, whose number of steps grows as the
    logarithm of the numbers involved.
  </p>
  <p>
    The fact that the number of steps required by Euclid’s Algorithm has
    logarithmic growth bears an interesting relation to the Fibonacci numbers:
  </p>
  <p>
    <strong>Lamé’s Theorem:</strong> If Euclid’s Algorithm requires <math><mi
    >k</mi></math> steps to compute the GCD of some pair, then the smaller
    number in the pair must be greater than or equal to the <math><mi
    >k</mi></math>th Fibonacci number.<sup id="r1.43s"><a href="#r1.43d"
    >43</a></sup>
  </p>
  <p>
    We can use this theorem to get an order-of-growth estimate for Euclid’s
    Algorithm. Let <math><mi>n</mi></math> be the smaller of the two inputs to
    the procedure. If the process takes <math><mi>k</mi></math> steps, then we
    must have <math><mi>n</mi><mo>&GreaterEqual;</mo><mtext>Fib</mtext><mo
    >&lpar;</mo><mi>k</mi><mo>&rpar;</mo><mo>≈</mo><msup><mi>ϕ</mi><mn
    >5</mn></msup><mo>/</mo><msqrt><mn>5</mn></msqrt></math>. Therefore the
    number of steps <math><mi>k</mi></math> grows as the logarithm (to the base
    <math><mi>ϕ</mi></math>) of <math><mi>n</mi></math>. Hence, the order of
    growth is <math><mi>θ</mi><mo>&lpar;</mo><mtext>log </mtext><mi>n</mi><mo
    >&rpar;</mo></math>.
  </p>
</section>

<section id="e1.20">
  <h5><a href="#e1.20">Exercise 1.20</a></h5>
  <p>
    The process that a procedure generates is of course dependent on the rules
    used by the interpreter. As an example, consider the iterative
    <code>gcd</code> procedure given above. Suppose we were to interpret this
    procedure using normal-order evaluation, as discussed in section <a
    href="#s1.1.5">1.1.5</a>. (The normal-order-evaluation rule for
    <code>if</code> is described in exercise <a href="#e1.5">1.5</a>.) Using
    the substitution method (for normal order), illustrate the process
    generated in evaluating <code>(gcd 206 40)</code> and indicate the
    <code>remainder</code> operations that are actually performed. How many
    <code>remainder</code> operations are actually performed in the
    normal-order evaluation of <code>(gcd 206 40)</code>? In the
    applicative-order evaluation?
  </p>
</section>

<section id="s1.2.6">
  <h4><a href="#s1.2.6">1.2.6 Example: Testing for Primality</a></h4>
  <p>
    This section describes two methods for checking the primality of an integer
    <math><mi>n</mi></math>, one with order of growth <math><mi>θ</mi><mo
    >&lpar;</mo><mi>n</mi><mo>&rpar;</mo></math>, and a “probabilistic”
    algorithm with order of growth <math><mi>θ</mi><mo>&lpar;</mo><mtext
    >log </mtext><mi>n</mi><mo>&rpar;</mo></math>. The exercises at the end of
    this section suggest programming projects based on these algorithms.
  </p>
</section>

<section id="ex1.2">
  <h5><a href="#ex1.2">Searching for divisors</a></h5>
  <p>
    Since ancient times, mathematicians have been fascinated by problems
    concerning prime numbers, and many people have worked on the problem of
    determining ways to test if numbers are prime. One way to test if a number
    is prime is to find the number’s divisors. The following program finds the
    smallest integral divisor (greater than 1) of a given number <math><mi
    >n</mi></math>. It does this in a straightforward way, by testing <math><mi
    >n</mi></math> for divisibility by successive integers starting with 2.
  </p>
  <textarea name="ex1.2-01" data-extends="1.1.4-01">
(define (smallest-divisor n)
  (find-divisor n 2))
(define (find-divisor n test-divisor)
  (cond ((> (square test-divisor) n) n)
        ((divides? test-divisor n) test-divisor)
        (else (find-divisor n (+ test-divisor 1)))))
(define (divides? a b)
  (= (remainder b a) 0))</textarea>
  <output for="ex1.2-01" class="block">smallest-divisor
find-divisor
divides?</output>
  <p>
    We can test whether a number is prime as follows: <math><mi>n</mi></math>
    is prime if and only if <math><mi>n</mi></math> is its own smallest
    divisor.
  </p>
  <textarea name="ex1.2-02" data-extends="ex1.2-01">
(define (prime? n)
  (= n (smallest-divisor n)))</textarea>
  <output for="ex1.2-02" class="block">prime?</output>
  <p>
    The end test for <code>find-divisor</code> is based on the fact that if
    <math><mi>n</mi></math> is not prime it must have a divisor less than or
    equal to <math><mi>n</mi></math>.<sup id="r1.44s"><a href="#r1.44d"
    >44</a></sup> This means that the algorithm need only test divisors between
    1 and <math><mi>n</mi></math>. Consequently, the number of steps required
    to identify <math><msqrt><mi>n</mi></msqrt></math> as prime will have order of growth
    <math><mi>θ</mi><mo>&lpar;</mo><msqrt><mi>n</mi></msqrt><mo
    >&rpar;</mo></math>.
  </p>
</section>

<section id="ex1.3">
  <h5><a href="#ex1.3">The Fermat test</a></h5>
  <p>
    The <math><mi>θ</mi><mo>&lpar;</mo><mtext>log </mtext><mi>n</mi><mo
    >&rpar;</mo></math> primality test is based on a result from number theory
    known as Fermat’s Little Theorem.<sup id="r1.45s"><a href="#r1.45d"
    >45</a></sup>
  </p>
  <p>
    <strong>Fermat’s Little Theorem:</strong> If <math><mi>n</mi></math> is a
    prime number and <math><mi>a</mi></math> is any positive integer less than
    <math><mi>n</mi></math>, then <math><mi>a</mi></math> raised to the <math
    ><mi>n</mi></math>th power is congruent to <math><mi>a</mi><mtext
    > modulo </mtext><mi>n</mi></math>.
  </p>
  <p>
    (Two numbers are said to be <em>congruent modulo</em> <math><mi
    >n</mi></math> if they both have the same remainder when divided by <math
    ><mi>n</mi></math>. The remainder of a number <math><mi>a</mi></math> when
    divided by <math><mi>n</mi></math> is also referred to as the <em>remainder
    of</em> <math><mi>a</mi></math> <em>modulo</em> <math><mi>n</mi></math>, or
    simply as <math><mi>a</mi><mtext> modulo </mtext><mi>n</mi></math>.)
  </p>
  <p>
    If <math><mi>n</mi></math> is not prime, then, in general, most of the
    numbers <math><mi>a</mi><mo>&lt;</mo><mi>n</mi></math> will not satisfy the
    above relation. This leads to the following algorithm for testing
    primality: Given a number <math><mi>n</mi></math>, pick a random number
    <math><mi>a</mi><mo>&lt;</mo><mi>n</mi></math> and compute the remainder of
    <math><msup><mi>a</mi><mi>n</mi></msup><mtext> modulo </mtext><mi
    >n</mi></math>. If the result is not equal to <math><mi>a</mi></math>, then
    <math><mi>n</mi></math> is certainly not prime. If it is <math><mi
    >a</mi></math>, then chances are good that <math><mi>n</mi></math> is
    prime. Now pick another random number <math><mi>a</mi></math> and test it
    with the same method. If it also satisfies the equation, then we can be
    even more confident that <math><mi>n</mi></math> is prime. By trying more
    and more values of <math><mi>a</mi></math>, we can increase our confidence
    in the result. This algorithm is known as the Fermat test.
  </p>
  <p>
    To implement the Fermat test, we need a procedure that computes the
    exponential of a number modulo another number:
  </p>
  <textarea name="ex1.3-01" data-extends="1.1.4-01 1.2.4-04">
(define (expmod base exp m)
  (cond ((= exp 0) 1)
        ((even? exp)
         (remainder (square (expmod base (/ exp 2) m))
                    m))
        (else
         (remainder (* base (expmod base (- exp 1) m))
                    m))))</textarea>
  <output for="ex1.3-01" class="block">expmod</output>
  <p>
    This is very similar to the <code>fast-expt</code> procedure of section <a
    href="#s1.2.4">1.2.4</a>. It uses successive squaring, so that the number
    of steps grows logarithmically with the exponent.<sup id="r1.46s"><a
    href="#r1.46d">46</a></sup>
  </p>
  <p>
    The Fermat test is performed by choosing at random a number <math><mi
    >a</mi></math> between 1 and <math><mi>n</mi><mo>-</mo><mn>1</mn></math>
    inclusive and checking whether the remainder modulo <math><mi
    >n</mi></math> of the <math><mi>n</mi></math>th power of <math><mi
    >a</mi></math> is equal to <math><mi>a</mi></math>. The random number <math
    ><mi>a</mi></math> is chosen using the procedure <code>random</code>, which
    we assume is included as a primitive in Scheme. <code>Random</code> returns
    a nonnegative integer less than its integer input. Hence, to obtain a
    random number between <math><mn>1</mn></math> and
    <math><mi>n</mi><mo>-</mo><mn>1</mn></math>, we call <code>random</code>
    with an input of <math><mi>n</mi><mo>-</mo><mn >1</mn></math> and add <math
    ><mn>1</mn></math> to the result:
  </p>
  <textarea name="ex1.3-02" data-extends="ex1.3-01">
(define (fermat-test n)
  (define (try-it a)
    (= (expmod a n n) a))
  (try-it (+ 1 (random (- n 1)))))</textarea>
  <output for="ex1.3-02" class="block">fermat-test</output>
  <p>
    The following procedure runs the test a given number of times, as specified
    by a parameter. Its value is true if the test succeeds every time, and
    false otherwise.
  </p>
  <textarea name="ex1.3-03" data-extends="ex1.3-02">
(define (fast-prime? n times)
  (cond ((= times 0) true)
        ((fermat-test n) (fast-prime? n (- times 1)))
        (else false)))</textarea>
  <output for="ex1.3-03" class="block">fast-prime?</output>
</section>

<section id="ex1.4">
  <h5><a href="#ex1.4">Probabilistic methods</a></h5>
  <p>
    The Fermat test differs in character from most familiar algorithms, in
    which one computes an answer that is guaranteed to be correct. Here, the
    answer obtained is only probably correct. More precisely, if <math><mi
    >n</mi></math> ever fails the Fermat test, we can be certain that <math><mi
    >n</mi></math> is not prime. But the fact that <math><mi>n</mi></math>
    passes the test, while an extremely strong indication, is still not a
    guarantee that <math><mi>n</mi></math> is prime. What we would like to say
    is that for any number <math><mi>n</mi></math>, if we perform the test
    enough times and find that <math><mi>n</mi></math> always passes the test,
    then the probability of error in our primality test can be made as small as
    we like.
  </p>
  <p>
    Unfortunately, this assertion is not quite correct. There do exist numbers
    that fool the Fermat test: numbers <math><mi>n</mi></math> that are not
    prime and yet have the property that <math><msup><mi>a</mi><mi
    >n</mi></msup></math> is congruent to <math><mi>a</mi><mtext
    > modulo </mtext><mi>n</mi></math> for all integers <math><mi>a</mi><mo
    >&lt;</mo><mi>n</mi></math>. Such numbers are extremely rare, so the Fermat
    test is quite reliable in practice.<sup id="r1.47s"><a href="#r1.47d"
    >47</a></sup> There are variations of the Fermat test that cannot be
    fooled. In these tests, as with the Fermat method, one tests the primality
    of an integer <math><mi>n</mi></math> by choosing a random integer <math
    ><mi>a</mi><mo>&lt;</mo><mi>n</mi></math> and checking some condition that
    depends upon <math><mi>n</mi></math> and <math><mi>a</mi></math>. (See
    exercise <a href="#e1.28">1.28</a> for an example of such a test.) On the
    other hand, in contrast to the Fermat test, one can prove that, for any
    <math><mi>n</mi></math>, the condition does not hold for most of the
    integers <math><mi>a</mi><mo >&lt;</mo><mi>n</mi></math> unless
    <math><mi>n</mi></math> is prime. Thus, if <math><mi>n</mi></math> passes
    the test for some random choice of <math><mi>a</mi></math>, the chances are
    better than even that <math><mi>n</mi></math> is prime. If <math><mi
    >n</mi></math> passes the test for two random choices of <math><mi
    >a</mi></math>, the chances are better than 3 out of 4 that <math><mi
    >n</mi></math> is prime. By running the test with more and more randomly
    chosen values of <math><mi>a</mi></math> we can make the probability of
    error as small as we like.
  </p>
  <p>
    The existence of tests for which one can prove that the chance of error
    becomes arbitrarily small has sparked interest in algorithms of this type,
    which have come to be known as <em>probabilistic algorithms</em>. There is
    a great deal of research activity in this area, and probabilistic
    algorithms have been fruitfully applied to many fields.<sup id="r1.48s"><a
    href="#r1.48d">48</a></sup>
  </p>
</section>

<section id="e1.21">
  <h5><a href="#e1.21">Exercise 1.21</a></h5>
  <p>
    Use the <code>smallest-divisor</code> procedure to find the smallest
    divisor of each of the following numbers: 199, 1999, 19999.
  </p>
  <textarea name="e1.21-01" data-extends="ex1.2-01"></textarea>
  <output for="e1.21-01" class="block">&nbsp;</output>
</section>

<section id="e1.22">
  <h5><a href="#e1.22">Exercise 1.22</a></h5>
  <p>
    Most Lisp implementations include a primitive called <code>runtime</code>
    that returns an integer that specifies the amount of time the system has
    been running (measured, for example, in microseconds). The following
    <code>timed-prime-test</code> procedure, when called with an integer <math
    ><mi>n</mi></math>, prints <math><mi>n</mi></math> and checks to see if
    <math><mi>n</mi></math> is prime. If <math><mi>n</mi></math> is prime, the
    procedure prints three asterisks followed by the amount of time used in
    performing the test.
  </p>
  <textarea name="e1.22-01" data-extends="ex1.2-02 ex1.3-03">
(define (timed-prime-test n)
  (newline)
  (display n)
  (start-prime-test n (runtime)))
(define (start-prime-test n start-time)
  (if (prime? n)
      (report-prime (- (runtime) start-time))))
(define (report-prime elapsed-time)
  (display " *** ")
  (display elapsed-time))</textarea>
  <output for="e1.22-01" class="block">timed-prime-test
start-prime-test
report-prime</output>
  <p>
    Using this procedure, write a procedure <code>search-for-primes</code> that
    checks the primality of consecutive odd integers in a specified range. Use
    your procedure to find the three smallest primes larger than 1000; larger
    than 10,000; larger than 100,000; larger than 1,000,000. Note the time
    needed to test each prime. Since the testing algorithm has order of growth
    of <math><mi>θ</mi><mo>&lpar;</mo><msqrt><mi>n</mi></msqrt><mo
    >&rpar;</mo></math>, you should expect that testing for primes around
    10,000 should take about <math><msqrt><mn>10</mn></msqrt></math> times as
    long as testing for primes around 1000. Do your timing data bear this out?
    How well do the data for 100,000 and 1,000,000 support the
    <math><msqrt><mi>n</mi></msqrt></math> prediction? Is your result
    compatible with the notion that programs on your machine run in time
    proportional to the number of steps required for the computation?
  </p>
</section>

<section id="e1.23">
  <h5><a href="#e1.23">Exercise 1.23</a></h5>
  <p>
    The <code>smallest-divisor</code> procedure shown at the start of this
    section does lots of needless testing: After it checks to see if the number
    is divisible by 2 there is no point in checking to see if it is divisible
    by any larger even numbers. This suggests that the values used for
    <code>test-divisor</code> should not be 2, 3, 4, 5, 6, …, but rather 2, 3,
    5, 7, 9, …. To implement this change, define a procedure <code>next</code>
    that returns 3 if its input is equal to 2 and otherwise returns its input
    plus 2. Modify the <code>smallest-divisor</code> procedure to use <code
    >(next test-divisor)</code> instead of <code>(+ test-divisor 1)</code>.
    With <code>timed-prime-test</code> incorporating this modified version of
    <code>smallest-divisor</code>, run the test for each of the 12 primes found
    in exercise <a href="#e1.22">1.22</a>. Since this modification halves the
    number of test steps, you should expect it to run about twice as fast. Is
    this expectation confirmed? If not, what is the observed ratio of the
    speeds of the two algorithms, and how do you explain the fact that it is
    different from 2?
  </p>
  <textarea name="e1.23-01"></textarea>
  <output for="e1.23-01" class="block">&ZeroWidthSpace;</output>
</section>

<section id="e1.24">
  <h5><a href="#e1.24">Exercise 1.24</a></h5>
  <p>
    Modify the <code>timed-prime-test</code> procedure of exercise <a
    href="#e1.22">1.22</a> to use <code>fast-prime?</code> (the Fermat method),
    and test each of the 12 primes you found in that exercise. Since the Fermat
    test has <math><mi>θ</mi><mo>&lpar;</mo><mtext>log </mtext><mi>n</mi><mo
    >&rpar;</mo></math> growth, how would you expect the time to test primes
    near 1,000,000 to compare with the time needed to test primes near 1000? Do
    your data bear this out? Can you explain any discrepancy you find?
  </p>
  <textarea name="e1.24-01"></textarea>
  <output for="e1.24-01" class="block">&ZeroWidthSpace;</output>
</section>

<section id="e1.25">
  <h5><a href="#e1.25">Exercise 1.25</a></h5>
  <p>
    Alyssa P. Hacker complains that we went to a lot of extra work in writing
    <code>expmod</code>. After all, she says, since we already know how to
    compute exponentials, we could have simply written
  </p>
  <textarea name="e1.25-01" data-extends="1.2.4-03">
(define (expmod base exp m)
  (remainder (fast-expt base exp) m))</textarea>
  <output for="e1.25-01" class="block">expmod</output>
  <p>
    Is she correct? Would this procedure serve as well for our fast prime
    tester? Explain.
  </p>
</section>

<section id="e1.26">
  <h5><a href="#e1.26">Exercise 1.26</a></h5>
  <p>
    Louis Reasoner is having great difficulty doing exercise <a
    href="#1.24">1.24</a>. His <code>fast-prime?</code> test seems to run more
    slowly than his <code>prime?</code> test. Louis calls his friend Eva Lu
    Ator over to help. When they examine Louis’s code, they find that he has
    rewritten the <code>expmod</code> procedure to use an explicit
    multiplication, rather than calling <code>square</code>:
  </p>
  <textarea name="e1.26-01">
(define (expmod base exp m)
  (cond ((= exp 0) 1)
        ((even? exp)
         (remainder (* (expmod base (/ exp 2) m)
                       (expmod base (/ exp 2) m))
                    m))
        (else
         (remainder (* base (expmod base (- exp 1) m))
                    m))))</textarea>
  <output for="e1.26-01" class="block">expmod</output>
  <p>
    “I don’t see what difference that could make,” says Louis. “I do.” says
    Eva. “By writing the procedure like that, you have transformed the <math
    ><mi>θ</mi><mo>&lpar;</mo><mtext>log </mtext><mi>n</mi><mo
    >&rpar;</mo></math> process into a <math><mi>θ</mi><mo>&lpar;</mo><mi
    >n</mi><mo>&rpar;</mo></math> process.” Explain.
  </p>
</section>

<section id="e1.27">
  <h5><a href="#e1.27">Exercise 1.27</a></h5>
  <p>
    Demonstrate that the Carmichael numbers listed in footnote <a
    href="#r1.47d">47</a> really do fool the Fermat test. That is, write a
    procedure that takes an integer <math><mi>n</mi></math> and tests whether
    <math><msup><mi>a</mi><mi>n</mi></msup></math> is congruent to <math><mi
    >a</mi><mtext> modulo </mtext><mi>n</mi></math> for every <math><mi
    >a</mi><mo>&lt;</mo><mi>n</mi></math>, and try your procedure on the given
    Carmichael numbers.
  </p>
  <textarea name="e1.27-01"></textarea>
  <output for="e1.27-01" class="block">&ZeroWidthSpace;</output>
</section>

<section id="e1.28">
  <h5><a href="#1.28">Exercise 1.28</a></h5>
  <p>
    One variant of the Fermat test that cannot be fooled is called the
    <em>Miller-Rabin test</em> (Miller 1976; Rabin 1980). This starts from an
    alternate form of Fermat’s Little Theorem, which states that if
    <math><mi>n</mi></math> is a prime number and <math><mi>a</mi></math> is
    any positive integer less than <math><mi>n</mi></math>, then <math><mi
    >a</mi></math> raised to the <math><mo>&lpar;</mo><mi>n</mi><mo>-</mo><mn
    >1</mn><mo>&rpar;</mo></math><sup>st</sup> power is congruent to
    <math><mn>1</mn><mtext> modulo </mtext><mi>n</mi></math>. To test the
    primality of a number <math><mi>n</mi></math> by the Miller-Rabin test, we
    pick a random number <math><mi>a</mi><mo>&lt;</mo><mi>n</mi></math> and
    raise a to the <math><mo>&lpar;</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo
    >&rpar;</mo></math><sup>st</sup> power modulo <math><mi>n</mi></math> using
    the <code>expmod</code> procedure. However, whenever we perform the
    squaring step in <code>expmod</code>, we check to see if we have discovered
    a “nontrivial square root of 1 modulo <math><mi>n</mi></math>,” that is, a
    number not equal to 1 or <math><mi>n</mi><mo>-</mo><mn >1</mn></math> whose
    square is equal to 1 modulo <math><mi>n</mi></math>. It is possible to
    prove that if such a nontrivial square root of 1 exists, then
    <math><mi>n</mi></math> is not prime. It is also possible to prove that if
    <math><mi>n</mi></math> is an odd number that is not prime, then, for at
    least half the numbers <math><mi>a</mi><mo>&lt;</mo><mi>n</mi></math>,
    computing <math><msup><mi>a</mi><mrow><mi>n</mi><mo>-</mo><mn
    >1</mn></mrow></msup></math> in this way will reveal a nontrivial square
    root of <math><mn>1</mn><mtext> modulo </mtext><mi>n</mi></math>. (This is
    why the Miller-Rabin test cannot be fooled.) Modify the <code>expmod</code>
    procedure to signal if it discovers a nontrivial square root of 1, and use
    this to implement the Miller-Rabin test with a procedure analogous to
    <code>fermat-test</code>. Check your procedure by testing various known
    primes and non-primes. Hint: One convenient way to make <code>expmod</code>
    signal is to have it return 0.
  </p>
  <textarea name="e1.28-01"></textarea>
  <output for="e1.28-01" class="block">&ZeroWidthSpace;</output>
</section>
<hr>
<footer>
  <p>
    <sup id="r1.29d"><a href="#r1.29s">29</a></sup> In a real program we would
    probably use the block structure introduced in the last section to hide the
    definition of <code>fact-iter</code>:
  </p>
  <textarea name="r1.29-01">
(define (factorial n)
  (define (iter product counter)
    (if (> counter n)
        product
        (iter (* counter product)
              (+ counter 1))))
  (iter 1 1))</textarea>
  <output for="r1.29-01" class="block">factorial</output>
  <p>
    <sup id="r1.30d"><a href="#r1.30s">30</a></sup> When we discuss the
    implementation of procedures on register machines in chapter 5, we will see
    that any iterative process can be realized “in hardware” as a machine that
    has a fixed set of registers and no auxiliary memory. In contrast,
    realizing a recursive process requires a machine that uses an auxiliary
    data structure known as a <em>stack</em>.
  </p>
  <p>
    <sup id="r1.31d"><a href="#r1.31s">31</a></sup> Tail recursion has long
    been known as a compiler optimization trick. A coherent semantic basis for
    tail recursion was provided by Carl Hewitt (1977), who explained it in
    terms of the “message-passing” model of computation that we shall discuss
    in chapter 3. Inspired by this, Gerald Jay Sussman and Guy Lewis Steele Jr.
    (see Steele 1975) constructed a tail-recursive interpreter for Scheme.
    Steele later showed how tail recursion is a consequence of the natural way
    to compile procedure calls (Steele 1977). The IEEE standard for Scheme
    requires that Scheme implementations be tail-recursive.
  </p>
  <p>
    <sup id="r1.32d"><a href="#r1.32s">32</a></sup> An example of this was
    hinted at in section <a href="#s1.1.3">1.1.3</a>: The interpreter itself
    evaluates expressions using a tree-recursive process.
  </p>
  <p>
    <sup id="r1.33d"><a href="#r1.33s">33</a></sup> For example, work through
    in detail how the reduction rule applies to the problem of making change
    for 10 cents using pennies and nickels.
  </p>
  <p>
    <sup id="r1.34d"><a href="#r1.34s">34</a></sup> One approach to coping with
    redundant computations is to arrange matters so that we automatically
    construct a table of values as they are computed. Each time we are asked to
    apply the procedure to some argument, we first look to see if the value is
    already stored in the table, in which case we avoid performing the
    redundant computation. This strategy, known as <em>tabulation</em> or
    <em>memoization</em>, can be implemented in a straightforward way.
    Tabulation can sometimes be used to transform processes that require an
    exponential number of steps (such as <code>count-change</code>) into
    processes whose space and time requirements grow linearly with the input.
    See exercise <a href="#e3.27">3.27</a>.
  </p>
  <p>
    <sup id="r1.35d"><a href="#r1.35s">35</a></sup> The elements of Pascal’s
    triangle are called the binomial coefficients, because the
    <math><mi>n</mi></math><sup>th</sup> row consists of the coefficients of
    the terms in the expansion of <math><msup><mrow><mo>&lpar;</mo><mi
    >x</mi><mo>+</mo><mi>y</mi><mo>&rpar;</mo></mrow><mi>n</mi></msup></math>.
    This pattern for computing the coefficients appeared in Blaise Pascal’s
    1653 seminal work on probability theory, <em>Traité du triangle
    arithmétique</em>. According to Knuth (1973), the same pattern appears in
    the <em>Szu-yuen Yü-chien</em> (“The Precious Mirror of the Four
    Elements”), published by the Chinese mathematician Chu Shih-chieh in 1303,
    in the works of the twelfth-century Persian poet and mathematician Omar
    Khayyam, and in the works of the twelfth-century Hindu mathematician
    Bháscara Áchárya.
  </p>
  <p>
    <sup id="r1.36d"><a href="#r1.36s">36</a></sup> These statements mask a
    great deal of oversimplification. For instance, if we count process steps
    as “machine operations” we are making the assumption that the number of
    machine operations needed to perform, say, a multiplication is independent
    of the size of the numbers to be multiplied, which is false if the numbers
    are sufficiently large. Similar remarks hold for the estimates of space.
    Like the design and description of a process, the analysis of a process can
    be carried out at various levels of abstraction.
  </p>
  <p>
    <sup id="r1.37d"><a href="#r1.37s">37</a></sup> More precisely, the number
    of multiplications required is equal to 1 less than the log base 2 of
    <math><mi>n</mi></math> plus the number of ones in the binary
    representation of <math><mi>n</mi></math>. This total is always less than
    twice the log base 2 of <math><mi>n</mi></math>. The arbitrary constants
    <math><msub><mi>k</mi><mn>1</mn></msub></math> and <math><msub><mi
    >k</mi><mn>2</mn></msub></math> in the definition of order notation imply
    that, for a logarithmic process, the base to which logarithms are taken
    does not matter, so all such processes are described as <math><mi>θ</mi><mo
    >&lpar;</mo><mtext>log </mtext><mi>n</mi><mo>&rpar;</mo></math>.
  </p>
  <p>
    <sup id="r1.38d"><a href="#r1.38s">38</a></sup> You may wonder why anyone
    would care about raising numbers to the 1000<sup>th</sup> power. See
    section <a href="#s1.2.6">1.2.6</a>.
  </p>
  <p>
    <sup id="r1.39d"><a href="#r1.39s">39</a></sup> This iterative algorithm is
    ancient. It appears in the Chandah-sutra by Áchárya Pingala, written before
    200 B.C. See Knuth 1981, section 4.6.3, for a full discussion and analysis
    of this and other methods of exponentiation.
  </p>
  <p>
    <sup id="r1.40d"><a href="#r1.40s">40</a></sup> This algorithm, which is
    sometimes known as the “Russian peasant method” of multiplication, is
    ancient. Examples of its use are found in the Rhind Papyrus, one of the two
    oldest mathematical documents in existence, written about 1700 B.C. (and
    copied from an even older document) by an Egyptian scribe named A’h-mose.
  </p>
  <p>
    <sup id="r1.41d"><a href="#r1.41s">41</a></sup> This exercise was suggested
    to us by Joe Stoy, based on an example in Kaldewaij 1990.
  </p>
  <p>
    <sup id="r1.42d"><a href="#r1.42s">42</a></sup> Euclid’s Algorithm is so
    called because it appears in Euclid’s Elements (Book 7, ca. 300 B.C.).
    According to Knuth (1973), it can be considered the oldest known nontrivial
    algorithm. The ancient Egyptian method of multiplication (exercise <a
    href="#e1.18">1.18</a>) is surely older, but, as Knuth explains, Euclid’s
    algorithm is the oldest known to have been presented as a general
    algorithm, rather than as a set of illustrative examples.
  </p>
  <p>
    <sup id="r1.43d"><a href="#r1.43s">43</a></sup> This theorem was proved in
    1845 by Gabriel Lamé, a French mathematician and engineer known chiefly for
    his contributions to mathematical physics. To prove the theorem, we
    consider pairs <math><mo>&lpar;</mo><msub><mi
    >a</mi><mi>k</mi></msub><mo>,</mo><msub><mi>b</mi><mi>k</mi></msub><mo
    >&rpar;</mo></math>, where <math><msub><mi>a</mi><mi>k</mi></msub><mo
    >&gt;</mo><msub><mi>b</mi><mi>k</mi></msub></math>, for which Euclid’s
    Algorithm terminates in <math><mi>k</mi></math> steps. The proof is based
    on the claim that, if <math><mo>&lpar;</mo><msub><mi>a</mi><mrow><mi
    >k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>b</mi><mrow
    ><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>&rpar;</mo><mo
    >&rarr;</mo><mo>&lpar;</mo><msub><mi>a</mi><mi>k</mi></msub><mo>,</mo><msub
    ><mi>b</mi><mi>k</mi></msub><mo>&rpar;</mo><mo>&rarr;</mo><mo
    >&lpar;</mo><msub><mi>a</mi><mrow><mi>k</mi><mo>-</mo><mn
    >1</mn></mrow></msub><mo>,</mo><msub><mi>b</mi><mrow><mi>k</mi><mo
    >-</mo><mn>1</mn></mrow></msub><mo>&rpar;</mo></math> are three successive
    pairs in the reduction process, then we must have <math><msub><mi
    >b</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>&ge;</mo><msub
    ><mi>b</mi><mi>k</mi></msub><mo>+</mo><msub><mi>b</mi><mrow><mi>k</mi><mo
    >-</mo><mn>1</mn></mrow></msub></math>. To
    verify the claim, consider that a reduction step is defined by applying the
    transformation <math><msub><mi>a</mi><mrow><mi>k</mi><mo>-</mo><mn
    >1</mn></mrow></msub><mo>=</mo><msub><mi>b</mi><mi>k</mi></msub></math>,
    <math><msub><mi>b</mi><mrow><mi>k</mi><mo>-</mo><mn
    >1</mn></mrow></msub></math> = remainder of <math><msub><mi>a</mi><mi
    >k</mi></msub></math> divided by <math><msub><mi>b</mi><mi
    >k</mi></msub></math>. The second equation means that <math><msub><mi
    >a</mi><mi>k</mi></msub><mo>=</mo><mi>q</mi><msub><mi>b</mi><mi
    >k</mi></msub><mo>+</mo><msub><mi>b</mi><mrow><mi>k</mi><mo>-</mo><mn
    >1</mn></mrow></msub></math> for some positive integer <math><mi
    >q</mi></math>. And since <math><mi>q</mi></math> must be at least 1 we
    have <math><msub><mi>a</mi><mi>k</mi></msub><mo>=</mo><mi>q</mi><msub><mi
    >b</mi><mi>k</mi></msub><mo>+</mo><msub><mi>b</mi><mrow><mi>k</mi><mo
    >-</mo><mn>1</mn></mrow></msub><mo>&ge;</mo><msub><mi>b</mi><mi
    >k</mi></msub><mo>+</mo><msub><mi>b</mi><mrow><mi>k</mi><mo>-</mo><mn
    >1</mn></mrow></msub></math>. But in the previous reduction step
    we have <math><msub><mi>b</mi><mrow><mi>k</mi><mo>+</mo><mn
    >1</mn></mrow></msub><mo>=</mo><msub><mi>a</mi><mi>k</mi></msub></math>.
    Therefore, <math><msub><mi>b</mi><mrow><mi>k</mi><mo>+</mo><mn
    >1</mn></mrow></msub><mo>=</mo><msub><mi>a</mi><mi>k</mi></msub><mo
    >&ge;</mo><msub><mi>b</mi><mi>k</mi></msub><mo>+</mo><msub><mi>b</mi><mrow
    ><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msub></math>. This verifies the
    claim. Now we can prove the theorem by induction on <math><mi
    >k</mi></math>, the number of steps that the algorithm requires to
    terminate. The result is true for <math><mi>k = 1</mi></math>, since this
    merely requires that <math><mi>b</mi></math> be at least as large as <math
    ><mtext>Fib</mtext><mo>&lpar;</mo><mn>1</mn><mo>&rpar;</mo><mo>=</mo><mn
    >1</mn></math>. Now, assume that the result is true for all integers less
    than or equal to <math><mi>k</mi></math> and establish the result for <math
    ><mi>k</mi><mo>+</mo><mn>1</mn></math>. Let <math><mo>&lpar;</mo><msub><mi
    >a</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><msub
    ><mi>b</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    >&rpar;</mo><mo>&rarr;</mo><mo>&lpar;</mo><msub><mi>a</mi><mi
    >k</mi></msub><mo>,</mo><msub><mi>b</mi><mi>k</mi></msub><mo>&rpar;</mo><mo
    >&rarr;</mo><mo>&lpar;</mo><msub><mi>a</mi><mrow><mi>k</mi><mo>-</mo><mn
    >1</mn></mrow></msub><mo>,</mo><msub><mi>b</mi><mrow><mi>k</mi><mo
    >-</mo><mn>1</mn></mrow></msub><mo>&rpar;</mo></math> be successive pairs
    in the reduction process. By our induction hypotheses, we have <math><msub
    ><mi>b</mi><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msub><mo
    >&ge;</mo><mtext>Fib</mtext><mo>&lpar;</mo><mi>k</mi><mo>-</mo><mn
    >1</mn><mo>&rpar;</mo></math> and <math><msub><mi>b</mi><mi
    >k</mi></msub><mo>&ge;</mo><mtext>Fib</mtext><mo>&lpar;</mo><mi>k</mi><mo
    >&rpar;</mo></math>. Thus, applying the claim we just proved together with
    the definition of the Fibonacci numbers gives <math><msub><mi>b</mi><mrow
    ><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>&ge;</mo><msub><mi
    >b</mi><mi>k</mi></msub><mo>+</mo><msub><mi>b</mi><mrow><mi>k</mi><mo
    >-</mo><mn>1</mn></mrow></msub><mo>&ge;</mo><mtext>Fib</mtext><mo
    >&lpar;</mo><mi>k</mi><mo>&rpar;</mo><mo>+</mo><mtext>Fib</mtext><mo
    >&lpar;</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo>&rpar;</mo><mo>=</mo><mtext
    >Fib</mtext><mo>&lpar;</mo><mi>k</mi><mo>+</mo><mn>1</mn><mo
    >&rpar;</mo></math>, which completes the proof of Lamé’s Theorem.
  </p>
  <p>
    <sup id="r1.44d"><a href="#r1.44s">44</a></sup> If <math><mi>d</mi></math>
    is a divisor of <math><mi>n</mi></math>, then so is <math><mi>n</mi><mo
    >/</mo><mi>d</mi></math>. But <math><mi>d</mi></math> and <math><mi
    >n</mi><mo>/</mo><mi>d</mi></math> cannot both be greater than <math
    ><msqrt><mi>n</mi></msqrt></math>.
  </p>
  <p>
    <sup id="r1.45d"><a href="#r1.45s">45</a></sup> Pierre de Fermat
    (1601-1665) is considered to be the founder of modern number theory. He
    obtained many important number-theoretic results, but he usually announced
    just the results, without providing his proofs. Fermat’s Little Theorem was
    stated in a letter he wrote in 1640. The first published proof was given by
    Euler in 1736 (and an earlier, identical proof was discovered in the
    unpublished manuscripts of Leibniz). The most famous of Fermat’s
    results—known as Fermat’s Last Theorem—was jotted down in 1637 in his copy
    of the book Arithmetic (by the third-century Greek mathematician
    Diophantus) with the remark “I have discovered a truly remarkable proof,
    but this margin is too small to contain it.” Finding a proof of Fermat’s
    Last Theorem became one of the most famous challenges in number theory. A
    complete solution was finally given in 1995 by Andrew Wiles of Princeton
    University.
  </p>
  <p>
    <sup id="r1.46d"><a href="#r1.46s">46</a></sup> The reduction steps in the
    cases where the exponent <math><mi>e</mi></math> is greater than 1 are
    based on the fact that, for any integers <math><mi>x</mi></math>, <math><mi
    >y</mi></math>, and <math><mi>m</mi></math>, we can find the remainder of
    <math><mi>x</mi></math> times <math><mi>y</mi></math> modulo <math><mi
    >m</mi></math> by computing separately the remainders of <math><mi
    >x</mi></math> modulo <math><mi>m</mi></math> and <math><mi>y</mi></math>
    modulo <math><mi>m</mi></math>, multiplying these, and then taking the
    remainder of the result modulo <math><mi>m</mi></math>. For instance, in
    the case where <math><mi>e</mi></math> is even, we compute the remainder of
    <math><msup><mi>b</mi><mrow><mi>e</mi><mo>/</mo><mn
    >2</mn></mrow></msup></math> modulo <math><mi>m</mi></math>, square this,
    and take the remainder modulo <math><mi>m</mi></math>. This technique is
    useful because it means we can perform our computation without ever having
    to deal with numbers much larger than <math><mi>m</mi></math>. (Compare
    exercise <a href="#e1.25">1.25</a>.)
  </p>
  <p>
    <sup id="r1.47d"><a href="#r1.47s">47</a></sup> Numbers that fool the
    Fermat test are called <em>Carmichael numbers</em>, and little is known
    about them other than that they are extremely rare. There are 255
    Carmichael numbers below 100,000,000. The smallest few are 561, 1105, 1729,
    2465, 2821, and 6601. In testing primality of very large numbers chosen at
    random, the chance of stumbling upon a value that fools the Fermat test is
    less than the chance that cosmic radiation will cause the computer to make
    an error in carrying out a “correct” algorithm. Considering an algorithm to
    be inadequate for the first reason but not for the second illustrates the
    difference between mathematics and engineering.
  </p>
  <p>
    <sup id="r1.48d"><a href="#r1.48s">48</a></sup> One of the most striking
    applications of probabilistic prime testing has been to the field of
    cryptography. Although it is now computationally infeasible to factor an
    arbitrary 200-digit number, the primality of such a number can be checked
    in a few seconds with the Fermat test. This fact forms the basis of a
    technique for constructing “unbreakable codes” suggested by Rivest, Shamir,
    and Adleman (1977). The resulting <em>RSA algorithm</em> has become a
    widely used technique for enhancing the security of electronic
    communications. Because of this and related developments, the study of
    prime numbers, once considered the epitome of a topic in “pure” mathematics
    to be studied only for its own sake, now turns out to have important
    practical applications to cryptography, electronic funds transfer, and
    information retrieval.
  </p>
</footer>

<section id="s1.3">
<section id="s1.3.0">
  <h3><a href="#1.3">1.3 Formulating Abstractions with Higher-Order
    Procedures</a></h3>
  <p>
    We have seen that procedures are, in effect, abstractions that describe
    compound operations on numbers independent of the particular numbers. For
    example, when we
  </p>
  <textarea name="1.3.0-01">(define (cube x) (* x x x))</textarea>
  <output for="1.3.0-01" class="block">cube</output>
  <p>
    we are not talking about the cube of a particular number, but rather about
    a method for obtaining the cube of any number. Of course we could get along
    without ever defining this procedure, by always writing expressions such as
  </p>
  <pre><code>(* 3 3 3)
(* x x x)
(* y y y)</code></pre>
  <p>
    and never mentioning <code>cube</code> explicitly. This would place us at a
    serious disadvantage, forcing us to work always at the level of the
    particular operations that happen to be primitives in the language
    (multiplication, in this case) rather than in terms of higher-level
    operations. Our programs would be able to compute cubes, but our language
    would lack the ability to express the concept of cubing. One of the things
    we should demand from a powerful programming language is the ability to
    build abstractions by assigning names to common patterns and then to work
    in terms of the abstractions directly. Procedures provide this ability.
    This is why all but the most primitive programming languages include
    mechanisms for defining procedures.
  </p>
  <p>
    Yet even in numerical processing we will be severely limited in our ability
    to create abstractions if we are restricted to procedures whose parameters
    must be numbers. Often the same programming pattern will be used with a
    number of different procedures. To express such patterns as concepts, we
    will need to construct procedures that can accept procedures as arguments
    or return procedures as values. Procedures that manipulate procedures are
    called <em>higher-order procedures</em>. This section shows how
    higher-order procedures can serve as powerful abstraction mechanisms,
    vastly increasing the expressive power of our language.
  </p>
</section>

<section id="s1.3.1">
  <h4><a href="#s1.3.1">1.3.1 Procedures as Arguments</a></h4>
  <p>
    Consider the following three procedures. The first computes the sum of the
    integers from <code>a</code> through <code>b</code>:
  </p>
  <textarea name="1.3.1-01">
(define (sum-integers a b)
  (if (&gt; a b)
      0
      (+ a (sum-integers (+ a 1) b))))</textarea>
  <output for="1.3.1-01" class="block">sum-integers</output>
  <p>
    The second computes the sum of the cubes of the integers in the given
    range:
  </p>
  <textarea name="1.3.1-02" data-extends="1.3.0-01">
(define (sum-cubes a b)
  (if (&gt; a b)
      0
      (+ (cube a) (sum-cubes (+ a 1) b))))</textarea>
  <output for="1.3.1-02" class="block">sum-cubes</output>
  <p>
    The third computes the sum of a sequence of terms in the series
  </p>
  <figure>
    <math display="block">
      <mfrac><mn>1</mn><mrow><mn>1</mn><mo>⋅</mo><mn>3</mn></mrow></mfrac>
      <mo>+</mo>
      <mfrac><mn>1</mn><mrow><mn>5</mn><mo>⋅</mo><mn>7</mn></mrow></mfrac>
      <mo>+</mo>
      <mfrac><mn>1</mn><mrow><mn>9</mn><mo>⋅</mo><mn>11</mn></mrow></mfrac>
      <mo>+</mo>
      <mi>…</mi>
    </math>
  </figure>
  <p>
    which converges to <math><mi>π</mi><mo>/</mo><mn>8</mn></math> (very
    slowly):<sup id="r1.49s"><a href="#r1.49d">49</a></sup>
  </p>
  <textarea name="1.3.1-03">
(define (pi-sum a b)
  (if (&gt; a b)
      0
      (+ (/ 1.0 (* a (+ a 2))) (pi-sum (+ a 4) b))))</textarea>
  <output for="1.3.1-03" class="block">pi-sum</output>
  <p>
    These three procedures clearly share a common underlying pattern. They are
    for the most part identical, differing only in the name of the procedure,
    the function of <code>a</code> used to compute the term to be added, and
    the function that provides the next value of <code>a</code>. We could
    generate each of the procedures by filling in slots in the same template:
  </p>
  <pre><code>(define (&lt;name&gt; a b)
  (if (&gt; a b)
      0
      (+ (&lt;term&gt; a)
         (&lt;name&gt; (&lt;next&gt; a) b))))</code></pre>
  <p>
    The presence of such a common pattern is strong evidence that there is a
    useful abstraction waiting to be brought to the surface. Indeed,
    mathematicians long ago identified the abstraction of <em>summation of a
    series</em> and invented “sigma notation,” for example
  </p>
  <figure>
    <math display="block">
        <munderover>
          <mo>∑</mo>
          <mrow><mi>n</mi><mo>=</mo><mi>a</mi></mrow>
          <mi>b</mi>
        </munderover>
        <mrow>
          <mi>f</mi><mo>&lpar;</mo><mi>n</mi><mo>&rpar;</mo>
          <mo>=</mo>
          <mi>f</mi><mo>&lpar;</mo><mi>a</mi><mo>&rpar;</mo>
          <mo>+</mo>
          <mi>…</mi>
          <mo>+</mo>
          <mi>f</mi><mo>&lpar;</mo><mi>b</mi><mo>&rpar;</mo>
        </mrow>
    </math>
  </figure>
  <p>
    to express this concept. The power of sigma notation is that it allows
    mathematicians to deal with the concept of summation itself rather than
    only with particular sums—for example, to formulate general results about
    sums that are independent of the particular series being summed.
  </p>
  <p>
    Similarly, as program designers, we would like our language to be powerful
    enough so that we can write a procedure that expresses the concept of
    summation itself rather than only procedures that compute particular sums.
    We can do so readily in our procedural language by taking the common
    template shown above and transforming the “slots” into formal parameters:
  </p>
  <textarea name="1.3.1-04">
(define (sum term a next b)
  (if (&gt; a b)
      0
      (+ (term a)
         (sum term (next a) next b))))</textarea>
  <output for="1.3.1-04" class="block">sum</output>
  <p>
    Notice that <code>sum</code> takes as its arguments the lower and upper
    bounds <code>a</code> and <code>b</code> together with the procedures
    <code>term</code> and <code>next</code>. We can use <code>sum</code> just
    as we would any procedure. For example, we can use it (along with a
    procedure <code>inc</code> that increments its argument by 1) to define
    <code>sum-cubes</code>:
  </p>
  <textarea name="1.3.1-05" data-extends="1.3.0-01 1.3.1-04">
(define (inc n) (+ n 1))
(define (sum-cubes a b)
  (sum cube a inc b))</textarea>
  <output for="1.3.1-05" class="block">inc
sum-cubes</output>
  <p>
    Using this, we can compute the sum of the cubes of the integers from 1 to
    10:
  </p>
  <textarea name="1.3.1-06" data-extends="1.3.1-05">(sum-cubes 1 10)</textarea>
  <output for="1.3.1-06" class="block">3025</output>
  <p>
    With the aid of an identity procedure to compute the term, we can define
    <code>sum-integers</code> in terms of <code>sum</code>:
  </p>
  <textarea name="1.3.1-07" data-extends="1.3.1-05">
(define (identity x) x)

(define (sum-integers a b)
  (sum identity a inc b))</textarea>
  <output for="1.3.1-07" class="block">identity
sum-integers</output>
  <p>
    Then we can add up the integers from 1 to 10:
  </p>
  <textarea name="1.3.1-08" data-extends="1.3.1-07">
(sum-integers 1 10)</textarea>
  <output for="1.3.1-08" class="block">55</output>
  <p>
    We can also define <code>pi-sum</code> in the same way:<sup id="r1.50s"><a
    href="#r1.50d">50</a></sup>
  </p>
  <textarea name="1.3.1-09" data-extends="1.3.1-04">
(define (pi-sum a b)
  (define (pi-term x)
    (/ 1.0 (* x (+ x 2))))
  (define (pi-next x)
    (+ x 4))
  (sum pi-term a pi-next b))</textarea>
  <output for="1.3.1-09" class="block">pi-sum</output>
  <p>
    Using these procedures, we can compute an approximation to <math><mi
    >π</mi></math>:
  </p>
  <textarea name="1.3.1-10" data-extends="1.3.1-09">
(* 8 (pi-sum 1 1000))</textarea>
  <output for="1.3.1-10" class="block">3.139592655589783</output>
  <p>
    Once we have <code>sum</code>, we can use it as a building block in
    formulating further concepts. For instance, the definite integral of a
    function <math><mi>f</mi></math> between the limits <math><mi>a</mi></math>
    and <math><mi>b</mi></math> can be approximated numerically using the
    formula
  </p>
  <figure>
    <math display="block">
      <mrow>
        <munderover>
          <mo>∫</mo>
          <mi>a</mi>
          <mi>b</mi>
        </munderover>
        <mi>f</mi>
        <mo>=</mo>
        <mo>&lbrack;</mo>
        <mi>f</mi><mo>&lpar;</mo><mi>a</mi><mo>+</mo><mfrac><mrow><mi>d</mi><mi
        >x</mi></mrow><mn>2</mn></mfrac><mo>&rpar;</mo>
        <mo>+</mo>
        <mi>f</mi><mo>&lpar;</mo><mi>a</mi><mo>+</mo><mi>d</mi><mi>x</mi><mo
        >+</mo><mfrac><mrow><mi>d</mi><mi>x</mi></mrow><mn>2</mn></mfrac><mo
        >&rpar;</mo>
        <mo>+</mo>
        <mi>f</mi><mo>&lpar;</mo><mi>a</mi><mo>+</mo><mn>2</mn><mi>d</mi><mi
        >x</mi><mo>+</mo><mfrac><mrow><mi>d</mi><mi>x</mi></mrow><mn
        >2</mn></mfrac><mo>&rpar;</mo>
        <mo>+</mo>
        <mi>…</mi>
        <mo>&rbrack;</mo>
        <mi>d</mi><mi>x</mi>
      </mrow>
    </math>
  </figure>
  <p>
    for small values of <math><mi>d</mi><mi>x</mi></math>. We can express this
    directly as a procedure:
  </p>
  <textarea name="1.3.1-11" data-extends="1.3.0-01 1.3.1-04">
(define (integral f a b dx)
  (define (add-dx x) (+ x dx))
  (* (sum f (+ a (/ dx 2.0)) add-dx b)
     dx))
(integral cube 0 1 0.01)
(integral cube 0 1 0.001)</textarea>
  <output for="1.3.1-11" class="block">integral
0.24998750000000042
0.249999875000001</output>
  <p>
    (The exact value of the integral of <code>cube</code> between 0 and 1 is
    1/4.)
  </p>
</section>

<section id="e1.29">
  <h5><a href="#e1.29">Exercise 1.29</a></h5>
  <p>
    Simpson’s Rule is a more accurate method of numerical integration than the
    method illustrated above. Using Simpson’s Rule, the integral of a function
    <math><mi>f</mi></math> between <math><mi>a</mi></math> and <math><mi
    >b</mi></math> is approximated as
  </p>
  <figure>
    <math display="block">
      <mrow>
        <mfrac><mi>h</mi><mn>3</mn></mfrac>
        <mo>&lbrack;</mo>
        <msub><mi>y</mi><mn>0</mn></msub>
        <mo>+</mo>
        <mn>4</mn><msub><mi>y</mi><mn>1</mn></msub>
        <mo>+</mo>
        <mn>2</mn><msub><mi>y</mi><mn>2</mn></msub>
        <mo>+</mo>
        <mn>4</mn><msub><mi>y</mi><mn>3</mn></msub>
        <mo>+</mo>
        <mn>2</mn><msub><mi>y</mi><mn>4</mn></msub>
        <mo>+</mo>
        <mi>…</mi>
        <mo>+</mo>
        <mn>2</mn><msub><mi>y</mi>
          <mrow><mi>n</mi><mo>-</mo><mn>2</mn></mrow></msub>
        <mo>+</mo>
        <mn>4</mn><msub><mi>y</mi>
          <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub>
        <mo>+</mo>
        <msub><mi>y</mi><mi>n</mi></msub>
        <mo>&rbrack;</mo>
      </mrow>
    </math>
  </figure>
  <p>
    where <math><mi>h</mi><mo>=</mo><mo>(</mo><mi>b</mi><mo>-</mo><mi>a</mi><mo
    >)</mo><mo>/</mo><mi>n</mi></math>, for some even integer <math><mi
    >n</mi></math>, and <math><msub><mi>y</mi><mi>k</mi></msub><mo>=</mo><mi
    >f</mi><mo>(</mo><mi>a</mi><mo>+</mo><mi>k</mi><mi>h</mi><mo>)</mo></math>.
    (Increasing <math><mi>n</mi></math> increases the accuracy of the
    approximation.) Define a procedure that takes as arguments <math><mi
    >f</mi></math>, <math><mi>a</mi></math>, <math><mi>b</mi></math>, and <math
    ><mi>n</mi></math> and returns the value of the integral, computed using
    Simpson’s Rule. Use your procedure to integrate <code>cube</code> between 0
    and 1 (with <math><mi>n</mi><mo>=</mo><mn>100</mn></math> and <math><mi
    >n</mi><mo>=</mo><mn>1000</mn></math>), and compare the results to those of
    the <code>integral</code> procedure shown above.
  </p>
  <textarea name="e1.29-01"></textarea>
  <output for="e1.29-01" class="block">&ZeroWidthSpace;</output>
</section>

<section id="e1.30">
  <h5><a href="#e1.30">Exercise 1.30</a></h5>
  <p>
    The <code>sum</code> procedure above generates a linear recursion. The
    procedure can be rewritten so that the sum is performed iteratively. Show
    how to do this by filling in the missing expressions in the following
    definition:
  </p>
  <textarea name="e1.30-01">
(define (sum term a next b)
  (define (iter a result)
    (if &lt;??&gt;
        &lt;??&gt;
        (iter &lt;??&gt; &lt;??&gt;)))
  (iter &lt;??&gt; &lt;??&gt;))</textarea>
  <output for="e1.30-01" class="block">sum</output>
</section>

<section id="e1.31">
  <h5><a href="#e1.31">Exercise 1.31</a></h5>
  <ol type="a">
    <li>
      <p>
        The <code>sum</code> procedure is only the simplest of a vast number of
        similar abstractions that can be captured as higher-order
        procedures.<sup id="r1.51s"><a href="#r1.51d">51</a></sup> Write an
        analogous procedure called <code>product</code> that returns the
        product of the values of a function at points over a given range. Show
        how to define <code>factorial</code> in terms of <code>product</code>.
        Also use <code>product</code> to compute approximations to <math><mi
        >π</mi></math> using the formula<sup id="r1.52s"><a href="#r1.52d"
        >52</a></sup>
      </p>
      <figure>
        <math display="block">
          <mrow>
            <mfrac><mi>π</mi><mn>4</mn></mfrac>
            <mo>=</mo>
            <mfrac>
              <mrow><mn>2</mn><mo>⋅</mo><mn>4</mn><mo>⋅</mo><mn>4</mn><mo
              >⋅</mo><mn>6</mn><mo>⋅</mo><mn>6</mn><mo>⋅</mo><mn>8</mn><mo
              >⋯</mo></mrow>
              <mrow><mn>3</mn><mo>⋅</mo><mn>3</mn><mo>⋅</mo><mn>5</mn><mo
              >⋅</mo><mn>5</mn><mo>⋅</mo><mn>7</mn><mo>⋅</mo><mn>7</mn><mo
              >⋯</mo></mrow>
            </mfrac>
          </mrow>
        </math>
      </figure>
    </li>
  </ol>
  <textarea name="e1.31-01"></textarea>
  <output for="e1.31-01" class="block">&ZeroWidthSpace;</output>
  <ol type="a" start="2">
    <li>
      If your <code>product</code> procedure generates a recursive process,
      write one that generates an iterative process. If it generates an
      iterative process, write one that generates a recursive process.
    </li>
  </ol>
  <textarea name="e1.31-02"></textarea>
  <output for="e1.31-02" class="block">&ZeroWidthSpace;</output>
</section>

<section id="e1.32">
  <h5><a href="#e1.32">Exercise 1.32</a></h5>
  <ol type="a">
    <li>
      <p>
        Show that <code>sum</code> and <code>product</code> (exercise <a
        href="#e1.31">1.31</a>) are both special cases of a still more general
        notion called <code>accumulate</code> that combines a collection of
        terms, using some general accumulation function:
      </p>
      <pre><code>(accumulate combiner null-value term a next b)</code></pre>
      <p>
        <code>Accumulate</code> takes as arguments the same term and range
        specifications as <code>sum</code> and <code>product</code>, together
        with a <code>combiner</code> procedure (of two arguments) that
        specifies how the current term is to be combined with the accumulation
        of the preceding terms and a null-value that specifies what base value
        to use when the terms run out. Write <code>accumulate</code> and show
        how <code>sum</code> and <code>product</code> can both be defined as
        simple calls to <code>accumulate</code>.
      </p>
    </li>
  </ol>
  <textarea name="e1.32-01"></textarea>
  <output for="e1.32-01" class="block">&ZeroWidthSpace;</output>
  <ol type="a" start="2">
    <li>
      If your <code>accumulate</code> procedure generates a recursive process,
      write one that generates an iterative process. If it generates an
      iterative process, write one that generates a recursive process.
    </li>
  </ol>
  <textarea name="e1.32-02"></textarea>
  <output for="e1.32-02" class="block">&ZeroWidthSpace;</output>
</section>

<section id="e1.33">
  <h5><a href="#e1.33">Exercise 1.33</a></h5>
  <p>
    You can obtain an even more general version of <code>accumulate</code>
    (exercise <a href="#e1.32">1.32</a>) by introducing the notion of a
    <em>filter</em> on the terms to be combined. That is, combine only those
    terms derived from values in the range that satisfy a specified condition.
    The resulting <code>filtered-accumulate</code> abstraction takes the same
    arguments as <code>accumulate</code>, together with an additional predicate
    of one argument that specifies the filter. Write <code
    >filtered-accumulate</code> as a procedure. Show how to express the
    following using <code>filtered-accumulate</code>:
  </p>
  <ol type="a">
    <li>
      the sum of the squares of the prime numbers in the interval <code
      >a</code> to <code>b</code> (assuming that you have a <code>prime?</code>
      predicate already written)
    </li>
  </ol>
  <textarea name="e1.33-01" data-extends="ex1.2-02"></textarea>
  <output for="e1.33-01" class="block">&ZeroWidthSpace;</output>
  <ol type="a" start="2">
    <li>
      the product of all the positive integers less than <math><mi>n</mi></math>
      that are relatively prime to <math><mi>n</mi></math> (i.e., all positive
      integers <math><mi>i</mi><mo>&lt;</mo><mi>n</mi></math> such that <math
      ><mtext>GCD</mtext><mo>(</mo><mi>i</mi><mo>,</mo><mi>n</mi><mo>)</mo><mo
      >=</mo><mn>1</mn></math>).
    </li>
  </ol>
  <textarea name="e1.33-02" data-extends="e1.33-01"></textarea>
  <output for="e1.33-02" class="block">&ZeroWidthSpace;</output>
</section>
</section>

<section id="s1.3.2">
  <h4><a href="#s1.3.2">1.3.2 Constructing Procedures Using <code
    >Lambda</code></a></h4>
  <p>
    In using <code>sum</code> as in section <a href="#s1.3.1">1.3.1</a>, it
    seems terribly awkward to have to define trivial procedures such as <code
    >pi-term</code> and <code>pi-next</code> just so we can use them as
    arguments to our higher-order procedure. Rather than define <code
    >pi-next</code> and <code>pi-term</code>, it would be more convenient to
    have a way to directly specify “the procedure that returns its input
    incremented by 4” and “the procedure that returns the reciprocal of its
    input times its input plus 2.” We can do this by introducing the special
    form <code>lambda</code>, which creates procedures. Using <code
    >lambda</code> we can describe what we want as
  </p>
  <textarea name="1.3.2-01">(lambda (x) (+ x 4))</textarea>
  <output for="1.3.2-01" class="block">lambda</output>
  <p>
    and
  </p>
  <textarea name="1.3.2-02">(lambda (x) (/ 1.0 (* x (+ x 2))))</textarea>
  <output for="1.3.2-02" class="block">lambda</output>
  <p>
    Then our <code>pi-sum</code> procedure can be expressed without defining
    any auxiliary procedures as
  </p>
  <textarea name="1.3.2-03" data-extends="1.3.1-04">
(define (pi-sum a b)
  (sum (lambda (x) (/ 1.0 (* x (+ x 2))))
       a
       (lambda (x) (+ x 4))
       b))</textarea>
  <output for="1.3.2-03" class="block">pi-sum</output>
  <p>
    Again using <code>lambda</code>, we can write the <code>integral</code>
    procedure without having to define the auxiliary procedure <code
    >add-dx</code>:
  </p>
  <textarea name="1.3.2-04" data-extends="1.3.1-04">
(define (integral f a b dx)
  (* (sum f
          (+ a (/ dx 2.0))
          (lambda (x) (+ x dx))
          b)
     dx))</textarea>
  <output for="1.3.2-04" class="block">integral</output>
  <p>
    In general, <code>lambda</code> is used to create procedures in the same
    way as <code>define</code>, except that no name is specified for the
    procedure:
  </p>
  <pre><code>(lambda (<em>&lt;formal-parameters&gt;</em>) <em
>&lt;body&gt;</em>)</code></pre>
  <p>
    The resulting procedure is just as much a procedure as one that is created
    using <code>define</code>. The only difference is that it has not been
    associated with any name in the environment. In fact,
  </p>
  <textarea name="1.3.2-05">(define (plus4 x) (+ x 4))</textarea>
  <output for="1.3.2-05" class="block">plus4</output>
  <p>
    is equivalent to
  </p>
  <textarea name="1.3.2-06">(define plus4 (lambda (x) (+ x 4)))</textarea>
  <output for="1.3.2-06" class="block">plus4</output>
  <p>
    We can read a <code>lambda</code> expression as follows:
  </p>
  <pre><code>    (lambda             (x)             (+    x     4))
        ↑                ↑               ↑    ↑     ↑
 the procedure   of an argument x  that adds  x and 4</code></pre>
  <p>
    Like any expression that has a procedure as its value, a <code
    >lambda</code> expression can be used as the operator in a combination such
    as
  </p>
  <textarea name="1.3.2-07" data-extends="1.1.4-01">
((lambda (x y z) (+ x y (square z))) 1 2 3)</textarea>
  <output for="1.3.2-07" class="block">12</output>
  <p>
    or, more generally, in any context where we would normally use a procedure
    name.<sup id="r1.53s"><a href="#r1.53d">53</a></sup>
  </p>
</section>

<section id="ex1.5">
  <h5><a href="#ex1.5">Using <code>let</code> to create local
    variables</a></h5>
  <p>
    Another use of <code>lambda</code> is in creating local variables. We often
    need local variables in our procedures other than those that have been
    bound as formal parameters. For example, suppose we wish to compute the
    function
  </p>
  <figure>
    <math display="block">
      <mrow>
        <mi>f</mi><mo>&lpar;</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>&rpar;</mo>
        <mo>=</mo>
        <mi>x</mi><msup><mrow><mo>&lpar;</mo><mn>1</mn><mo>+</mo><mi>x</mi><mi
        >y</mi><mo>&rpar;</mo></mrow><mn>2</mn></msup>
        <mo>+</mo>
        <mi>y</mi><mo>&lpar;</mo><mn>1</mn><mo>-</mo><mi>y</mi><mo>&rpar;</mo>
        <mo>+</mo>
        <mo>&lpar;</mo><mn>1</mn><mo>+</mo><mi>x</mi><mi>y</mi><mo>&rpar;</mo>
        <mo>&lpar;</mo><mn>1</mn><mo>-</mo><mi>y</mi><mo>&rpar;</mo>
      </mrow>
    </math>
  </figure>
  <p>
    which we could also express as
  </p>
  <figure>
    <math display="block">
      <mtable>
        <mtr>
          <mtd><mi>a</mi></mtd>
          <mtd><mo>=</mo></mtd>
          <mtd><mn>1</mn><mo>+</mo><mi>x</mi><mi>y</mi></mtd>
        </mtr>
        <mtr>
          <mtd><mi>b</mi></mtd>
          <mtd><mo>=</mo></mtd>
          <mtd><mn>1</mn><mo>-</mo><mi>y</mi></mtd>
        </mtr>
        <mtr>
          <mtd><mi>f</mi><mo>&lpar;</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo
            >&rpar;</mo></mtd>
          <mtd><mo>=</mo></mtd>
          <mtd><mi>x</mi><msup><mi>a</mi><mn>2</mn></msup><mo>+</mo><mi
            >y</mi><mi>b</mi><mo>+</mo><mi>a</mi><mi>b</mi></mtd>
        </mtr>
      </mtable>
    </math>
  </figure>
  <p>
    In writing a procedure to compute <math><mi>f</mi></math>, we would like to
    include as local variables not only <math><mi>x</mi></math> and <math><mi
    >y</mi></math> but also the names of intermediate quantities like <math><mi
    >a</mi></math> and <math><mi>b</mi></math>. One way to accomplish this is
    to use an auxiliary procedure to bind the local variables:
  </p>
  <textarea name="ex1.5-01" data-extends="1.1.4-01">
(define (f x y)
  (define (f-helper a b)
    (+ (* x (square a))
       (* y b)
       (* a b)))
  (f-helper (+ 1 (* x y))
            (- 1 y)))</textarea>
  <output for="ex1.5-01" class="block">f</output>
  <p>
    Of course, we could use a <code>lambda</code> expression to specify an
    anonymous procedure for binding our local variables. The body of <code
    >f</code> then becomes a single call to that procedure:
  </p>
  <textarea name="ex1.5-02" data-extends="1.1.4-01">
(define (f x y)
  ((lambda (a b)
     (+ (* x (square a))
        (* y b)
        (* a b)))
   (+ 1 (* x y))
   (- 1 y)))</textarea>
  <output for="ex1.5-02" class="block">f</output>
  <p>
    This construct is so useful that there is a special form called <code
    >let</code> to make its use more convenient. Using <code>let</code>, the
    <code>f</code> procedure could be written as
  </p>
  <textarea name="ex1.5-03" data-extends="1.1.4-01">
(define (f x y)
  (let ((a (+ 1 (* x y)))
        (b (- 1 y)))
    (+ (* x (square a))
       (* y b)
       (* a b))))</textarea>
  <output for="ex1.5-03" class="block">f</output>
  <p>
    The general form of a <code>let</code> expression is
  </p>
  <pre><code>(let ((<em>&lt;var<sub>1</sub>&gt; &lt;exp<sub>1</sub>&gt;</em>)
     (<em>&lt;var<sub>2</sub>&gt; &lt;exp<sub>2</sub>&gt;</em>)
     ⋮
     (<em>&lt;var<sub>n</sub>&gt; &lt;exp<sub>n</sub>&gt;</em>))
  <em>&lt;body&gt;</em>)</code></pre>
  <p>
    which can be thought of as saying
  </p>
  <pre><code>let &lt;var<sub>1</sub>&gt; have the value &lt;exp<sub
>1</sub>&gt; and
    &lt;var<sub>2</sub>&gt; have the value &lt;exp<sub>2</sub>&gt; and
    &lt;var<sub>n</sub>&gt; have the value &lt;exp<sub>n</sub>&gt;
  in &lt;body&gt;</code></pre>
  <p>
    The first part of the <code>let</code> expression is a list of
    name-expression pairs. When the <code>let</code> is evaluated, each name is
    associated with the value of the corresponding expression. The body of the
    <code>let</code> is evaluated with these names bound as local variables.
    The way this happens is that the <code>let</code> expression is interpreted
    as an alternate syntax for
  </p>
  <pre><code>((lambda (<em>&lt;var<sub>1</sub>&gt;</em> … <em>&lt;var<sub
>n</sub>&gt;</em>)
    <em>&lt;body&gt;</em>)
 <em>&lt;exp<sub>1</sub>&gt;</em>
 ⋮
 <em>&lt;exp<sub>n</sub>&gt;</em>)</code></pre>
  <p>
    No new mechanism is required in the interpreter in order to provide local
    variables. A <code>let</code> expression is simply syntactic sugar for the
    underlying <code>lambda</code> application.
  </p>
  <p>
    We can see from this equivalence that the scope of a variable specified by
    a <code>let</code> expression is the body of the <code>let</code>. This
    implies that:
  </p>
  <ul>
    <li>
      <p>
        <code>Let</code> allows one to bind variables as locally as possible to
        where they are to be used. For example, if the value of <code>x</code>
        is 5, the value of the expression
      </p>
      <textarea name="ex1.5-04" hidden>(define x 5)</textarea>
      <textarea name="ex1.5-05" data-extends="ex1.5-04">
(+ (let ((x 3))
     (+ x (* x 10)))
   x)</textarea>
      <p>
        is <output for="ex1.5-05">38</output>. Here, the <code>x</code> in the
        body of the <code>let</code> is 3, so the value of the <code>let</code>
        expression is 33. On the other hand, the <code>x</code> that is the
        second argument to the outermost <code>+</code> is still 5.
      </p>
    </li>
    <li>
      <p>
        The variables’ values are computed outside the <code>let</code>. This
        matters when the expressions that provide the values for the local
        variables depend upon variables having the same names as the local
        variables themselves. For example, if the value of <code>x</code> is 2,
        the expression
      </p>
      <textarea name="ex1.5-06" hidden>(define x 2)</textarea>
      <textarea name="ex1.5-07" data-extends="ex1.5-06">
(let ((x 3)
      (y (+ x 2)))
  (* x y))</textarea>
      <p>
        will have the value <output for="ex1.5-07">12</output> because, inside
        the body of the <code>let</code>, <code>x</code> will be 3 and <code
        >y</code> will be 4 (which is the outer <code>x</code> plus 2).
      </p>
    </li>
  </ul>
  <p>
    Sometimes we can use internal definitions to get the same effect as with
    <code>let</code>. For example, we could have defined the procedure <code
    >f</code> above as
  </p>
  <textarea name="ex1.5-08">
(define (f x y)
  (define a (+ 1 (* x y)))
  (define b (- 1 y))
  (+ (* x (square a))
     (* y b)
     (* a b)))</textarea>
  <output for="ex1.5-08" class="block">f</output>
  <p>
    We prefer, however, to use <code>let</code> in situations like this and to
    use internal define only for internal procedures.<sup id="r1.54s"><a
    href="#r1.54d">54</a></sup>
  </p>
</section>

<section id="e1.34">
  <h5><a href="#e1.34">Exercise 1.34</a></h5>
  <p>
    Suppose we define the procedure
  </p>
  <textarea name="e1.34-01">
(define (f g)
  (g 2))</textarea>
  <output for="e1.34-01" class="block">f</output>
  <p>
    Then we have
  </p>
  <textarea name="e1.34-02" data-extends="1.1.4-01 e1.34-01">
(f square)
(f (lambda (z) (* z (+ z 1))))</textarea>
  <output for="e1.34-02" class="block">4
6</output>
  <p>
    What happens if we (perversely) ask the interpreter to evaluate the
    combination <code>(f f)</code>? Explain.
  </p>
  <textarea name="e1.34-03" data-extends="e1.34-02"></textarea>
  <output for="e1.34-03" class="block">&ZeroWidthSpace;</output>
</section>

<section id="s1.3.3">
  <h4><a href="#s1.3.3">1.3.3 Procedures as General Methods</a></h4>
  <p>
    We introduced compound procedures in section <a href="#s1.1.4">1.1.4</a> as
    a mechanism for abstracting patterns of numerical operations so as to make
    them independent of the particular numbers involved. With higher-order
    procedures, such as the <code>integral</code> procedure of section <a
    href="#s1.3.1">1.3.1</a>, we began to see a more powerful kind of
    abstraction: procedures used to express general methods of computation,
    independent of the particular functions involved. In this section we
    discuss two more elaborate examples—general methods for finding zeros and
    fixed points of functions—and show how these methods can be expressed
    directly as procedures.
  </p>
</section>

<section id="ex1.6">
  <h5><a href="#ex1.6">Finding roots of equations by the half-interval
    method</a></h5>
  <p>
    The <em>half-interval method</em> is a simple but powerful technique for
    finding roots of an equation <math><mi>f</mi><mo>(</mo><mi>x</mi><mo
    >)</mo><mo>=</mo><mn>0</mn></math>, where <math><mi>f</mi></math> is a
    continuous function. The idea is that, if we are given points <math><mi
    >a</mi></math> and <math><mi>b</mi></math> such that <math><mi>f</mi><mo
    >(</mo><mi>a</mi><mo>)</mo><mo>&lt;</mo><mn>0</mn><mo>&lt;</mo><mi
    >f</mi><mo>(</mo><mi>b</mi><mo>)</mo></math>, then <math><mi>f</mi></math>
    must have at least one zero between <math><mi>a</mi></math> and <math><mi
    >b</mi></math>. To locate a zero, let <math><mi>x</mi></math> be the
    average of <math><mi>a</mi></math> and <math><mi>b</mi></math> and compute
    <math><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo></math>. If <math><mi
    >f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>&gt;</mo><mn>0</mn></math>, then
    <math><mi>f</mi></math> must have a zero between <math><mi>a</mi></math>
    and <math><mi>x</mi></math>. If <math><mi>f</mi><mo>(</mo><mi>x</mi><mo
    >)</mo><mo>&lt;</mo><mn>0</mn></math>, then <math><mi>f</mi></math> must
    have a zero between <math><mi>x</mi></math> and <math><mi>b</mi></math>.
    Continuing in this way, we can identify smaller and smaller intervals on
    which <math><mi>f</mi></math> must have a zero. When we reach a point where
    the interval is small enough, the process stops. Since the interval of
    uncertainty is reduced by half at each step of the process, the number of
    steps required grows as <math><mi>θ</mi><mo>(</mo><mtext>log</mtext><mo
    >(</mo><mi>L</mi><mo>/</mo><mi>T</mi><mo>)</mo><mo>)</mo></math>, where
    <math><mi>L</mi></math> is the length of the original interval and
    <math><mi>T</mi></math> is the error tolerance (that is, the size of the
    interval we will consider “small enough”). Here is a procedure that
    implements this strategy:
  </p>
  <textarea name="ex1.6-01" hidden>
(define (negative? x) (&lt; x 0))
(define (positive? x) (&gt; x 0))</textarea>
  <textarea name="ex1.6-02" data-extends="1.1.7-03 ex1.6-01 ex1.6-03">
(define (search f neg-point pos-point)
  (let ((midpoint (average neg-point pos-point)))
    (if (close-enough? neg-point pos-point)
        midpoint
        (let ((test-value (f midpoint)))
          (cond ((positive? test-value)
                 (search f neg-point midpoint))
                ((negative? test-value)
                 (search f midpoint pos-point))
                (else midpoint))))))</textarea>
  <output for="ex1.6-02" class="block">search</output>
  <p>
    We assume that we are initially given the function <math><mi>f</mi></math>
    together with points at which its values are negative and positive. We
    first compute the midpoint of the two given points. Next we check to see if
    the given interval is small enough, and if so we simply return the midpoint
    as our answer. Otherwise, we compute as a test value the value of <math><mi
    >f</mi></math> at the midpoint. If the test value is positive, then we
    continue the process with a new interval running from the original negative
    point to the midpoint. If the test value is negative, we continue with the
    interval from the midpoint to the positive point. Finally, there is the
    possibility that the test value is 0, in which case the midpoint is itself
    the root we are searching for.
  </p>
  <p>
    To test whether the endpoints are “close enough” we can use a procedure
    similar to the one used in section <a href="#s1.1.7">1.1.7</a> for
    computing square roots:<sup id="r1.55s"><a href="#r1.55d">55</a></sup>
  </p>
  <textarea name="ex1.6-03" data-extends="1.1.6-03">
(define (close-enough? x y)
  (&lt; (abs (- x y)) 0.001))</textarea>
  <output for="ex1.6-03" class="block">close-enough?</output>
  <p>
    <code>Search</code> is awkward to use directly, because we can accidentally
    give it points at which <math><mi>f</mi></math>’s values do not have the
    required sign, in which case we get a wrong answer. Instead we will use
    <code>search</code> via the following procedure, which checks to see which
    of the endpoints has a negative function value and which has a positive
    value, and calls the search procedure accordingly. If the function has the
    same sign on the two given points, the <code>half-interval</code> method
    cannot be used, in which case the procedure signals an error.<sup
    id="r1.56s"><a href="#r1.56d">56</a></sup>
  </p>
  <textarea name="ex1.6-04" data-extends="ex1.6-02">
(define (half-interval-method f a b)
  (let ((a-value (f a))
        (b-value (f b)))
    (cond ((and (negative? a-value) (positive? b-value))
           (search f a b))
          ((and (negative? b-value) (positive? a-value))
           (search f b a))
          (else
           (error "Values are not of opposite sign" a b)))))</textarea>
  <output for="ex1.6-04">half-interval-method</output>
  <p>
    The following example uses the <code>half-interval</code> method to
    approximate <math><mi>π</mi></math> as the root between 2 and 4 of <math
    ><mtext>sin </mtext><mi>x</mi><mo>=</mo><mn>0</mn></math>:
  </p>
  <textarea name="ex1.6-05" data-extends="ex1.6-04">
(half-interval-method sin 2.0 4.0)</textarea>
  <output for="ex1.6-05" class="block">3.14111328125</output>
  <p>
    Here is another example, using the half-interval method to search for a
    root of the equation <math><msup><mi>x</mi><mn>3</mn></msup><mo>-</mo><mn
    >2</mn><mi>x</mi><mo>-</mo><mn>3</mn><mo>=</mo><mo>0</mo></math> between 1
    and 2:
  </p>
  <textarea name="ex1.6-06" data-extends="ex1.6-04">
(half-interval-method (lambda (x) (- (* x x x) (* 2 x) 3))
                      1.0
                      2.0)</textarea>
  <output for="ex1.6-06" class="block">1.89306640625</output>
</section>

<section id="ex1.7">
  <h5><a href="#ex1.7">Finding fixed points of functions</a></h5>
  <p>
    A number <math><mi>x</mi></math> is called a fixed point of a function
    <math><mi>f</mi></math> if <math><mi>x</mi></math> satisfies the equation
    <math><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>x</mi></math>.
    For some functions <math><mi>f</mi></math> we can locate a fixed point by
    beginning with an initial guess and applying <math><mi>f</mi></math>
    repeatedly,
  </p>
  <figure>
    <math display="block">
      <mrow>
        <mi>f</mi><mo>&lpar;</mo><mi>x</mi><mo>&rpar;</mo><mo>,</mo>
        <mi>f</mi><mo>&lpar;</mo><mi>f</mi><mo>&lpar;</mo><mi>x</mi><mo
        >&rpar;</mo><mo>&rpar;</mo><mo>,</mo>
        <mi>f</mi><mo>&lpar;</mo><mi>f</mi><mo>&lpar;</mo><mi>f</mi><mo
        >&lpar;</mo><mi>x</mi><mo>&rpar;</mo><mo>&rpar;</mo><mo>&rpar;</mo><mo
        >…</mo>
      </mrow>
    </math>
  </figure>
  <p>
    until the value does not change very much. Using this idea, we can devise a
    procedure <code>fixed-point</code> that takes as inputs a function and an
    initial guess and produces an approximation to a fixed point of the
    function. We apply the function repeatedly until we find two successive
    values whose difference is less than some prescribed tolerance:
  </p>
  <textarea name="ex1.7-01" data-extends="ex1.6-03">
(define tolerance 0.00001)
(define (fixed-point f first-guess)
  (define (close-enough? v1 v2)
    (&lt; (abs (- v1 v2)) tolerance))
  (define (try guess)
    (let ((next (f guess)))
      (if (close-enough? guess next)
          next
          (try next))))
  (try first-guess))</textarea>
  <output for="ex1.7-01" class="block">tolerance
fixed-point</output>
  <p>
    For example, we can use this method to approximate the fixed point of the
    cosine function, starting with 1 as an initial approximation:<sup
    id="r1.57s"><a href="#r1.57d">57</a></sup>
  </p>
  <textarea name="ex1.7-02" data-extends="ex1.7-01">
(fixed-point cos 1.0)</textarea>
  <output for="ex1.7-02" class="block">0.7390822985224024</output>
  <p>
    Similarly, we can find a solution to the equation <math><mi>y</mi><mo
    >=</mo><mtext>sin </mtext><mi>y</mi><mo>+</mo><mtext>cos </mtext><mi
    >y</mi></math>:
  </p>
  <textarea name="ex1.7-03" data-extends="ex1.7-01">
(fixed-point (lambda (y) (+ (sin y) (cos y)))
             1.0)</textarea>
  <output for="ex1.7-03" class="block">1.2587315962971173</output>
  <p>
    The fixed-point process is reminiscent of the process we used for finding
    square roots in section <a href="#s1.1.7">1.1.7</a>. Both are based on the
    idea of repeatedly improving a guess until the result satisfies some
    criterion. In fact, we can readily formulate the square-root computation as
    a fixed-point search. Computing the square root of some number <math><mi
    >x</mi></math> requires finding a <math><mi>y</mi></math> such that
    <math><msup><mi>y</mi><mn>2</mn></msup><mo>=</mo><mi>x</mi></math>. Putting 
    this equation into the equivalent form <math><mi>y</mi><mo>=</mo><mi
    >x</mi><mo>/</mo><mi>y</mi></math>, we recognize that we are looking for a
    fixed point of the function<sup id="r1.58s"><a href="#r1.58d">58</a></sup>
    <math><mi>y</mi><mo>&RightTeeArrow;</mo><mi>x</mi><mo>/</mo><mi
    >y</mi></math>, and we can therefore try to compute square roots as
  </p>
  <textarea name="ex1.7-04" data-extends="ex1.7-01">
(define (sqrt x)
  (fixed-point (lambda (y) (/ x y))
               1.0))</textarea>
  <output for="ex1.7-04" class="block">sqrt</output>
  <p>
    Unfortunately, this fixed-point search does not converge. Consider an
    initial guess <math><msub><mi>y</mi><mn>1</mn></msub></math>. The next
    guess is <math><msub><mi>y</mi><mn>2</mn></msub><mo>=</mo><mi>x</mi><mo
    >/</mo><msub><mi>y</mi><mn>1</mn></msub></math> and the next guess is
    <math><msub><mi>y</mi><mn>3</mn></msub><mo>=</mo><mi>x</mi><mo
    >/</mo><mo>(</mo><mi>x</mi><mo>/</mo><msub><mi>y</mi><mn>1</mn></msub><mo
    >)</mo><mo>=</mo><msub><mi>y</mi><mn>1</mn></msub></math>. This results in
    an infinite loop in which the two guesses <math><msub><mi>y</mi><mn
    >1</mn></msub></math> and <math><msub><mi>y</mi><mn>2</mn></msub></math>
    repeat over and over, oscillating about the answer.
  </p>
  <p>
    One way to control such oscillations is to prevent the guesses from
    changing so much. Since the answer is always between our guess <math><mi
    >y</mi></math> and <math><mi>x</mi><mo>/</mo><mi>y</mi></math>, we can make
    a new guess that is not as far from <math><mi>y</mi></math> as <math><mi
    >x</mi><mo>/</mo><mi>y</mi></math> by averaging <math><mi>y</mi></math>
    with <math><mi>x</mi><mo>/</mo><mi>y</mi></math>, so that the next guess
    after <math><mi>y</mi></math> is <math><mo>(</mo><mn>1</mn><mo>/</mo><mn
    >2</mn><mo>)</mo><mo>(</mo><mi>y</mi><mo>+</mo><mi>x</mi><mo>/</mo><mi
    >y</mi><mo>)</mo></math> instead of <math><mi>x</mi><mo>/</mo><mi
    >y</mi></math>. The process of making such a sequence of guesses is simply
    the process of looking for a fixed point of <math><mi>y</mi><mo
    >&RightTeeArrow;</mo><mo>(</mo><mn>1</mn><mo>/</mo><mn>2</mn><mo>)</mo><mo
    >(</mo><mi>y</mi><mo>+</mo><mi>x</mi><mo>/</mo><mi>y</mi><mo>)</mo></math>:
  </p>
  <textarea name="ex1.7-05" data-extends="1.1.7-03 ex1.7-01">
(define (sqrt x)
  (fixed-point (lambda (y) (average y (/ x y)))
               1.0))</textarea>
  <output for="ex1.7-05" class="block">sqrt</output>
  <p>
    (Note that <math><mi>y</mi><mo>=</mo><mo>(</mo><mn>1</mn><mo>/</mo><mn
    >2</mn><mo>)</mo><mo>(</mo><mi>y</mi><mo>+</mo><mi>x</mi><mo>/</mo><mi
    >y</mi><mo>)</mo></math> is a simple transformation of the equation
    <math><mi>y</mi><mo>=</mo><mi>x</mi><mo>/</mo><mi>y</mi></math>; to derive
    it, add <math><mi>y</mi></math> to both sides of the equation and divide by
    2.)
  </p>
  <p>
    With this modification, the square-root procedure works. In fact, if we
    unravel the definitions, we can see that the sequence of approximations to
    the square root generated here is precisely the same as the one generated
    by our original square-root procedure of section <a href="#s1.1.7"
    >1.1.7</a>. This approach of averaging successive approximations to a
    solution, a technique we that we call <em>average damping</em>, often aids
    the convergence of fixed-point searches.
  </p>
</section>

<section id="e1.35">
  <h5><a href="#e1.35">Exercise 1.35</a></h5>
  <p>
    Show that the golden ratio φ (section <a href="#s1.2.2">1.2.2</a>) is a
    fixed point of the transformation <math><mi>x</mi><mo
    >&RightTeeArrow;</mo><mn>1</mn><mo>+</mo><mn>1</mn><mo>/</mo><mi
    >x</mi></math>, and use this fact to compute φ by means of the <code
    >fixed-point</code> procedure.
  </p>
  <textarea name="e1.35-01" data-extends="ex1.7-01"></textarea>
  <output for="e1.35-01" class="block">&ZeroWidthSpace;</output>
</section>

<section id="e1.36">
  <h5><a href="#e1.36">Exercise 1.36</a></h5>
  <p>
    Modify <code>fixed-point</code> so that it prints the sequence of
    approximations it generates, using the <code>newline</code> and <code
    >display</code> primitives shown in exercise <a href="#e1.22">1.22</a>.
    Then find a solution to <math><msup><mi>x</mi><mi>x</mi></msup><mo
    >=</mo><mn>1000</mn></math> by finding a fixed point of <math><mi>x</mi><mo
    >&RightTeeArrow;</mo><mtext>log</mtext><mo>(</mo><mn>1000</mn><mo>)</mo><mo
    >/</mo><mtext>log</mtext><mo>(</mo><mi>x</mi><mo>)</mo></math>. (Use
    Scheme’s primitive <code>log</code> procedure, which computes natural
    logarithms.) Compare the number of steps this takes with and without
    average damping. (Note that you cannot start <code>fixed-point</code> with
    a guess of 1, as this would cause division by <math><mtext>log</mtext><mo
    >(</mo><mn>1</mn><mo>)</mo><mo>=</mo><mn>0</mn></math>.)
  </p>
  <textarea name="e1.36-01" data-extends="ex1.6-03">
(define tolerance 0.00001)
(define (fixed-point f first-guess)
  (define (close-enough? v1 v2)
    (&lt; (abs (- v1 v2)) tolerance))
  (define (try guess)
    (let ((next (f guess)))
      (if (close-enough? guess next)
          next
          (try next))))
  (try first-guess))</textarea>
  <output for="e1.36-01" class="block">tolerance
fixed-point</output>
</section>
</section>

<section id="e1.37">
  <h3><a href="#e1.37">Exercise 1.37</a></h3>
  <ol type="a">
    <li>
      <p>
        An infinite <em>continued fraction</em> is an expression of the form
      </p>
      <figure>
        <math display="block">
          <mrow>
            <mi>f</mi>
            <mo>=</mo>
            <mfrac>
              <msub><mi>N</mi><mn>1</mn></msub>
              <mrow>
                <msub><mi>D</mi><mn>1</mn></msub>
                <mo>+</mo>
                <mfrac>
                  <msub><mi>N</mi><mn>2</mn></msub>
                  <mrow>
                    <msub><mi>D</mi><mn>2</mn></msub>
                    <mo>+</mo>
                    <mfrac>
                      <msub><mi>N</mi><mn>3</mn></msub>
                      <mrow>
                        <msub><mi>D</mi><mi>3</mi></msub>
                        <mo>+</mo>
                        <mi>⋯</mi>
                      </mrow>
                    </mfrac>
                  </mrow>
                </mfrac>
              </mrow>
            </mfrac>
          </mrow>
        </math>
      </figure>
      <p>
        As an example, one can show that the infinite continued fraction
        expansion with the <math><msub><mi>N</mi><mi>i</mi></msub></math> and
        the <math><msub><mi>D</mi><mi>i</mi></msub></math> all equal to 1
        produces <math><mn>1</mn><mo>/</mo><mi>φ</mi></math>, where <math><mi
        >φ</mi></math> is the golden ratio (described in section <a
        href="#s1.2.2">1.2.2</a>). One way to approximate an infinite continued
        fraction is to truncate the expansion after a given number of terms.
        Such a truncation—a so-called <em>k-term finite continued
        fraction</em>—has the form
      </p>
      <figure>
        <math display="block">
          <mfrac>
            <msub><mi>N</mi><mn>1</mn></msub>
            <mrow>
              <msub><mn>D</mn><mn>1</mn></msub>
              <mo>+</mo>
              <mfrac>
                <msub><mi>N</mi><mn>2</mn></msub>
                <mrow>
                  <mi>⋱</mi>
                  <mo>+</mo>
                  <mfrac>
                    <msub><mi>N</mi><mi>K</mi></msub>
                    <msub><mi>D</mi><mi>K</mi></msub>
                  </mfrac>
                </mrow>
              </mfrac>
            </mrow>
          </mfrac>
        </math>
      </figure>
      <p>
        Suppose that <code>n</code> and <code>d</code> are procedures of one
        argument (the term index <math><mi>i</mi></math>) that return the <math
        ><mi>Ni</mi></math> and <math><mi>Di</mi></math> of the terms of the
        continued fraction. Define a procedure <code>cont-frac</code> such that
        evaluating <code>(cont-frac n d k)</code> computes the value of the
        <math><mi>k</mi></math>-term finite continued fraction. Check your
        procedure by approximating <math><mn>1</mn><mo>/</mo><mi>φ</mi></math>
        using
      </p>
      <pre><code>(cont-frac (lambda (i) 1.0)
           (lambda (i) 1.0)
           k)</code></pre>
      <p>
        for successive values of <code>k</code>. How large must you make <code
        >k</code> in order to get an approximation that is accurate to 4
        decimal places?
      </p>
    </li>
  </ol>
  <textarea name="e1.37-01"></textarea>
  <output for="e1.37-01" class="block">&ZeroWidthSpace;</output>
  <ol type="a" start="2">
    <li>
      If your <code>cont-frac</code> procedure generates a recursive process,
      write one that generates an iterative process. If it generates an
      iterative process, write one that generates a recursive process.
    </li>
  </ol>
  <textarea name="e1.37-02"></textarea>
  <output for="e1.37-02" class="block">&ZeroWidthSpace;</output>
</section>

<section id="e1.38">
  <h3><a href="#e1.38">Exercise 1.38</a></h3>
  <p>
    In 1737, the Swiss mathematician Leonhard Euler published a memoir <em>De
    Fractionibus Continuis</em>, which included a continued fraction expansion
    for <math><mi>e</mi><mo>-</mo><mn>2</mn></math>, where <math><mi
    >e</mi></math> is the base of the natural logarithms. In this fraction, the
    <math><msub><mi>N</mi><mi>i</mi></msub></math> are all 1, and the <math
    ><msub><mi>D</mi><mi>i</mi></msub></math> are successively 1, 2, 1, 1, 4,
    1, 1, 6, 1, 1, 8, …. Write a program that uses your <code>cont-frac</code>
    procedure from exercise <a href="#e1.37">1.37</a> to approximate <math><mi
    >e</mi></math>, based on Euler’s expansion.
  </p>
  <textarea name="e1.38-01" data-extends="e1.37-01"></textarea>
  <output for="e1.38-01" class="block">&ZeroWidthSpace;</output>
</section>

<section id="e1.39">
  <h3><a href="#e1.39">Exercise 1.39</a></h3>
  <p>
    A continued fraction representation of the tangent function was published
    in 1770 by the German mathematician J.H. Lambert:
  </p>
  <figure>
    <math display="block">
      <mrow>
        <mtext>tan </mtext><mi>x</mi>
        <mo>=</mo>
        <mfrac>
          <mi>x</mi>
          <mrow>
            <mn>1</mn>
            <mo>-</mo>
            <mfrac>
              <msup><mi>x</mi><mn>2</mn></msup>
              <mrow>
                <mn>3</mn>
                <mo>-</mo>
                <mfrac>
                  <msup><mi>x</mi><mn>2</mn></msup>
                  <mrow>
                    <mn>5</mn>
                    <mo>-</mo>
                    <mi>⋱</mi>
                  </mrow>
                </mfrac>
              </mrow>
            </mfrac>
          </mrow>
        </mfrac>
      </mrow>
    </math>
  </figure>
  <p>
    where <math><mi>x</mi></math> is in radians. Define a procedure <code
    >(tan-cf x k)</code> that computes an approximation to the tangent function
    based on Lambert’s formula. <code>K</code> specifies the number of terms to
    compute, as in exercise <a href="#e1.37">1.37</a>.
  </p>
  <textarea name="e1.39-01" data-extends="e1.37-01"></textarea>
  <output for="e1.39-01" class="block">&ZeroWidthSpace;</output>
</section>
</main>
</main>

<footer>
  <h2>Attribution</h2>
  <p>
    <a href="https://creativecommons.org/licenses/by-sa/4.0/"><img
      src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" loading="lazy"
      ></a> <br>
    Structure and Interpretation of Computer Programs by <a
    href="https://mitpress.mit.edu/sicp">Harold Abelson and Gerald Jay Sussman
    with Julie Sussman</a> is licensed under a <a
    href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons
    Attribution-ShareAlike 4.0 International License</a> by the MIT Press.
  </p>
</footer>
</body>
